{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "- [1. Import Libraries & Data](#import-libraries)\n",
    "- [2. Data Preprocessing](#data-preprocessing)\n",
    "- [3. Models Experiments](#models)\n",
    "    - [3.1 All Features](#All-Features)\n",
    "    - [3.2 Feature-Selection](#Feature-Selection)\n",
    "    - [3.3 Feature Selection using PCA](#Feature-Selection-PCA)\n",
    "- [4. ANN](#ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import-libraries\"></a>\n",
    "# 1. Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn. linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# evaluation\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/processed/trial4.csv')\n",
    "compound_smile = pd.read_csv(\"../../data/processed/compound_smiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logkpl</th>\n",
       "      <th>Texpi</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>...</th>\n",
       "      <th>MW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.25</td>\n",
       "      <td>298</td>\n",
       "      <td>1.4570</td>\n",
       "      <td>2.122849</td>\n",
       "      <td>35.0768</td>\n",
       "      <td>17.399965</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>139.109037</td>\n",
       "      <td>19.385708</td>\n",
       "      <td>1.938571</td>\n",
       "      <td>10.344796</td>\n",
       "      <td>7.326862</td>\n",
       "      <td>3.017933</td>\n",
       "      <td>120.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.508</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.95</td>\n",
       "      <td>310</td>\n",
       "      <td>3.6304</td>\n",
       "      <td>13.179804</td>\n",
       "      <td>49.4776</td>\n",
       "      <td>32.539860</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>172.264984</td>\n",
       "      <td>22.661828</td>\n",
       "      <td>1.888486</td>\n",
       "      <td>4.763098</td>\n",
       "      <td>4.763098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.156</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   logkpl  Texpi   ALogP     ALogp2      AMR       apol  nAcid  naAromAtom  \\\n",
       "0   -2.25    298  1.4570   2.122849  35.0768  17.399965      0           6   \n",
       "1   -2.95    310  3.6304  13.179804  49.4776  32.539860      1           0   \n",
       "\n",
       "   nAromBond  nAtom  ...          MW     WTPT-1    WTPT-2     WTPT-3  \\\n",
       "0          6     15  ...  139.109037  19.385708  1.938571  10.344796   \n",
       "1          0     32  ...  172.264984  22.661828  1.888486   4.763098   \n",
       "\n",
       "     WTPT-4    WTPT-5  WPATH  WPOL  XLogP  Zagreb  \n",
       "0  7.326862  3.017933  120.0  11.0  1.508    46.0  \n",
       "1  4.763098  0.000000  277.0   9.0  4.156    44.0  \n",
       "\n",
       "[2 rows x 224 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 224)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no missing values\n",
    "data.isna().sum().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-preprocessing\"></a>\n",
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (337, 223) \t Shape of y_train: (337,)\n",
      "Shape of X_test: (113, 223) \t Shape of y_test: (113,)\n"
     ]
    }
   ],
   "source": [
    "X = model_data.drop([\"logkpl\"], axis=1)\n",
    "y = model_data['logkpl']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)\n",
    "\n",
    "print(\"Shape of X_train: {} \\t Shape of y_train: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Shape of X_test: {} \\t Shape of y_test: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "# 3. Models Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, cols, model_name, slice=20):\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = cols#X.columns#selected_features_X.columns #\n",
    "\n",
    "    # Create a pandas DataFrame with the feature importances\n",
    "    df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "\n",
    "    # Sort the DataFrame by importance score\n",
    "    df = df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    df[:slice].to_excel(\"../../../results/feature_importance.xlsx\", index=False)\n",
    "\n",
    "    # Create a bar plot using Seaborn\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=df[:slice])\n",
    "    plt.title(\"Top 20 Feature Importances {}\".format(model_name))\n",
    "    plt.ylabel(\"Feature Name\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"All-Features\"></a>\n",
    "### 3.1 All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Dense(256, input_shape=[X.shape[1]]),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(8),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss=\"mean_absolute_error\")\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=2500, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # MAE, MSE, RMSE\n",
    "    print(\"MAE: {}\".format(mean_absolute_error(y_test, predictions)))\n",
    "    print(\"MSE: {}\".format(mean_squared_error(y_test, predictions)))\n",
    "    print(\"RMSE: {}\".format(mean_squared_error(y_test, predictions, squared=False)))\n",
    "    print(\"MAPE: {}\".format(mean_absolute_percentage_error(y_test, predictions)))\n",
    "    print(\"R2: {}\".format(r2_score(y_test, predictions)))\n",
    "\n",
    "    final_result = pd.DataFrame({\"predicted\": model.predict(X_test).reshape(113,), \n",
    "              \"actual\": y_test})\n",
    "    \n",
    "    # saving the model\n",
    "\n",
    "# serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"../../models/ANN_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"../../models/ANN_model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# loaded_model.predict(X_test[0].reshape((1, 512, 512, 1))).argmax(axis=1)\n",
    "# print(\"Loaded model from disk\")\n",
    "    \n",
    "    return final_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_df, i, model_name, model, X, y, X_test, y_test):\n",
    "    \"\"\"\n",
    "    this function is for regression takes the model with the data and calculate\n",
    "    the scores, with cross validation techniques, in addition to MAE, MSE, RMSE, MAPE\n",
    "    R Squared and Adjusted R Squared\n",
    "\n",
    "    :param model: model\n",
    "    :param X_train, X_test, y_train, y_test: data that was used\n",
    "    \"\"\"\n",
    "\n",
    "    # cross validation with 5 folds\n",
    "    all_cv_5 = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "    #print(\"all CV 5: {}\".format(all_cv_5))\n",
    "    # print(\"Mean Cross-Validation score: {}\".format(all_cv_5.mean()))\n",
    "\n",
    "    # predictions from our model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # calculating R squared and Adjusted R squared\n",
    "    r_sqre = r2_score(y_test, predictions)\n",
    "    n = len(y_test)\n",
    "    p = X_test.shape[1] # number of independant features\n",
    "\n",
    "    Adj_r2 = 1 - ((1 - r_sqre) * (n - 1) / (n - 1 - p))\n",
    "    \n",
    "    test_mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    test_mse = mean_squared_error(y_test, predictions)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #print(\"=\" * 40)\n",
    "    model_df.loc[i] = [model_name, all_cv_5.mean(),\n",
    "                    test_mae, mean_absolute_percentage_error(y_test, predictions),\n",
    "                   test_mse, test_rmse, r_sqre, Adj_r2]\n",
    "\n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evalute(X_train, X_test, y_train, y_test, metric):\n",
    "    # scaler = StandardScaler()\n",
    "    # X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # print(\"Shape of X_train: {} \\t Shape of y_train: {}\".format(X_train.shape, y_train.shape))\n",
    "    # print(\"Shape of X_test: {} \\t Shape of y_test: {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "    # Building pipelins of standard scaler and model for varios regressors.\n",
    "\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    lasso = Lasso()\n",
    "\n",
    "    dt = DecisionTreeRegressor()\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    xgb = XGBRegressor()\n",
    "\n",
    "    gbr = GradientBoostingRegressor()\n",
    "\n",
    "    eln = ElasticNet()\n",
    "\n",
    "    br = BayesianRidge()\n",
    "\n",
    "    cat = CatBoostRegressor(allow_writing_files=False, verbose=0, task_type=\"GPU\")\n",
    "\n",
    "    lgbm = LGBMRegressor()\n",
    "\n",
    "\n",
    "\n",
    "    # List of all the pipelines\n",
    "    pipelines = [lr, lasso, dt, rf, xgb, gbr,\n",
    "                eln, br, cat, lgbm] # \n",
    "\n",
    "    # Dictionary of pipelines and model types for ease of reference\n",
    "    ml_dict = {0: \"LinearRegression\", 1: \"Lasso\", 2: \"DecisionTree\", 3: \"RandomForest\", 4: \"XGBRegressor\", 5: \"GradientBoostingRegressor\",\n",
    "                    6: \"Elastic Net\", 7:\"BayesianRidge\", 8: \"CatBoostRegressor\", 9: \"LGBMRegressor\"}\n",
    "        #, \n",
    "\n",
    "    models_scores_df = pd.DataFrame(columns=[\"model\", \"Mean CV\", \"MAE\",\n",
    "                                            \"MAPE\", \"MSE\", \"RMSE\", \"R_Squared\", \"Adjusted_R_Squared\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Fit the pipelines and display the scores with Cross validation\n",
    "    for i, pipe in enumerate(pipelines):\n",
    "        # getting the name of our model\n",
    "        model_name = ml_dict[i]\n",
    "        print(model_name)\n",
    "        \n",
    "        # fitting our data\n",
    "        pipe.fit(X_train, y_train)\n",
    "        \n",
    "        evaluate_model(models_scores_df, i, model_name, pipe, X, y, X_test, y_test)\n",
    "\n",
    "\n",
    "    # selecting top 3 score based on metric\n",
    "    filtered_models_scores_df =  models_scores_df.sort_values(metric).iloc[:3, :]\n",
    "\n",
    "    results_df = pd.DataFrame({\"actual\": y_test})\n",
    "    selected_compounds = compound_smile.iloc[results_df.index]\n",
    "    results_df['Compound'] = selected_compounds['Compound']\n",
    "    results_df['SMILES'] = selected_compounds['SMILES']\n",
    "\n",
    "    results_df = results_df.reset_index(drop=True)\n",
    "    li = []\n",
    "\n",
    "    for i in filtered_models_scores_df.index:\n",
    "        predictions = pipelines[i].predict(X_test)\n",
    "        \n",
    "        predictions_df = pd.DataFrame({\"predictions_{}\".format(ml_dict[i]): predictions})\n",
    "        li.append(predictions_df)\n",
    "        plot_feature_importance(pipelines[i],  X.columns, ml_dict[i])\n",
    "\n",
    "        # save the model to disk\n",
    "        filename = '../../models/{}_model.sav'.format(ml_dict[i])\n",
    "        pickle.dump(pipelines[i], open(filename, 'wb'))\n",
    "\n",
    "    li_df = pd.concat(li, axis=1)\n",
    "\n",
    "    return models_scores_df, pd.concat([results_df, li_df], axis=1)\n",
    "# load the model from disk\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "Lasso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.451e+00, tolerance: 5.008e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.813e-01, tolerance: 3.921e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.423e-01, tolerance: 4.703e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.702e-01, tolerance: 4.572e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "RandomForest\n",
      "XGBRegressor\n",
      "GradientBoostingRegressor\n",
      "Elastic Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.873e+00, tolerance: 4.524e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.355e+01, tolerance: 5.008e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+01, tolerance: 3.921e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+00, tolerance: 4.703e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+01, tolerance: 4.572e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianRidge\n",
      "CatBoostRegressor\n",
      "LGBMRegressor\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\..\\..\\results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_features, results_scores \u001b[39m=\u001b[39m train_and_evalute(X_train, X_test, y_train, y_test, metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m all_features\n",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m, in \u001b[0;36mtrain_and_evalute\u001b[1;34m(X_train, X_test, y_train, y_test, metric)\u001b[0m\n\u001b[0;32m     74\u001b[0m predictions_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mpredictions_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(ml_dict[i]): predictions})\n\u001b[0;32m     75\u001b[0m li\u001b[39m.\u001b[39mappend(predictions_df)\n\u001b[1;32m---> 76\u001b[0m plot_feature_importance(pipelines[i],  X\u001b[39m.\u001b[39;49mcolumns, ml_dict[i])\n\u001b[0;32m     78\u001b[0m \u001b[39m# save the model to disk\u001b[39;00m\n\u001b[0;32m     79\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../../models/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_model.sav\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(ml_dict[i])\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mplot_feature_importance\u001b[1;34m(model, cols, model_name, slice)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Sort the DataFrame by importance score\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(\u001b[39m\"\u001b[39m\u001b[39mimportance\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m df[:\u001b[39mslice\u001b[39;49m]\u001b[39m.\u001b[39;49mto_excel(\u001b[39m\"\u001b[39;49m\u001b[39m../../../results/feature_importance.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Create a bar plot using Seaborn\u001b[39;00m\n\u001b[0;32m     13\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m8\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\core\\generic.py:2374\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2361\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mformats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexcel\u001b[39;00m \u001b[39mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2363\u001b[0m formatter \u001b[39m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2364\u001b[0m     df,\n\u001b[0;32m   2365\u001b[0m     na_rep\u001b[39m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     inf_rep\u001b[39m=\u001b[39minf_rep,\n\u001b[0;32m   2373\u001b[0m )\n\u001b[1;32m-> 2374\u001b[0m formatter\u001b[39m.\u001b[39;49mwrite(\n\u001b[0;32m   2375\u001b[0m     excel_writer,\n\u001b[0;32m   2376\u001b[0m     sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[0;32m   2377\u001b[0m     startrow\u001b[39m=\u001b[39;49mstartrow,\n\u001b[0;32m   2378\u001b[0m     startcol\u001b[39m=\u001b[39;49mstartcol,\n\u001b[0;32m   2379\u001b[0m     freeze_panes\u001b[39m=\u001b[39;49mfreeze_panes,\n\u001b[0;32m   2380\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m   2381\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2382\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\io\\formats\\excel.py:918\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    914\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    916\u001b[0m     \u001b[39m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[0;32m    917\u001b[0m     \u001b[39m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     writer \u001b[39m=\u001b[39m ExcelWriter(  \u001b[39m# type: ignore[abstract]\u001b[39;49;00m\n\u001b[0;32m    919\u001b[0m         writer, engine\u001b[39m=\u001b[39;49mengine, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m    920\u001b[0m     )\n\u001b[0;32m    921\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:60\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenpyxl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkbook\u001b[39;00m \u001b[39mimport\u001b[39;00m Workbook\n\u001b[0;32m     58\u001b[0m engine_kwargs \u001b[39m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m---> 60\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     61\u001b[0m     path,\n\u001b[0;32m     62\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m     63\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m     64\u001b[0m     if_sheet_exists\u001b[39m=\u001b[39;49mif_sheet_exists,\n\u001b[0;32m     65\u001b[0m     engine_kwargs\u001b[39m=\u001b[39;49mengine_kwargs,\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[39m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m# the file and later write to it\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode:  \u001b[39m# Load from existing workbook\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1313\u001b[0m, in \u001b[0;36mExcelWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handles \u001b[39m=\u001b[39m IOHandles(\n\u001b[0;32m   1310\u001b[0m     cast(IO[\u001b[39mbytes\u001b[39m], path), compression\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m   1311\u001b[0m )\n\u001b[0;32m   1312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, ExcelWriter):\n\u001b[1;32m-> 1313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1314\u001b[0m         path, mode, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1315\u001b[0m     )\n\u001b[0;32m   1316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cur_sheet \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m date_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\io\\common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 734\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    737\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    738\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\ds\\lib\\site-packages\\pandas\\io\\common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    595\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '..\\..\\..\\results'"
     ]
    }
   ],
   "source": [
    "all_features, results_scores = train_and_evalute(X_train, X_test, y_train, y_test, metric=\"MAE\")\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>Compound</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>predictions_LGBMRegressor</th>\n",
       "      <th>predictions_GradientBoostingRegressor</th>\n",
       "      <th>predictions_CatBoostRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.30</td>\n",
       "      <td>butoxyethanol</td>\n",
       "      <td>CCCCOCCO</td>\n",
       "      <td>-2.255343</td>\n",
       "      <td>-2.315491</td>\n",
       "      <td>-2.260737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.65</td>\n",
       "      <td>Dimethyl phthalate</td>\n",
       "      <td>COC(=O)C1=CC=CC=C1C(=O)OC</td>\n",
       "      <td>-1.645769</td>\n",
       "      <td>-1.605519</td>\n",
       "      <td>-1.519364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.67</td>\n",
       "      <td>Corticosterone</td>\n",
       "      <td>CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O</td>\n",
       "      <td>-2.497803</td>\n",
       "      <td>-2.499318</td>\n",
       "      <td>-2.504789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.49</td>\n",
       "      <td>Estradiol</td>\n",
       "      <td>CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O</td>\n",
       "      <td>-2.516031</td>\n",
       "      <td>-2.517663</td>\n",
       "      <td>-2.517964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.37</td>\n",
       "      <td>Corticosterone</td>\n",
       "      <td>CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O</td>\n",
       "      <td>-2.497803</td>\n",
       "      <td>-2.499318</td>\n",
       "      <td>-2.504789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-2.80</td>\n",
       "      <td>Methanol</td>\n",
       "      <td>CO</td>\n",
       "      <td>-3.143522</td>\n",
       "      <td>-3.147833</td>\n",
       "      <td>-3.229730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-2.38</td>\n",
       "      <td>Estradiol</td>\n",
       "      <td>CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O</td>\n",
       "      <td>-2.516031</td>\n",
       "      <td>-2.517663</td>\n",
       "      <td>-2.517964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-2.40</td>\n",
       "      <td>Lidocaine</td>\n",
       "      <td>CCN(CC)CC(=O)NC1=C(C=CC=C1C)C</td>\n",
       "      <td>-2.506980</td>\n",
       "      <td>-2.398611</td>\n",
       "      <td>-2.519638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-1.21</td>\n",
       "      <td>Octanol</td>\n",
       "      <td>CCCCCCCCO</td>\n",
       "      <td>-1.173194</td>\n",
       "      <td>-1.245904</td>\n",
       "      <td>-1.191847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-1.55</td>\n",
       "      <td>Naphthalene</td>\n",
       "      <td>C1=CC=C2C=CC=CC2=C1</td>\n",
       "      <td>-1.597767</td>\n",
       "      <td>-1.623966</td>\n",
       "      <td>-1.770463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual            Compound                                        SMILES  \\\n",
       "0     -2.30       butoxyethanol                                      CCCCOCCO   \n",
       "1     -1.65  Dimethyl phthalate                     COC(=O)C1=CC=CC=C1C(=O)OC   \n",
       "2     -2.67      Corticosterone  CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O   \n",
       "3     -2.49           Estradiol           CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O   \n",
       "4     -2.37      Corticosterone  CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O   \n",
       "..      ...                 ...                                           ...   \n",
       "108   -2.80            Methanol                                            CO   \n",
       "109   -2.38           Estradiol           CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O   \n",
       "110   -2.40           Lidocaine                 CCN(CC)CC(=O)NC1=C(C=CC=C1C)C   \n",
       "111   -1.21             Octanol                                     CCCCCCCCO   \n",
       "112   -1.55         Naphthalene                           C1=CC=C2C=CC=CC2=C1   \n",
       "\n",
       "     predictions_LGBMRegressor  predictions_GradientBoostingRegressor  \\\n",
       "0                    -2.255343                              -2.315491   \n",
       "1                    -1.645769                              -1.605519   \n",
       "2                    -2.497803                              -2.499318   \n",
       "3                    -2.516031                              -2.517663   \n",
       "4                    -2.497803                              -2.499318   \n",
       "..                         ...                                    ...   \n",
       "108                  -3.143522                              -3.147833   \n",
       "109                  -2.516031                              -2.517663   \n",
       "110                  -2.506980                              -2.398611   \n",
       "111                  -1.173194                              -1.245904   \n",
       "112                  -1.597767                              -1.623966   \n",
       "\n",
       "     predictions_CatBoostRegressor  \n",
       "0                        -2.260737  \n",
       "1                        -1.519364  \n",
       "2                        -2.504789  \n",
       "3                        -2.517964  \n",
       "4                        -2.504789  \n",
       "..                             ...  \n",
       "108                      -3.229730  \n",
       "109                      -2.517964  \n",
       "110                      -2.519638  \n",
       "111                      -1.191847  \n",
       "112                      -1.770463  \n",
       "\n",
       "[113 rows x 6 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2500\n",
      "11/11 - 0s - loss: 3.0685 - val_loss: 2.5540 - 414ms/epoch - 38ms/step\n",
      "Epoch 2/2500\n",
      "11/11 - 0s - loss: 2.7891 - val_loss: 2.5047 - 37ms/epoch - 3ms/step\n",
      "Epoch 3/2500\n",
      "11/11 - 0s - loss: 2.6928 - val_loss: 2.4570 - 37ms/epoch - 3ms/step\n",
      "Epoch 4/2500\n",
      "11/11 - 0s - loss: 2.6467 - val_loss: 2.3935 - 35ms/epoch - 3ms/step\n",
      "Epoch 5/2500\n",
      "11/11 - 0s - loss: 2.7582 - val_loss: 2.3503 - 36ms/epoch - 3ms/step\n",
      "Epoch 6/2500\n",
      "11/11 - 0s - loss: 2.4848 - val_loss: 2.3080 - 36ms/epoch - 3ms/step\n",
      "Epoch 7/2500\n",
      "11/11 - 0s - loss: 2.4944 - val_loss: 2.2618 - 36ms/epoch - 3ms/step\n",
      "Epoch 8/2500\n",
      "11/11 - 0s - loss: 2.3943 - val_loss: 2.2150 - 36ms/epoch - 3ms/step\n",
      "Epoch 9/2500\n",
      "11/11 - 0s - loss: 2.2941 - val_loss: 2.1558 - 35ms/epoch - 3ms/step\n",
      "Epoch 10/2500\n",
      "11/11 - 0s - loss: 2.2774 - val_loss: 2.0980 - 36ms/epoch - 3ms/step\n",
      "Epoch 11/2500\n",
      "11/11 - 0s - loss: 2.3618 - val_loss: 2.0560 - 32ms/epoch - 3ms/step\n",
      "Epoch 12/2500\n",
      "11/11 - 0s - loss: 2.3822 - val_loss: 2.0412 - 37ms/epoch - 3ms/step\n",
      "Epoch 13/2500\n",
      "11/11 - 0s - loss: 2.2115 - val_loss: 1.9957 - 36ms/epoch - 3ms/step\n",
      "Epoch 14/2500\n",
      "11/11 - 0s - loss: 1.9314 - val_loss: 1.9482 - 35ms/epoch - 3ms/step\n",
      "Epoch 15/2500\n",
      "11/11 - 0s - loss: 2.0567 - val_loss: 1.9103 - 35ms/epoch - 3ms/step\n",
      "Epoch 16/2500\n",
      "11/11 - 0s - loss: 2.0262 - val_loss: 1.8629 - 36ms/epoch - 3ms/step\n",
      "Epoch 17/2500\n",
      "11/11 - 0s - loss: 1.9798 - val_loss: 1.8182 - 32ms/epoch - 3ms/step\n",
      "Epoch 18/2500\n",
      "11/11 - 0s - loss: 2.0451 - val_loss: 1.7588 - 32ms/epoch - 3ms/step\n",
      "Epoch 19/2500\n",
      "11/11 - 0s - loss: 2.0331 - val_loss: 1.6976 - 32ms/epoch - 3ms/step\n",
      "Epoch 20/2500\n",
      "11/11 - 0s - loss: 1.8883 - val_loss: 1.6518 - 32ms/epoch - 3ms/step\n",
      "Epoch 21/2500\n",
      "11/11 - 0s - loss: 1.8300 - val_loss: 1.5886 - 33ms/epoch - 3ms/step\n",
      "Epoch 22/2500\n",
      "11/11 - 0s - loss: 1.7718 - val_loss: 1.5281 - 32ms/epoch - 3ms/step\n",
      "Epoch 23/2500\n",
      "11/11 - 0s - loss: 1.8094 - val_loss: 1.4760 - 42ms/epoch - 4ms/step\n",
      "Epoch 24/2500\n",
      "11/11 - 0s - loss: 1.7049 - val_loss: 1.4500 - 38ms/epoch - 3ms/step\n",
      "Epoch 25/2500\n",
      "11/11 - 0s - loss: 1.6726 - val_loss: 1.4088 - 38ms/epoch - 3ms/step\n",
      "Epoch 26/2500\n",
      "11/11 - 0s - loss: 1.6228 - val_loss: 1.3608 - 33ms/epoch - 3ms/step\n",
      "Epoch 27/2500\n",
      "11/11 - 0s - loss: 1.5323 - val_loss: 1.3103 - 36ms/epoch - 3ms/step\n",
      "Epoch 28/2500\n",
      "11/11 - 0s - loss: 1.4838 - val_loss: 1.2516 - 38ms/epoch - 3ms/step\n",
      "Epoch 29/2500\n",
      "11/11 - 0s - loss: 1.6255 - val_loss: 1.1911 - 35ms/epoch - 3ms/step\n",
      "Epoch 30/2500\n",
      "11/11 - 0s - loss: 1.3830 - val_loss: 1.1251 - 36ms/epoch - 3ms/step\n",
      "Epoch 31/2500\n",
      "11/11 - 0s - loss: 1.5379 - val_loss: 1.1157 - 36ms/epoch - 3ms/step\n",
      "Epoch 32/2500\n",
      "11/11 - 0s - loss: 1.4948 - val_loss: 1.1284 - 35ms/epoch - 3ms/step\n",
      "Epoch 33/2500\n",
      "11/11 - 0s - loss: 1.5396 - val_loss: 1.0565 - 34ms/epoch - 3ms/step\n",
      "Epoch 34/2500\n",
      "11/11 - 0s - loss: 1.3492 - val_loss: 1.0206 - 36ms/epoch - 3ms/step\n",
      "Epoch 35/2500\n",
      "11/11 - 0s - loss: 1.4501 - val_loss: 1.0100 - 34ms/epoch - 3ms/step\n",
      "Epoch 36/2500\n",
      "11/11 - 0s - loss: 1.3997 - val_loss: 0.9783 - 35ms/epoch - 3ms/step\n",
      "Epoch 37/2500\n",
      "11/11 - 0s - loss: 1.3003 - val_loss: 0.9099 - 34ms/epoch - 3ms/step\n",
      "Epoch 38/2500\n",
      "11/11 - 0s - loss: 1.3655 - val_loss: 0.9092 - 34ms/epoch - 3ms/step\n",
      "Epoch 39/2500\n",
      "11/11 - 0s - loss: 1.3838 - val_loss: 0.9431 - 34ms/epoch - 3ms/step\n",
      "Epoch 40/2500\n",
      "11/11 - 0s - loss: 1.2910 - val_loss: 0.9296 - 34ms/epoch - 3ms/step\n",
      "Epoch 41/2500\n",
      "11/11 - 0s - loss: 1.2719 - val_loss: 0.8678 - 34ms/epoch - 3ms/step\n",
      "Epoch 42/2500\n",
      "11/11 - 0s - loss: 1.2795 - val_loss: 0.8297 - 34ms/epoch - 3ms/step\n",
      "Epoch 43/2500\n",
      "11/11 - 0s - loss: 1.3237 - val_loss: 0.7892 - 33ms/epoch - 3ms/step\n",
      "Epoch 44/2500\n",
      "11/11 - 0s - loss: 1.2593 - val_loss: 0.7751 - 33ms/epoch - 3ms/step\n",
      "Epoch 45/2500\n",
      "11/11 - 0s - loss: 1.1854 - val_loss: 0.7667 - 32ms/epoch - 3ms/step\n",
      "Epoch 46/2500\n",
      "11/11 - 0s - loss: 1.2174 - val_loss: 0.7451 - 34ms/epoch - 3ms/step\n",
      "Epoch 47/2500\n",
      "11/11 - 0s - loss: 1.1474 - val_loss: 0.7378 - 33ms/epoch - 3ms/step\n",
      "Epoch 48/2500\n",
      "11/11 - 0s - loss: 1.1576 - val_loss: 0.7445 - 33ms/epoch - 3ms/step\n",
      "Epoch 49/2500\n",
      "11/11 - 0s - loss: 1.1655 - val_loss: 0.7177 - 32ms/epoch - 3ms/step\n",
      "Epoch 50/2500\n",
      "11/11 - 0s - loss: 1.2649 - val_loss: 0.7261 - 33ms/epoch - 3ms/step\n",
      "Epoch 51/2500\n",
      "11/11 - 0s - loss: 1.1684 - val_loss: 0.7529 - 33ms/epoch - 3ms/step\n",
      "Epoch 52/2500\n",
      "11/11 - 0s - loss: 1.0444 - val_loss: 0.7436 - 39ms/epoch - 4ms/step\n",
      "Epoch 53/2500\n",
      "11/11 - 0s - loss: 1.0813 - val_loss: 0.7128 - 41ms/epoch - 4ms/step\n",
      "Epoch 54/2500\n",
      "11/11 - 0s - loss: 1.0804 - val_loss: 0.7267 - 34ms/epoch - 3ms/step\n",
      "Epoch 55/2500\n",
      "11/11 - 0s - loss: 1.0709 - val_loss: 0.6839 - 35ms/epoch - 3ms/step\n",
      "Epoch 56/2500\n",
      "11/11 - 0s - loss: 1.1598 - val_loss: 0.6508 - 34ms/epoch - 3ms/step\n",
      "Epoch 57/2500\n",
      "11/11 - 0s - loss: 1.1480 - val_loss: 0.6694 - 35ms/epoch - 3ms/step\n",
      "Epoch 58/2500\n",
      "11/11 - 0s - loss: 1.0822 - val_loss: 0.6552 - 36ms/epoch - 3ms/step\n",
      "Epoch 59/2500\n",
      "11/11 - 0s - loss: 1.0933 - val_loss: 0.6363 - 34ms/epoch - 3ms/step\n",
      "Epoch 60/2500\n",
      "11/11 - 0s - loss: 1.1056 - val_loss: 0.6245 - 34ms/epoch - 3ms/step\n",
      "Epoch 61/2500\n",
      "11/11 - 0s - loss: 1.0230 - val_loss: 0.6284 - 32ms/epoch - 3ms/step\n",
      "Epoch 62/2500\n",
      "11/11 - 0s - loss: 1.0429 - val_loss: 0.6503 - 32ms/epoch - 3ms/step\n",
      "Epoch 63/2500\n",
      "11/11 - 0s - loss: 0.9991 - val_loss: 0.6330 - 32ms/epoch - 3ms/step\n",
      "Epoch 64/2500\n",
      "11/11 - 0s - loss: 0.9902 - val_loss: 0.6113 - 32ms/epoch - 3ms/step\n",
      "Epoch 65/2500\n",
      "11/11 - 0s - loss: 1.0667 - val_loss: 0.6312 - 32ms/epoch - 3ms/step\n",
      "Epoch 66/2500\n",
      "11/11 - 0s - loss: 1.0675 - val_loss: 0.6226 - 32ms/epoch - 3ms/step\n",
      "Epoch 67/2500\n",
      "11/11 - 0s - loss: 1.0137 - val_loss: 0.6279 - 32ms/epoch - 3ms/step\n",
      "Epoch 68/2500\n",
      "11/11 - 0s - loss: 1.0258 - val_loss: 0.6430 - 32ms/epoch - 3ms/step\n",
      "Epoch 69/2500\n",
      "11/11 - 0s - loss: 1.0405 - val_loss: 0.6013 - 31ms/epoch - 3ms/step\n",
      "Epoch 70/2500\n",
      "11/11 - 0s - loss: 1.0488 - val_loss: 0.5894 - 32ms/epoch - 3ms/step\n",
      "Epoch 71/2500\n",
      "11/11 - 0s - loss: 0.9643 - val_loss: 0.6620 - 32ms/epoch - 3ms/step\n",
      "Epoch 72/2500\n",
      "11/11 - 0s - loss: 1.0065 - val_loss: 0.6137 - 32ms/epoch - 3ms/step\n",
      "Epoch 73/2500\n",
      "11/11 - 0s - loss: 0.9929 - val_loss: 0.6251 - 31ms/epoch - 3ms/step\n",
      "Epoch 74/2500\n",
      "11/11 - 0s - loss: 0.9128 - val_loss: 0.6446 - 32ms/epoch - 3ms/step\n",
      "Epoch 75/2500\n",
      "11/11 - 0s - loss: 0.9437 - val_loss: 0.6153 - 32ms/epoch - 3ms/step\n",
      "Epoch 76/2500\n",
      "11/11 - 0s - loss: 1.0029 - val_loss: 0.6106 - 32ms/epoch - 3ms/step\n",
      "Epoch 77/2500\n",
      "11/11 - 0s - loss: 0.9970 - val_loss: 0.6181 - 32ms/epoch - 3ms/step\n",
      "Epoch 78/2500\n",
      "11/11 - 0s - loss: 0.9694 - val_loss: 0.5985 - 36ms/epoch - 3ms/step\n",
      "Epoch 79/2500\n",
      "11/11 - 0s - loss: 0.8964 - val_loss: 0.5963 - 31ms/epoch - 3ms/step\n",
      "Epoch 80/2500\n",
      "11/11 - 0s - loss: 0.9668 - val_loss: 0.5871 - 32ms/epoch - 3ms/step\n",
      "Epoch 81/2500\n",
      "11/11 - 0s - loss: 0.8788 - val_loss: 0.6122 - 32ms/epoch - 3ms/step\n",
      "Epoch 82/2500\n",
      "11/11 - 0s - loss: 0.8679 - val_loss: 0.5669 - 31ms/epoch - 3ms/step\n",
      "Epoch 83/2500\n",
      "11/11 - 0s - loss: 0.8444 - val_loss: 0.5620 - 32ms/epoch - 3ms/step\n",
      "Epoch 84/2500\n",
      "11/11 - 0s - loss: 0.8920 - val_loss: 0.5766 - 38ms/epoch - 3ms/step\n",
      "Epoch 85/2500\n",
      "11/11 - 0s - loss: 0.8159 - val_loss: 0.5887 - 35ms/epoch - 3ms/step\n",
      "Epoch 86/2500\n",
      "11/11 - 0s - loss: 0.9161 - val_loss: 0.6186 - 34ms/epoch - 3ms/step\n",
      "Epoch 87/2500\n",
      "11/11 - 0s - loss: 0.9143 - val_loss: 0.5561 - 33ms/epoch - 3ms/step\n",
      "Epoch 88/2500\n",
      "11/11 - 0s - loss: 0.9381 - val_loss: 0.5616 - 34ms/epoch - 3ms/step\n",
      "Epoch 89/2500\n",
      "11/11 - 0s - loss: 0.9187 - val_loss: 0.5613 - 32ms/epoch - 3ms/step\n",
      "Epoch 90/2500\n",
      "11/11 - 0s - loss: 0.9127 - val_loss: 0.5827 - 45ms/epoch - 4ms/step\n",
      "Epoch 91/2500\n",
      "11/11 - 0s - loss: 0.9324 - val_loss: 0.5754 - 34ms/epoch - 3ms/step\n",
      "Epoch 92/2500\n",
      "11/11 - 0s - loss: 0.9082 - val_loss: 0.5782 - 33ms/epoch - 3ms/step\n",
      "Epoch 93/2500\n",
      "11/11 - 0s - loss: 0.8794 - val_loss: 0.5702 - 32ms/epoch - 3ms/step\n",
      "Epoch 94/2500\n",
      "11/11 - 0s - loss: 0.8817 - val_loss: 0.5473 - 31ms/epoch - 3ms/step\n",
      "Epoch 95/2500\n",
      "11/11 - 0s - loss: 0.8617 - val_loss: 0.5443 - 32ms/epoch - 3ms/step\n",
      "Epoch 96/2500\n",
      "11/11 - 0s - loss: 0.8522 - val_loss: 0.5325 - 31ms/epoch - 3ms/step\n",
      "Epoch 97/2500\n",
      "11/11 - 0s - loss: 0.8324 - val_loss: 0.5330 - 32ms/epoch - 3ms/step\n",
      "Epoch 98/2500\n",
      "11/11 - 0s - loss: 0.8494 - val_loss: 0.5540 - 34ms/epoch - 3ms/step\n",
      "Epoch 99/2500\n",
      "11/11 - 0s - loss: 0.7977 - val_loss: 0.5723 - 34ms/epoch - 3ms/step\n",
      "Epoch 100/2500\n",
      "11/11 - 0s - loss: 0.8581 - val_loss: 0.5518 - 32ms/epoch - 3ms/step\n",
      "Epoch 101/2500\n",
      "11/11 - 0s - loss: 0.8570 - val_loss: 0.5347 - 33ms/epoch - 3ms/step\n",
      "Epoch 102/2500\n",
      "11/11 - 0s - loss: 0.8367 - val_loss: 0.5282 - 32ms/epoch - 3ms/step\n",
      "Epoch 103/2500\n",
      "11/11 - 0s - loss: 0.8674 - val_loss: 0.5269 - 32ms/epoch - 3ms/step\n",
      "Epoch 104/2500\n",
      "11/11 - 0s - loss: 0.8145 - val_loss: 0.5302 - 34ms/epoch - 3ms/step\n",
      "Epoch 105/2500\n",
      "11/11 - 0s - loss: 0.7706 - val_loss: 0.5254 - 33ms/epoch - 3ms/step\n",
      "Epoch 106/2500\n",
      "11/11 - 0s - loss: 0.8338 - val_loss: 0.5253 - 33ms/epoch - 3ms/step\n",
      "Epoch 107/2500\n",
      "11/11 - 0s - loss: 0.8109 - val_loss: 0.5346 - 33ms/epoch - 3ms/step\n",
      "Epoch 108/2500\n",
      "11/11 - 0s - loss: 0.8466 - val_loss: 0.5348 - 32ms/epoch - 3ms/step\n",
      "Epoch 109/2500\n",
      "11/11 - 0s - loss: 0.8001 - val_loss: 0.5065 - 33ms/epoch - 3ms/step\n",
      "Epoch 110/2500\n",
      "11/11 - 0s - loss: 0.8389 - val_loss: 0.4952 - 34ms/epoch - 3ms/step\n",
      "Epoch 111/2500\n",
      "11/11 - 0s - loss: 0.7507 - val_loss: 0.4996 - 33ms/epoch - 3ms/step\n",
      "Epoch 112/2500\n",
      "11/11 - 0s - loss: 0.8708 - val_loss: 0.5148 - 32ms/epoch - 3ms/step\n",
      "Epoch 113/2500\n",
      "11/11 - 0s - loss: 0.8680 - val_loss: 0.5152 - 32ms/epoch - 3ms/step\n",
      "Epoch 114/2500\n",
      "11/11 - 0s - loss: 0.8694 - val_loss: 0.5361 - 31ms/epoch - 3ms/step\n",
      "Epoch 115/2500\n",
      "11/11 - 0s - loss: 0.7639 - val_loss: 0.5432 - 31ms/epoch - 3ms/step\n",
      "Epoch 116/2500\n",
      "11/11 - 0s - loss: 0.7722 - val_loss: 0.5320 - 33ms/epoch - 3ms/step\n",
      "Epoch 117/2500\n",
      "11/11 - 0s - loss: 0.7834 - val_loss: 0.5175 - 36ms/epoch - 3ms/step\n",
      "Epoch 118/2500\n",
      "11/11 - 0s - loss: 0.8193 - val_loss: 0.5006 - 32ms/epoch - 3ms/step\n",
      "Epoch 119/2500\n",
      "11/11 - 0s - loss: 0.8011 - val_loss: 0.4890 - 33ms/epoch - 3ms/step\n",
      "Epoch 120/2500\n",
      "11/11 - 0s - loss: 0.7932 - val_loss: 0.5028 - 34ms/epoch - 3ms/step\n",
      "Epoch 121/2500\n",
      "11/11 - 0s - loss: 0.7848 - val_loss: 0.5476 - 34ms/epoch - 3ms/step\n",
      "Epoch 122/2500\n",
      "11/11 - 0s - loss: 0.7792 - val_loss: 0.5153 - 36ms/epoch - 3ms/step\n",
      "Epoch 123/2500\n",
      "11/11 - 0s - loss: 0.8117 - val_loss: 0.5159 - 38ms/epoch - 3ms/step\n",
      "Epoch 124/2500\n",
      "11/11 - 0s - loss: 0.7748 - val_loss: 0.5060 - 36ms/epoch - 3ms/step\n",
      "Epoch 125/2500\n",
      "11/11 - 0s - loss: 0.6995 - val_loss: 0.5235 - 34ms/epoch - 3ms/step\n",
      "Epoch 126/2500\n",
      "11/11 - 0s - loss: 0.7698 - val_loss: 0.5348 - 32ms/epoch - 3ms/step\n",
      "Epoch 127/2500\n",
      "11/11 - 0s - loss: 0.8047 - val_loss: 0.5557 - 32ms/epoch - 3ms/step\n",
      "Epoch 128/2500\n",
      "11/11 - 0s - loss: 0.7668 - val_loss: 0.5117 - 50ms/epoch - 5ms/step\n",
      "Epoch 129/2500\n",
      "11/11 - 0s - loss: 0.7611 - val_loss: 0.4921 - 35ms/epoch - 3ms/step\n",
      "Epoch 130/2500\n",
      "11/11 - 0s - loss: 0.7522 - val_loss: 0.4842 - 35ms/epoch - 3ms/step\n",
      "Epoch 131/2500\n",
      "11/11 - 0s - loss: 0.8095 - val_loss: 0.4738 - 34ms/epoch - 3ms/step\n",
      "Epoch 132/2500\n",
      "11/11 - 0s - loss: 0.7431 - val_loss: 0.4733 - 34ms/epoch - 3ms/step\n",
      "Epoch 133/2500\n",
      "11/11 - 0s - loss: 0.7445 - val_loss: 0.4802 - 38ms/epoch - 3ms/step\n",
      "Epoch 134/2500\n",
      "11/11 - 0s - loss: 0.7693 - val_loss: 0.4822 - 34ms/epoch - 3ms/step\n",
      "Epoch 135/2500\n",
      "11/11 - 0s - loss: 0.7633 - val_loss: 0.4933 - 34ms/epoch - 3ms/step\n",
      "Epoch 136/2500\n",
      "11/11 - 0s - loss: 0.7849 - val_loss: 0.5016 - 36ms/epoch - 3ms/step\n",
      "Epoch 137/2500\n",
      "11/11 - 0s - loss: 0.7969 - val_loss: 0.5002 - 33ms/epoch - 3ms/step\n",
      "Epoch 138/2500\n",
      "11/11 - 0s - loss: 0.7169 - val_loss: 0.4951 - 32ms/epoch - 3ms/step\n",
      "Epoch 139/2500\n",
      "11/11 - 0s - loss: 0.7625 - val_loss: 0.5164 - 32ms/epoch - 3ms/step\n",
      "Epoch 140/2500\n",
      "11/11 - 0s - loss: 0.7433 - val_loss: 0.5167 - 32ms/epoch - 3ms/step\n",
      "Epoch 141/2500\n",
      "11/11 - 0s - loss: 0.7378 - val_loss: 0.5045 - 33ms/epoch - 3ms/step\n",
      "Epoch 142/2500\n",
      "11/11 - 0s - loss: 0.8176 - val_loss: 0.4940 - 32ms/epoch - 3ms/step\n",
      "Epoch 143/2500\n",
      "11/11 - 0s - loss: 0.7710 - val_loss: 0.4914 - 33ms/epoch - 3ms/step\n",
      "Epoch 144/2500\n",
      "11/11 - 0s - loss: 0.7719 - val_loss: 0.4917 - 38ms/epoch - 3ms/step\n",
      "Epoch 145/2500\n",
      "11/11 - 0s - loss: 0.7034 - val_loss: 0.4918 - 34ms/epoch - 3ms/step\n",
      "Epoch 146/2500\n",
      "11/11 - 0s - loss: 0.7309 - val_loss: 0.4870 - 33ms/epoch - 3ms/step\n",
      "Epoch 147/2500\n",
      "11/11 - 0s - loss: 0.7582 - val_loss: 0.4898 - 33ms/epoch - 3ms/step\n",
      "Epoch 148/2500\n",
      "11/11 - 0s - loss: 0.6894 - val_loss: 0.4914 - 30ms/epoch - 3ms/step\n",
      "Epoch 149/2500\n",
      "11/11 - 0s - loss: 0.7491 - val_loss: 0.4870 - 30ms/epoch - 3ms/step\n",
      "Epoch 150/2500\n",
      "11/11 - 0s - loss: 0.7259 - val_loss: 0.4911 - 30ms/epoch - 3ms/step\n",
      "Epoch 151/2500\n",
      "11/11 - 0s - loss: 0.6968 - val_loss: 0.4936 - 32ms/epoch - 3ms/step\n",
      "Epoch 152/2500\n",
      "11/11 - 0s - loss: 0.7680 - val_loss: 0.5026 - 32ms/epoch - 3ms/step\n",
      "Epoch 153/2500\n",
      "11/11 - 0s - loss: 0.6921 - val_loss: 0.4960 - 34ms/epoch - 3ms/step\n",
      "Epoch 154/2500\n",
      "11/11 - 0s - loss: 0.7210 - val_loss: 0.4899 - 33ms/epoch - 3ms/step\n",
      "Epoch 155/2500\n",
      "11/11 - 0s - loss: 0.6621 - val_loss: 0.4859 - 32ms/epoch - 3ms/step\n",
      "Epoch 156/2500\n",
      "11/11 - 0s - loss: 0.6744 - val_loss: 0.4875 - 34ms/epoch - 3ms/step\n",
      "Epoch 157/2500\n",
      "11/11 - 0s - loss: 0.6832 - val_loss: 0.4823 - 33ms/epoch - 3ms/step\n",
      "Epoch 158/2500\n",
      "11/11 - 0s - loss: 0.6795 - val_loss: 0.4748 - 34ms/epoch - 3ms/step\n",
      "Epoch 159/2500\n",
      "11/11 - 0s - loss: 0.7069 - val_loss: 0.4646 - 33ms/epoch - 3ms/step\n",
      "Epoch 160/2500\n",
      "11/11 - 0s - loss: 0.7400 - val_loss: 0.4590 - 32ms/epoch - 3ms/step\n",
      "Epoch 161/2500\n",
      "11/11 - 0s - loss: 0.7260 - val_loss: 0.4645 - 32ms/epoch - 3ms/step\n",
      "Epoch 162/2500\n",
      "11/11 - 0s - loss: 0.7319 - val_loss: 0.4716 - 30ms/epoch - 3ms/step\n",
      "Epoch 163/2500\n",
      "11/11 - 0s - loss: 0.7039 - val_loss: 0.4932 - 41ms/epoch - 4ms/step\n",
      "Epoch 164/2500\n",
      "11/11 - 0s - loss: 0.7302 - val_loss: 0.4801 - 32ms/epoch - 3ms/step\n",
      "Epoch 165/2500\n",
      "11/11 - 0s - loss: 0.6855 - val_loss: 0.4864 - 31ms/epoch - 3ms/step\n",
      "Epoch 166/2500\n",
      "11/11 - 0s - loss: 0.7090 - val_loss: 0.4836 - 33ms/epoch - 3ms/step\n",
      "Epoch 167/2500\n",
      "11/11 - 0s - loss: 0.8046 - val_loss: 0.4799 - 35ms/epoch - 3ms/step\n",
      "Epoch 168/2500\n",
      "11/11 - 0s - loss: 0.6709 - val_loss: 0.4847 - 33ms/epoch - 3ms/step\n",
      "Epoch 169/2500\n",
      "11/11 - 0s - loss: 0.7429 - val_loss: 0.4978 - 33ms/epoch - 3ms/step\n",
      "Epoch 170/2500\n",
      "11/11 - 0s - loss: 0.7331 - val_loss: 0.5144 - 32ms/epoch - 3ms/step\n",
      "Epoch 171/2500\n",
      "11/11 - 0s - loss: 0.6518 - val_loss: 0.5100 - 32ms/epoch - 3ms/step\n",
      "Epoch 172/2500\n",
      "11/11 - 0s - loss: 0.6637 - val_loss: 0.4958 - 33ms/epoch - 3ms/step\n",
      "Epoch 173/2500\n",
      "11/11 - 0s - loss: 0.6341 - val_loss: 0.4868 - 37ms/epoch - 3ms/step\n",
      "Epoch 174/2500\n",
      "11/11 - 0s - loss: 0.6663 - val_loss: 0.4966 - 34ms/epoch - 3ms/step\n",
      "Epoch 175/2500\n",
      "11/11 - 0s - loss: 0.6623 - val_loss: 0.4997 - 32ms/epoch - 3ms/step\n",
      "Epoch 176/2500\n",
      "11/11 - 0s - loss: 0.7158 - val_loss: 0.4940 - 31ms/epoch - 3ms/step\n",
      "Epoch 177/2500\n",
      "11/11 - 0s - loss: 0.6686 - val_loss: 0.4878 - 30ms/epoch - 3ms/step\n",
      "Epoch 178/2500\n",
      "11/11 - 0s - loss: 0.6221 - val_loss: 0.4866 - 31ms/epoch - 3ms/step\n",
      "Epoch 179/2500\n",
      "11/11 - 0s - loss: 0.6376 - val_loss: 0.4959 - 30ms/epoch - 3ms/step\n",
      "Epoch 180/2500\n",
      "11/11 - 0s - loss: 0.6724 - val_loss: 0.4895 - 30ms/epoch - 3ms/step\n",
      "Epoch 181/2500\n",
      "11/11 - 0s - loss: 0.6902 - val_loss: 0.4909 - 33ms/epoch - 3ms/step\n",
      "Epoch 182/2500\n",
      "11/11 - 0s - loss: 0.6447 - val_loss: 0.4978 - 36ms/epoch - 3ms/step\n",
      "Epoch 183/2500\n",
      "11/11 - 0s - loss: 0.6849 - val_loss: 0.5016 - 36ms/epoch - 3ms/step\n",
      "Epoch 184/2500\n",
      "11/11 - 0s - loss: 0.6934 - val_loss: 0.5092 - 31ms/epoch - 3ms/step\n",
      "Epoch 185/2500\n",
      "11/11 - 0s - loss: 0.6924 - val_loss: 0.4920 - 31ms/epoch - 3ms/step\n",
      "Epoch 186/2500\n",
      "11/11 - 0s - loss: 0.6647 - val_loss: 0.4777 - 33ms/epoch - 3ms/step\n",
      "Epoch 187/2500\n",
      "11/11 - 0s - loss: 0.6624 - val_loss: 0.4850 - 32ms/epoch - 3ms/step\n",
      "Epoch 188/2500\n",
      "11/11 - 0s - loss: 0.7154 - val_loss: 0.4797 - 31ms/epoch - 3ms/step\n",
      "Epoch 189/2500\n",
      "11/11 - 0s - loss: 0.6731 - val_loss: 0.4692 - 32ms/epoch - 3ms/step\n",
      "Epoch 190/2500\n",
      "11/11 - 0s - loss: 0.7197 - val_loss: 0.4827 - 33ms/epoch - 3ms/step\n",
      "Epoch 191/2500\n",
      "11/11 - 0s - loss: 0.6939 - val_loss: 0.4767 - 33ms/epoch - 3ms/step\n",
      "Epoch 192/2500\n",
      "11/11 - 0s - loss: 0.6764 - val_loss: 0.4821 - 48ms/epoch - 4ms/step\n",
      "Epoch 193/2500\n",
      "11/11 - 0s - loss: 0.6585 - val_loss: 0.5138 - 33ms/epoch - 3ms/step\n",
      "Epoch 194/2500\n",
      "11/11 - 0s - loss: 0.6533 - val_loss: 0.4980 - 33ms/epoch - 3ms/step\n",
      "Epoch 195/2500\n",
      "11/11 - 0s - loss: 0.6658 - val_loss: 0.4945 - 31ms/epoch - 3ms/step\n",
      "Epoch 196/2500\n",
      "11/11 - 0s - loss: 0.6110 - val_loss: 0.4984 - 31ms/epoch - 3ms/step\n",
      "Epoch 197/2500\n",
      "11/11 - 0s - loss: 0.7217 - val_loss: 0.4952 - 31ms/epoch - 3ms/step\n",
      "Epoch 198/2500\n",
      "11/11 - 0s - loss: 0.6348 - val_loss: 0.4949 - 32ms/epoch - 3ms/step\n",
      "Epoch 199/2500\n",
      "11/11 - 0s - loss: 0.6503 - val_loss: 0.4867 - 33ms/epoch - 3ms/step\n",
      "Epoch 200/2500\n",
      "11/11 - 0s - loss: 0.6631 - val_loss: 0.4775 - 35ms/epoch - 3ms/step\n",
      "Epoch 201/2500\n",
      "11/11 - 0s - loss: 0.6709 - val_loss: 0.4747 - 37ms/epoch - 3ms/step\n",
      "Epoch 202/2500\n",
      "11/11 - 0s - loss: 0.6529 - val_loss: 0.4590 - 39ms/epoch - 4ms/step\n",
      "Epoch 203/2500\n",
      "11/11 - 0s - loss: 0.6464 - val_loss: 0.4691 - 34ms/epoch - 3ms/step\n",
      "Epoch 204/2500\n",
      "11/11 - 0s - loss: 0.6744 - val_loss: 0.4794 - 33ms/epoch - 3ms/step\n",
      "Epoch 205/2500\n",
      "11/11 - 0s - loss: 0.7188 - val_loss: 0.4826 - 35ms/epoch - 3ms/step\n",
      "Epoch 206/2500\n",
      "11/11 - 0s - loss: 0.6416 - val_loss: 0.4734 - 34ms/epoch - 3ms/step\n",
      "Epoch 207/2500\n",
      "11/11 - 0s - loss: 0.6557 - val_loss: 0.4753 - 33ms/epoch - 3ms/step\n",
      "Epoch 208/2500\n",
      "11/11 - 0s - loss: 0.6128 - val_loss: 0.4751 - 35ms/epoch - 3ms/step\n",
      "Epoch 209/2500\n",
      "11/11 - 0s - loss: 0.6216 - val_loss: 0.4764 - 37ms/epoch - 3ms/step\n",
      "Epoch 210/2500\n",
      "11/11 - 0s - loss: 0.6182 - val_loss: 0.4698 - 35ms/epoch - 3ms/step\n",
      "Epoch 211/2500\n",
      "11/11 - 0s - loss: 0.6528 - val_loss: 0.4760 - 36ms/epoch - 3ms/step\n",
      "Epoch 212/2500\n",
      "11/11 - 0s - loss: 0.6360 - val_loss: 0.4836 - 34ms/epoch - 3ms/step\n",
      "Epoch 213/2500\n",
      "11/11 - 0s - loss: 0.6790 - val_loss: 0.4704 - 35ms/epoch - 3ms/step\n",
      "Epoch 214/2500\n",
      "11/11 - 0s - loss: 0.6784 - val_loss: 0.4733 - 36ms/epoch - 3ms/step\n",
      "Epoch 215/2500\n",
      "11/11 - 0s - loss: 0.6283 - val_loss: 0.4719 - 34ms/epoch - 3ms/step\n",
      "Epoch 216/2500\n",
      "11/11 - 0s - loss: 0.5991 - val_loss: 0.4724 - 35ms/epoch - 3ms/step\n",
      "Epoch 217/2500\n",
      "11/11 - 0s - loss: 0.6097 - val_loss: 0.4970 - 36ms/epoch - 3ms/step\n",
      "Epoch 218/2500\n",
      "11/11 - 0s - loss: 0.6536 - val_loss: 0.4724 - 33ms/epoch - 3ms/step\n",
      "Epoch 219/2500\n",
      "11/11 - 0s - loss: 0.6354 - val_loss: 0.4783 - 31ms/epoch - 3ms/step\n",
      "Epoch 220/2500\n",
      "11/11 - 0s - loss: 0.6585 - val_loss: 0.4731 - 33ms/epoch - 3ms/step\n",
      "Epoch 221/2500\n",
      "11/11 - 0s - loss: 0.6367 - val_loss: 0.4698 - 33ms/epoch - 3ms/step\n",
      "Epoch 222/2500\n",
      "11/11 - 0s - loss: 0.6617 - val_loss: 0.4723 - 47ms/epoch - 4ms/step\n",
      "Epoch 223/2500\n",
      "11/11 - 0s - loss: 0.6224 - val_loss: 0.4790 - 33ms/epoch - 3ms/step\n",
      "Epoch 224/2500\n",
      "11/11 - 0s - loss: 0.6308 - val_loss: 0.4766 - 33ms/epoch - 3ms/step\n",
      "Epoch 225/2500\n",
      "11/11 - 0s - loss: 0.6229 - val_loss: 0.4774 - 34ms/epoch - 3ms/step\n",
      "Epoch 226/2500\n",
      "11/11 - 0s - loss: 0.6213 - val_loss: 0.4728 - 31ms/epoch - 3ms/step\n",
      "Epoch 227/2500\n",
      "11/11 - 0s - loss: 0.6358 - val_loss: 0.4746 - 34ms/epoch - 3ms/step\n",
      "Epoch 228/2500\n",
      "11/11 - 0s - loss: 0.6254 - val_loss: 0.5046 - 34ms/epoch - 3ms/step\n",
      "Epoch 229/2500\n",
      "11/11 - 0s - loss: 0.6400 - val_loss: 0.4804 - 33ms/epoch - 3ms/step\n",
      "Epoch 230/2500\n",
      "11/11 - 0s - loss: 0.5894 - val_loss: 0.4701 - 34ms/epoch - 3ms/step\n",
      "Epoch 231/2500\n",
      "11/11 - 0s - loss: 0.6216 - val_loss: 0.4714 - 34ms/epoch - 3ms/step\n",
      "Epoch 232/2500\n",
      "11/11 - 0s - loss: 0.6118 - val_loss: 0.4628 - 32ms/epoch - 3ms/step\n",
      "Epoch 233/2500\n",
      "11/11 - 0s - loss: 0.5924 - val_loss: 0.4714 - 32ms/epoch - 3ms/step\n",
      "Epoch 234/2500\n",
      "11/11 - 0s - loss: 0.6307 - val_loss: 0.4555 - 30ms/epoch - 3ms/step\n",
      "Epoch 235/2500\n",
      "11/11 - 0s - loss: 0.6536 - val_loss: 0.4568 - 31ms/epoch - 3ms/step\n",
      "Epoch 236/2500\n",
      "11/11 - 0s - loss: 0.6573 - val_loss: 0.4731 - 32ms/epoch - 3ms/step\n",
      "Epoch 237/2500\n",
      "11/11 - 0s - loss: 0.6769 - val_loss: 0.4703 - 32ms/epoch - 3ms/step\n",
      "Epoch 238/2500\n",
      "11/11 - 0s - loss: 0.6049 - val_loss: 0.4660 - 32ms/epoch - 3ms/step\n",
      "Epoch 239/2500\n",
      "11/11 - 0s - loss: 0.6451 - val_loss: 0.4649 - 34ms/epoch - 3ms/step\n",
      "Epoch 240/2500\n",
      "11/11 - 0s - loss: 0.6257 - val_loss: 0.4584 - 32ms/epoch - 3ms/step\n",
      "Epoch 241/2500\n",
      "11/11 - 0s - loss: 0.6266 - val_loss: 0.4446 - 30ms/epoch - 3ms/step\n",
      "Epoch 242/2500\n",
      "11/11 - 0s - loss: 0.6414 - val_loss: 0.4393 - 30ms/epoch - 3ms/step\n",
      "Epoch 243/2500\n",
      "11/11 - 0s - loss: 0.6379 - val_loss: 0.4437 - 31ms/epoch - 3ms/step\n",
      "Epoch 244/2500\n",
      "11/11 - 0s - loss: 0.6270 - val_loss: 0.4522 - 34ms/epoch - 3ms/step\n",
      "Epoch 245/2500\n",
      "11/11 - 0s - loss: 0.6612 - val_loss: 0.4543 - 33ms/epoch - 3ms/step\n",
      "Epoch 246/2500\n",
      "11/11 - 0s - loss: 0.6062 - val_loss: 0.4637 - 33ms/epoch - 3ms/step\n",
      "Epoch 247/2500\n",
      "11/11 - 0s - loss: 0.5881 - val_loss: 0.4688 - 31ms/epoch - 3ms/step\n",
      "Epoch 248/2500\n",
      "11/11 - 0s - loss: 0.6019 - val_loss: 0.4764 - 30ms/epoch - 3ms/step\n",
      "Epoch 249/2500\n",
      "11/11 - 0s - loss: 0.6159 - val_loss: 0.4835 - 30ms/epoch - 3ms/step\n",
      "Epoch 250/2500\n",
      "11/11 - 0s - loss: 0.6056 - val_loss: 0.4791 - 30ms/epoch - 3ms/step\n",
      "Epoch 251/2500\n",
      "11/11 - 0s - loss: 0.6111 - val_loss: 0.4794 - 30ms/epoch - 3ms/step\n",
      "Epoch 252/2500\n",
      "11/11 - 0s - loss: 0.6218 - val_loss: 0.4730 - 31ms/epoch - 3ms/step\n",
      "Epoch 253/2500\n",
      "11/11 - 0s - loss: 0.6608 - val_loss: 0.4700 - 31ms/epoch - 3ms/step\n",
      "Epoch 254/2500\n",
      "11/11 - 0s - loss: 0.5777 - val_loss: 0.4701 - 30ms/epoch - 3ms/step\n",
      "Epoch 255/2500\n",
      "11/11 - 0s - loss: 0.5775 - val_loss: 0.4638 - 32ms/epoch - 3ms/step\n",
      "Epoch 256/2500\n",
      "11/11 - 0s - loss: 0.6177 - val_loss: 0.4626 - 48ms/epoch - 4ms/step\n",
      "Epoch 257/2500\n",
      "11/11 - 0s - loss: 0.6099 - val_loss: 0.4708 - 33ms/epoch - 3ms/step\n",
      "Epoch 258/2500\n",
      "11/11 - 0s - loss: 0.5968 - val_loss: 0.4850 - 30ms/epoch - 3ms/step\n",
      "Epoch 259/2500\n",
      "11/11 - 0s - loss: 0.6075 - val_loss: 0.4671 - 30ms/epoch - 3ms/step\n",
      "Epoch 260/2500\n",
      "11/11 - 0s - loss: 0.5974 - val_loss: 0.4658 - 30ms/epoch - 3ms/step\n",
      "Epoch 261/2500\n",
      "11/11 - 0s - loss: 0.6188 - val_loss: 0.4721 - 32ms/epoch - 3ms/step\n",
      "Epoch 262/2500\n",
      "11/11 - 0s - loss: 0.6354 - val_loss: 0.4701 - 30ms/epoch - 3ms/step\n",
      "Epoch 263/2500\n",
      "11/11 - 0s - loss: 0.6463 - val_loss: 0.4719 - 32ms/epoch - 3ms/step\n",
      "Epoch 264/2500\n",
      "11/11 - 0s - loss: 0.6028 - val_loss: 0.4719 - 31ms/epoch - 3ms/step\n",
      "Epoch 265/2500\n",
      "11/11 - 0s - loss: 0.6012 - val_loss: 0.4693 - 31ms/epoch - 3ms/step\n",
      "Epoch 266/2500\n",
      "11/11 - 0s - loss: 0.6028 - val_loss: 0.4635 - 31ms/epoch - 3ms/step\n",
      "Epoch 267/2500\n",
      "11/11 - 0s - loss: 0.6129 - val_loss: 0.4698 - 30ms/epoch - 3ms/step\n",
      "Epoch 268/2500\n",
      "11/11 - 0s - loss: 0.6017 - val_loss: 0.4699 - 37ms/epoch - 3ms/step\n",
      "Epoch 269/2500\n",
      "11/11 - 0s - loss: 0.5704 - val_loss: 0.4681 - 33ms/epoch - 3ms/step\n",
      "Epoch 270/2500\n",
      "11/11 - 0s - loss: 0.5873 - val_loss: 0.4721 - 34ms/epoch - 3ms/step\n",
      "Epoch 271/2500\n",
      "11/11 - 0s - loss: 0.5559 - val_loss: 0.4645 - 33ms/epoch - 3ms/step\n",
      "Epoch 272/2500\n",
      "11/11 - 0s - loss: 0.5653 - val_loss: 0.4593 - 32ms/epoch - 3ms/step\n",
      "Epoch 273/2500\n",
      "11/11 - 0s - loss: 0.5711 - val_loss: 0.4572 - 34ms/epoch - 3ms/step\n",
      "Epoch 274/2500\n",
      "11/11 - 0s - loss: 0.5953 - val_loss: 0.4560 - 34ms/epoch - 3ms/step\n",
      "Epoch 275/2500\n",
      "11/11 - 0s - loss: 0.6041 - val_loss: 0.4677 - 39ms/epoch - 4ms/step\n",
      "Epoch 276/2500\n",
      "11/11 - 0s - loss: 0.5693 - val_loss: 0.4869 - 33ms/epoch - 3ms/step\n",
      "Epoch 277/2500\n",
      "11/11 - 0s - loss: 0.6128 - val_loss: 0.4757 - 33ms/epoch - 3ms/step\n",
      "Epoch 278/2500\n",
      "11/11 - 0s - loss: 0.5264 - val_loss: 0.4771 - 33ms/epoch - 3ms/step\n",
      "Epoch 279/2500\n",
      "11/11 - 0s - loss: 0.5901 - val_loss: 0.4768 - 32ms/epoch - 3ms/step\n",
      "Epoch 280/2500\n",
      "11/11 - 0s - loss: 0.5846 - val_loss: 0.4711 - 32ms/epoch - 3ms/step\n",
      "Epoch 281/2500\n",
      "11/11 - 0s - loss: 0.5860 - val_loss: 0.4651 - 31ms/epoch - 3ms/step\n",
      "Epoch 282/2500\n",
      "11/11 - 0s - loss: 0.6026 - val_loss: 0.4710 - 30ms/epoch - 3ms/step\n",
      "Epoch 283/2500\n",
      "11/11 - 0s - loss: 0.5947 - val_loss: 0.4693 - 31ms/epoch - 3ms/step\n",
      "Epoch 284/2500\n",
      "11/11 - 0s - loss: 0.5865 - val_loss: 0.4628 - 30ms/epoch - 3ms/step\n",
      "Epoch 285/2500\n",
      "11/11 - 0s - loss: 0.5813 - val_loss: 0.4564 - 31ms/epoch - 3ms/step\n",
      "Epoch 286/2500\n",
      "11/11 - 0s - loss: 0.6055 - val_loss: 0.4579 - 31ms/epoch - 3ms/step\n",
      "Epoch 287/2500\n",
      "11/11 - 0s - loss: 0.5750 - val_loss: 0.4709 - 30ms/epoch - 3ms/step\n",
      "Epoch 288/2500\n",
      "11/11 - 0s - loss: 0.6163 - val_loss: 0.4721 - 30ms/epoch - 3ms/step\n",
      "Epoch 289/2500\n",
      "11/11 - 0s - loss: 0.5544 - val_loss: 0.4686 - 31ms/epoch - 3ms/step\n",
      "Epoch 290/2500\n",
      "11/11 - 0s - loss: 0.5537 - val_loss: 0.4603 - 31ms/epoch - 3ms/step\n",
      "Epoch 291/2500\n",
      "11/11 - 0s - loss: 0.5496 - val_loss: 0.4553 - 31ms/epoch - 3ms/step\n",
      "Epoch 292/2500\n",
      "11/11 - 0s - loss: 0.5803 - val_loss: 0.4487 - 44ms/epoch - 4ms/step\n",
      "Epoch 293/2500\n",
      "11/11 - 0s - loss: 0.5661 - val_loss: 0.4571 - 30ms/epoch - 3ms/step\n",
      "Epoch 294/2500\n",
      "11/11 - 0s - loss: 0.5279 - val_loss: 0.4620 - 31ms/epoch - 3ms/step\n",
      "Epoch 295/2500\n",
      "11/11 - 0s - loss: 0.5543 - val_loss: 0.4639 - 38ms/epoch - 3ms/step\n",
      "Epoch 296/2500\n",
      "11/11 - 0s - loss: 0.6195 - val_loss: 0.4626 - 33ms/epoch - 3ms/step\n",
      "Epoch 297/2500\n",
      "11/11 - 0s - loss: 0.5851 - val_loss: 0.4603 - 30ms/epoch - 3ms/step\n",
      "Epoch 298/2500\n",
      "11/11 - 0s - loss: 0.5808 - val_loss: 0.4564 - 31ms/epoch - 3ms/step\n",
      "Epoch 299/2500\n",
      "11/11 - 0s - loss: 0.5861 - val_loss: 0.4562 - 32ms/epoch - 3ms/step\n",
      "Epoch 300/2500\n",
      "11/11 - 0s - loss: 0.5731 - val_loss: 0.4588 - 31ms/epoch - 3ms/step\n",
      "Epoch 301/2500\n",
      "11/11 - 0s - loss: 0.5734 - val_loss: 0.4562 - 32ms/epoch - 3ms/step\n",
      "Epoch 302/2500\n",
      "11/11 - 0s - loss: 0.5457 - val_loss: 0.4508 - 31ms/epoch - 3ms/step\n",
      "Epoch 303/2500\n",
      "11/11 - 0s - loss: 0.5988 - val_loss: 0.4552 - 33ms/epoch - 3ms/step\n",
      "Epoch 304/2500\n",
      "11/11 - 0s - loss: 0.5826 - val_loss: 0.4586 - 32ms/epoch - 3ms/step\n",
      "Epoch 305/2500\n",
      "11/11 - 0s - loss: 0.5850 - val_loss: 0.4626 - 31ms/epoch - 3ms/step\n",
      "Epoch 306/2500\n",
      "11/11 - 0s - loss: 0.5658 - val_loss: 0.4586 - 34ms/epoch - 3ms/step\n",
      "Epoch 307/2500\n",
      "11/11 - 0s - loss: 0.5442 - val_loss: 0.4578 - 31ms/epoch - 3ms/step\n",
      "Epoch 308/2500\n",
      "11/11 - 0s - loss: 0.5346 - val_loss: 0.4562 - 30ms/epoch - 3ms/step\n",
      "Epoch 309/2500\n",
      "11/11 - 0s - loss: 0.5968 - val_loss: 0.4744 - 30ms/epoch - 3ms/step\n",
      "Epoch 310/2500\n",
      "11/11 - 0s - loss: 0.5695 - val_loss: 0.4768 - 30ms/epoch - 3ms/step\n",
      "Epoch 311/2500\n",
      "11/11 - 0s - loss: 0.5758 - val_loss: 0.4735 - 30ms/epoch - 3ms/step\n",
      "Epoch 312/2500\n",
      "11/11 - 0s - loss: 0.5653 - val_loss: 0.4664 - 30ms/epoch - 3ms/step\n",
      "Epoch 313/2500\n",
      "11/11 - 0s - loss: 0.5438 - val_loss: 0.4577 - 30ms/epoch - 3ms/step\n",
      "Epoch 314/2500\n",
      "11/11 - 0s - loss: 0.5787 - val_loss: 0.4614 - 30ms/epoch - 3ms/step\n",
      "Epoch 315/2500\n",
      "11/11 - 0s - loss: 0.5673 - val_loss: 0.4605 - 30ms/epoch - 3ms/step\n",
      "Epoch 316/2500\n",
      "11/11 - 0s - loss: 0.6159 - val_loss: 0.4614 - 31ms/epoch - 3ms/step\n",
      "Epoch 317/2500\n",
      "11/11 - 0s - loss: 0.6094 - val_loss: 0.4547 - 31ms/epoch - 3ms/step\n",
      "Epoch 318/2500\n",
      "11/11 - 0s - loss: 0.5806 - val_loss: 0.4644 - 30ms/epoch - 3ms/step\n",
      "Epoch 319/2500\n",
      "11/11 - 0s - loss: 0.5646 - val_loss: 0.4696 - 30ms/epoch - 3ms/step\n",
      "Epoch 320/2500\n",
      "11/11 - 0s - loss: 0.5690 - val_loss: 0.4751 - 30ms/epoch - 3ms/step\n",
      "Epoch 321/2500\n",
      "11/11 - 0s - loss: 0.5741 - val_loss: 0.4750 - 30ms/epoch - 3ms/step\n",
      "Epoch 322/2500\n",
      "11/11 - 0s - loss: 0.5450 - val_loss: 0.4703 - 31ms/epoch - 3ms/step\n",
      "Epoch 323/2500\n",
      "11/11 - 0s - loss: 0.5829 - val_loss: 0.4659 - 31ms/epoch - 3ms/step\n",
      "Epoch 324/2500\n",
      "11/11 - 0s - loss: 0.5453 - val_loss: 0.4656 - 31ms/epoch - 3ms/step\n",
      "Epoch 325/2500\n",
      "11/11 - 0s - loss: 0.5547 - val_loss: 0.4774 - 46ms/epoch - 4ms/step\n",
      "Epoch 326/2500\n",
      "11/11 - 0s - loss: 0.5256 - val_loss: 0.4731 - 31ms/epoch - 3ms/step\n",
      "Epoch 327/2500\n",
      "11/11 - 0s - loss: 0.5334 - val_loss: 0.4722 - 31ms/epoch - 3ms/step\n",
      "Epoch 328/2500\n",
      "11/11 - 0s - loss: 0.5629 - val_loss: 0.4597 - 31ms/epoch - 3ms/step\n",
      "Epoch 329/2500\n",
      "11/11 - 0s - loss: 0.5046 - val_loss: 0.4606 - 31ms/epoch - 3ms/step\n",
      "Epoch 330/2500\n",
      "11/11 - 0s - loss: 0.5231 - val_loss: 0.4581 - 30ms/epoch - 3ms/step\n",
      "Epoch 331/2500\n",
      "11/11 - 0s - loss: 0.5794 - val_loss: 0.4579 - 30ms/epoch - 3ms/step\n",
      "Epoch 332/2500\n",
      "11/11 - 0s - loss: 0.5342 - val_loss: 0.4637 - 31ms/epoch - 3ms/step\n",
      "Epoch 333/2500\n",
      "11/11 - 0s - loss: 0.5742 - val_loss: 0.4586 - 30ms/epoch - 3ms/step\n",
      "Epoch 334/2500\n",
      "11/11 - 0s - loss: 0.5469 - val_loss: 0.4603 - 32ms/epoch - 3ms/step\n",
      "Epoch 335/2500\n",
      "11/11 - 0s - loss: 0.5445 - val_loss: 0.4423 - 30ms/epoch - 3ms/step\n",
      "Epoch 336/2500\n",
      "11/11 - 0s - loss: 0.5438 - val_loss: 0.4526 - 30ms/epoch - 3ms/step\n",
      "Epoch 337/2500\n",
      "11/11 - 0s - loss: 0.5241 - val_loss: 0.4535 - 30ms/epoch - 3ms/step\n",
      "Epoch 338/2500\n",
      "11/11 - 0s - loss: 0.5655 - val_loss: 0.4616 - 30ms/epoch - 3ms/step\n",
      "Epoch 339/2500\n",
      "11/11 - 0s - loss: 0.5321 - val_loss: 0.4573 - 30ms/epoch - 3ms/step\n",
      "Epoch 340/2500\n",
      "11/11 - 0s - loss: 0.5271 - val_loss: 0.4610 - 34ms/epoch - 3ms/step\n",
      "Epoch 341/2500\n",
      "11/11 - 0s - loss: 0.5905 - val_loss: 0.4631 - 31ms/epoch - 3ms/step\n",
      "Epoch 342/2500\n",
      "11/11 - 0s - loss: 0.5500 - val_loss: 0.4551 - 30ms/epoch - 3ms/step\n",
      "Epoch 343/2500\n",
      "11/11 - 0s - loss: 0.5515 - val_loss: 0.4595 - 30ms/epoch - 3ms/step\n",
      "Epoch 344/2500\n",
      "11/11 - 0s - loss: 0.5384 - val_loss: 0.4619 - 30ms/epoch - 3ms/step\n",
      "Epoch 345/2500\n",
      "11/11 - 0s - loss: 0.5661 - val_loss: 0.4552 - 30ms/epoch - 3ms/step\n",
      "Epoch 346/2500\n",
      "11/11 - 0s - loss: 0.5284 - val_loss: 0.4515 - 30ms/epoch - 3ms/step\n",
      "Epoch 347/2500\n",
      "11/11 - 0s - loss: 0.5229 - val_loss: 0.4603 - 32ms/epoch - 3ms/step\n",
      "Epoch 348/2500\n",
      "11/11 - 0s - loss: 0.5203 - val_loss: 0.4561 - 32ms/epoch - 3ms/step\n",
      "Epoch 349/2500\n",
      "11/11 - 0s - loss: 0.5254 - val_loss: 0.4618 - 33ms/epoch - 3ms/step\n",
      "Epoch 350/2500\n",
      "11/11 - 0s - loss: 0.5297 - val_loss: 0.4709 - 31ms/epoch - 3ms/step\n",
      "Epoch 351/2500\n",
      "11/11 - 0s - loss: 0.5560 - val_loss: 0.4696 - 32ms/epoch - 3ms/step\n",
      "Epoch 352/2500\n",
      "11/11 - 0s - loss: 0.5154 - val_loss: 0.4643 - 32ms/epoch - 3ms/step\n",
      "Epoch 353/2500\n",
      "11/11 - 0s - loss: 0.5462 - val_loss: 0.4499 - 33ms/epoch - 3ms/step\n",
      "Epoch 354/2500\n",
      "11/11 - 0s - loss: 0.5551 - val_loss: 0.4541 - 36ms/epoch - 3ms/step\n",
      "Epoch 355/2500\n",
      "11/11 - 0s - loss: 0.5456 - val_loss: 0.4538 - 41ms/epoch - 4ms/step\n",
      "Epoch 356/2500\n",
      "11/11 - 0s - loss: 0.5217 - val_loss: 0.4490 - 38ms/epoch - 3ms/step\n",
      "Epoch 357/2500\n",
      "11/11 - 0s - loss: 0.5612 - val_loss: 0.4506 - 32ms/epoch - 3ms/step\n",
      "Epoch 358/2500\n",
      "11/11 - 0s - loss: 0.5248 - val_loss: 0.4508 - 31ms/epoch - 3ms/step\n",
      "Epoch 359/2500\n",
      "11/11 - 0s - loss: 0.5253 - val_loss: 0.4539 - 31ms/epoch - 3ms/step\n",
      "Epoch 360/2500\n",
      "11/11 - 0s - loss: 0.5500 - val_loss: 0.4548 - 32ms/epoch - 3ms/step\n",
      "Epoch 361/2500\n",
      "11/11 - 0s - loss: 0.5724 - val_loss: 0.4564 - 32ms/epoch - 3ms/step\n",
      "Epoch 362/2500\n",
      "11/11 - 0s - loss: 0.5307 - val_loss: 0.4571 - 31ms/epoch - 3ms/step\n",
      "Epoch 363/2500\n",
      "11/11 - 0s - loss: 0.5326 - val_loss: 0.4554 - 30ms/epoch - 3ms/step\n",
      "Epoch 364/2500\n",
      "11/11 - 0s - loss: 0.5830 - val_loss: 0.4555 - 31ms/epoch - 3ms/step\n",
      "Epoch 365/2500\n",
      "11/11 - 0s - loss: 0.5521 - val_loss: 0.4565 - 30ms/epoch - 3ms/step\n",
      "Epoch 366/2500\n",
      "11/11 - 0s - loss: 0.5746 - val_loss: 0.4598 - 30ms/epoch - 3ms/step\n",
      "Epoch 367/2500\n",
      "11/11 - 0s - loss: 0.5233 - val_loss: 0.4565 - 30ms/epoch - 3ms/step\n",
      "Epoch 368/2500\n",
      "11/11 - 0s - loss: 0.5178 - val_loss: 0.4579 - 30ms/epoch - 3ms/step\n",
      "Epoch 369/2500\n",
      "11/11 - 0s - loss: 0.5333 - val_loss: 0.4605 - 31ms/epoch - 3ms/step\n",
      "Epoch 370/2500\n",
      "11/11 - 0s - loss: 0.5460 - val_loss: 0.4657 - 31ms/epoch - 3ms/step\n",
      "Epoch 371/2500\n",
      "11/11 - 0s - loss: 0.5316 - val_loss: 0.4602 - 34ms/epoch - 3ms/step\n",
      "Epoch 372/2500\n",
      "11/11 - 0s - loss: 0.5762 - val_loss: 0.4562 - 30ms/epoch - 3ms/step\n",
      "Epoch 373/2500\n",
      "11/11 - 0s - loss: 0.5170 - val_loss: 0.4584 - 31ms/epoch - 3ms/step\n",
      "Epoch 374/2500\n",
      "11/11 - 0s - loss: 0.5389 - val_loss: 0.4607 - 30ms/epoch - 3ms/step\n",
      "Epoch 375/2500\n",
      "11/11 - 0s - loss: 0.5371 - val_loss: 0.4561 - 31ms/epoch - 3ms/step\n",
      "Epoch 376/2500\n",
      "11/11 - 0s - loss: 0.5147 - val_loss: 0.4562 - 31ms/epoch - 3ms/step\n",
      "Epoch 377/2500\n",
      "11/11 - 0s - loss: 0.5467 - val_loss: 0.4556 - 31ms/epoch - 3ms/step\n",
      "Epoch 378/2500\n",
      "11/11 - 0s - loss: 0.5569 - val_loss: 0.4564 - 31ms/epoch - 3ms/step\n",
      "Epoch 379/2500\n",
      "11/11 - 0s - loss: 0.5298 - val_loss: 0.4602 - 30ms/epoch - 3ms/step\n",
      "Epoch 380/2500\n",
      "11/11 - 0s - loss: 0.5439 - val_loss: 0.4637 - 31ms/epoch - 3ms/step\n",
      "Epoch 381/2500\n",
      "11/11 - 0s - loss: 0.5537 - val_loss: 0.4599 - 30ms/epoch - 3ms/step\n",
      "Epoch 382/2500\n",
      "11/11 - 0s - loss: 0.5486 - val_loss: 0.4578 - 31ms/epoch - 3ms/step\n",
      "Epoch 383/2500\n",
      "11/11 - 0s - loss: 0.5426 - val_loss: 0.4556 - 36ms/epoch - 3ms/step\n",
      "Epoch 384/2500\n",
      "11/11 - 0s - loss: 0.5376 - val_loss: 0.4640 - 44ms/epoch - 4ms/step\n",
      "Epoch 385/2500\n",
      "11/11 - 0s - loss: 0.5263 - val_loss: 0.4788 - 36ms/epoch - 3ms/step\n",
      "Epoch 386/2500\n",
      "11/11 - 0s - loss: 0.5391 - val_loss: 0.4832 - 31ms/epoch - 3ms/step\n",
      "Epoch 387/2500\n",
      "11/11 - 0s - loss: 0.5419 - val_loss: 0.4762 - 31ms/epoch - 3ms/step\n",
      "Epoch 388/2500\n",
      "11/11 - 0s - loss: 0.5315 - val_loss: 0.4759 - 30ms/epoch - 3ms/step\n",
      "Epoch 389/2500\n",
      "11/11 - 0s - loss: 0.5002 - val_loss: 0.4824 - 31ms/epoch - 3ms/step\n",
      "Epoch 390/2500\n",
      "11/11 - 0s - loss: 0.5714 - val_loss: 0.4700 - 30ms/epoch - 3ms/step\n",
      "Epoch 391/2500\n",
      "11/11 - 0s - loss: 0.5276 - val_loss: 0.4628 - 31ms/epoch - 3ms/step\n",
      "Epoch 392/2500\n",
      "11/11 - 0s - loss: 0.4901 - val_loss: 0.4612 - 32ms/epoch - 3ms/step\n",
      "Epoch 393/2500\n",
      "11/11 - 0s - loss: 0.5337 - val_loss: 0.4679 - 30ms/epoch - 3ms/step\n",
      "Epoch 394/2500\n",
      "11/11 - 0s - loss: 0.5182 - val_loss: 0.4640 - 30ms/epoch - 3ms/step\n",
      "Epoch 395/2500\n",
      "11/11 - 0s - loss: 0.5344 - val_loss: 0.4630 - 31ms/epoch - 3ms/step\n",
      "Epoch 396/2500\n",
      "11/11 - 0s - loss: 0.5439 - val_loss: 0.4703 - 30ms/epoch - 3ms/step\n",
      "Epoch 397/2500\n",
      "11/11 - 0s - loss: 0.5505 - val_loss: 0.4706 - 31ms/epoch - 3ms/step\n",
      "Epoch 398/2500\n",
      "11/11 - 0s - loss: 0.5517 - val_loss: 0.4668 - 30ms/epoch - 3ms/step\n",
      "Epoch 399/2500\n",
      "11/11 - 0s - loss: 0.5618 - val_loss: 0.4656 - 30ms/epoch - 3ms/step\n",
      "Epoch 400/2500\n",
      "11/11 - 0s - loss: 0.5292 - val_loss: 0.4648 - 32ms/epoch - 3ms/step\n",
      "Epoch 401/2500\n",
      "11/11 - 0s - loss: 0.5274 - val_loss: 0.4618 - 33ms/epoch - 3ms/step\n",
      "Epoch 402/2500\n",
      "11/11 - 0s - loss: 0.5390 - val_loss: 0.4624 - 30ms/epoch - 3ms/step\n",
      "Epoch 403/2500\n",
      "11/11 - 0s - loss: 0.5494 - val_loss: 0.4534 - 30ms/epoch - 3ms/step\n",
      "Epoch 404/2500\n",
      "11/11 - 0s - loss: 0.4860 - val_loss: 0.4542 - 30ms/epoch - 3ms/step\n",
      "Epoch 405/2500\n",
      "11/11 - 0s - loss: 0.5165 - val_loss: 0.4566 - 31ms/epoch - 3ms/step\n",
      "Epoch 406/2500\n",
      "11/11 - 0s - loss: 0.5290 - val_loss: 0.4604 - 30ms/epoch - 3ms/step\n",
      "Epoch 407/2500\n",
      "11/11 - 0s - loss: 0.5257 - val_loss: 0.4663 - 30ms/epoch - 3ms/step\n",
      "Epoch 408/2500\n",
      "11/11 - 0s - loss: 0.5335 - val_loss: 0.4678 - 30ms/epoch - 3ms/step\n",
      "Epoch 409/2500\n",
      "11/11 - 0s - loss: 0.5079 - val_loss: 0.4635 - 31ms/epoch - 3ms/step\n",
      "Epoch 410/2500\n",
      "11/11 - 0s - loss: 0.5582 - val_loss: 0.4612 - 30ms/epoch - 3ms/step\n",
      "Epoch 411/2500\n",
      "11/11 - 0s - loss: 0.5162 - val_loss: 0.4612 - 30ms/epoch - 3ms/step\n",
      "Epoch 412/2500\n",
      "11/11 - 0s - loss: 0.5284 - val_loss: 0.4658 - 30ms/epoch - 3ms/step\n",
      "Epoch 413/2500\n",
      "11/11 - 0s - loss: 0.5422 - val_loss: 0.4678 - 30ms/epoch - 3ms/step\n",
      "Epoch 414/2500\n",
      "11/11 - 0s - loss: 0.5180 - val_loss: 0.4612 - 30ms/epoch - 3ms/step\n",
      "Epoch 415/2500\n",
      "11/11 - 0s - loss: 0.5085 - val_loss: 0.4593 - 30ms/epoch - 3ms/step\n",
      "Epoch 416/2500\n",
      "11/11 - 0s - loss: 0.4856 - val_loss: 0.4554 - 33ms/epoch - 3ms/step\n",
      "Epoch 417/2500\n",
      "11/11 - 0s - loss: 0.5216 - val_loss: 0.4491 - 39ms/epoch - 4ms/step\n",
      "Epoch 418/2500\n",
      "11/11 - 0s - loss: 0.5371 - val_loss: 0.4501 - 32ms/epoch - 3ms/step\n",
      "Epoch 419/2500\n",
      "11/11 - 0s - loss: 0.5130 - val_loss: 0.4478 - 32ms/epoch - 3ms/step\n",
      "Epoch 420/2500\n",
      "11/11 - 0s - loss: 0.5219 - val_loss: 0.4497 - 32ms/epoch - 3ms/step\n",
      "Epoch 421/2500\n",
      "11/11 - 0s - loss: 0.5086 - val_loss: 0.4555 - 32ms/epoch - 3ms/step\n",
      "Epoch 422/2500\n",
      "11/11 - 0s - loss: 0.5323 - val_loss: 0.4591 - 34ms/epoch - 3ms/step\n",
      "Epoch 423/2500\n",
      "11/11 - 0s - loss: 0.5200 - val_loss: 0.4599 - 32ms/epoch - 3ms/step\n",
      "Epoch 424/2500\n",
      "11/11 - 0s - loss: 0.5199 - val_loss: 0.4653 - 31ms/epoch - 3ms/step\n",
      "Epoch 425/2500\n",
      "11/11 - 0s - loss: 0.5244 - val_loss: 0.4541 - 32ms/epoch - 3ms/step\n",
      "Epoch 426/2500\n",
      "11/11 - 0s - loss: 0.5285 - val_loss: 0.4533 - 33ms/epoch - 3ms/step\n",
      "Epoch 427/2500\n",
      "11/11 - 0s - loss: 0.5265 - val_loss: 0.4534 - 34ms/epoch - 3ms/step\n",
      "Epoch 428/2500\n",
      "11/11 - 0s - loss: 0.5253 - val_loss: 0.4543 - 32ms/epoch - 3ms/step\n",
      "Epoch 429/2500\n",
      "11/11 - 0s - loss: 0.5424 - val_loss: 0.4592 - 31ms/epoch - 3ms/step\n",
      "Epoch 430/2500\n",
      "11/11 - 0s - loss: 0.4848 - val_loss: 0.4570 - 32ms/epoch - 3ms/step\n",
      "Epoch 431/2500\n",
      "11/11 - 0s - loss: 0.5080 - val_loss: 0.4535 - 31ms/epoch - 3ms/step\n",
      "Epoch 432/2500\n",
      "11/11 - 0s - loss: 0.5119 - val_loss: 0.4631 - 32ms/epoch - 3ms/step\n",
      "Epoch 433/2500\n",
      "11/11 - 0s - loss: 0.5077 - val_loss: 0.4557 - 32ms/epoch - 3ms/step\n",
      "Epoch 434/2500\n",
      "11/11 - 0s - loss: 0.5217 - val_loss: 0.4598 - 32ms/epoch - 3ms/step\n",
      "Epoch 435/2500\n",
      "11/11 - 0s - loss: 0.5069 - val_loss: 0.4635 - 31ms/epoch - 3ms/step\n",
      "Epoch 436/2500\n",
      "11/11 - 0s - loss: 0.5357 - val_loss: 0.4592 - 31ms/epoch - 3ms/step\n",
      "Epoch 437/2500\n",
      "11/11 - 0s - loss: 0.5088 - val_loss: 0.4673 - 31ms/epoch - 3ms/step\n",
      "Epoch 438/2500\n",
      "11/11 - 0s - loss: 0.5197 - val_loss: 0.4665 - 31ms/epoch - 3ms/step\n",
      "Epoch 439/2500\n",
      "11/11 - 0s - loss: 0.5222 - val_loss: 0.4639 - 30ms/epoch - 3ms/step\n",
      "Epoch 440/2500\n",
      "11/11 - 0s - loss: 0.5199 - val_loss: 0.4649 - 30ms/epoch - 3ms/step\n",
      "Epoch 441/2500\n",
      "11/11 - 0s - loss: 0.5205 - val_loss: 0.4576 - 30ms/epoch - 3ms/step\n",
      "Epoch 442/2500\n",
      "11/11 - 0s - loss: 0.5030 - val_loss: 0.4504 - 31ms/epoch - 3ms/step\n",
      "Epoch 443/2500\n",
      "11/11 - 0s - loss: 0.5041 - val_loss: 0.4580 - 30ms/epoch - 3ms/step\n",
      "Epoch 444/2500\n",
      "11/11 - 0s - loss: 0.4972 - val_loss: 0.4601 - 31ms/epoch - 3ms/step\n",
      "Epoch 445/2500\n",
      "11/11 - 0s - loss: 0.5096 - val_loss: 0.4616 - 31ms/epoch - 3ms/step\n",
      "Epoch 446/2500\n",
      "11/11 - 0s - loss: 0.5240 - val_loss: 0.4606 - 30ms/epoch - 3ms/step\n",
      "Epoch 447/2500\n",
      "11/11 - 0s - loss: 0.5029 - val_loss: 0.4535 - 30ms/epoch - 3ms/step\n",
      "Epoch 448/2500\n",
      "11/11 - 0s - loss: 0.4939 - val_loss: 0.4534 - 30ms/epoch - 3ms/step\n",
      "Epoch 449/2500\n",
      "11/11 - 0s - loss: 0.5377 - val_loss: 0.4537 - 42ms/epoch - 4ms/step\n",
      "Epoch 450/2500\n",
      "11/11 - 0s - loss: 0.4970 - val_loss: 0.4547 - 30ms/epoch - 3ms/step\n",
      "Epoch 451/2500\n",
      "11/11 - 0s - loss: 0.5075 - val_loss: 0.4482 - 31ms/epoch - 3ms/step\n",
      "Epoch 452/2500\n",
      "11/11 - 0s - loss: 0.5144 - val_loss: 0.4505 - 30ms/epoch - 3ms/step\n",
      "Epoch 453/2500\n",
      "11/11 - 0s - loss: 0.5302 - val_loss: 0.4520 - 34ms/epoch - 3ms/step\n",
      "Epoch 454/2500\n",
      "11/11 - 0s - loss: 0.5127 - val_loss: 0.4549 - 30ms/epoch - 3ms/step\n",
      "Epoch 455/2500\n",
      "11/11 - 0s - loss: 0.4935 - val_loss: 0.4602 - 31ms/epoch - 3ms/step\n",
      "Epoch 456/2500\n",
      "11/11 - 0s - loss: 0.4921 - val_loss: 0.4580 - 29ms/epoch - 3ms/step\n",
      "Epoch 457/2500\n",
      "11/11 - 0s - loss: 0.5001 - val_loss: 0.4635 - 30ms/epoch - 3ms/step\n",
      "Epoch 458/2500\n",
      "11/11 - 0s - loss: 0.5044 - val_loss: 0.4633 - 30ms/epoch - 3ms/step\n",
      "Epoch 459/2500\n",
      "11/11 - 0s - loss: 0.4948 - val_loss: 0.4601 - 30ms/epoch - 3ms/step\n",
      "Epoch 460/2500\n",
      "11/11 - 0s - loss: 0.5183 - val_loss: 0.4554 - 32ms/epoch - 3ms/step\n",
      "Epoch 461/2500\n",
      "11/11 - 0s - loss: 0.4917 - val_loss: 0.4554 - 29ms/epoch - 3ms/step\n",
      "Epoch 462/2500\n",
      "11/11 - 0s - loss: 0.5056 - val_loss: 0.4659 - 31ms/epoch - 3ms/step\n",
      "Epoch 463/2500\n",
      "11/11 - 0s - loss: 0.5174 - val_loss: 0.4704 - 30ms/epoch - 3ms/step\n",
      "Epoch 464/2500\n",
      "11/11 - 0s - loss: 0.5030 - val_loss: 0.4598 - 30ms/epoch - 3ms/step\n",
      "Epoch 465/2500\n",
      "11/11 - 0s - loss: 0.4785 - val_loss: 0.4618 - 31ms/epoch - 3ms/step\n",
      "Epoch 466/2500\n",
      "11/11 - 0s - loss: 0.4996 - val_loss: 0.4549 - 31ms/epoch - 3ms/step\n",
      "Epoch 467/2500\n",
      "11/11 - 0s - loss: 0.5145 - val_loss: 0.4546 - 30ms/epoch - 3ms/step\n",
      "Epoch 468/2500\n",
      "11/11 - 0s - loss: 0.5078 - val_loss: 0.4556 - 31ms/epoch - 3ms/step\n",
      "Epoch 469/2500\n",
      "11/11 - 0s - loss: 0.5050 - val_loss: 0.4582 - 31ms/epoch - 3ms/step\n",
      "Epoch 470/2500\n",
      "11/11 - 0s - loss: 0.4952 - val_loss: 0.4629 - 30ms/epoch - 3ms/step\n",
      "Epoch 471/2500\n",
      "11/11 - 0s - loss: 0.4849 - val_loss: 0.4622 - 31ms/epoch - 3ms/step\n",
      "Epoch 472/2500\n",
      "11/11 - 0s - loss: 0.5123 - val_loss: 0.4598 - 31ms/epoch - 3ms/step\n",
      "Epoch 473/2500\n",
      "11/11 - 0s - loss: 0.5263 - val_loss: 0.4583 - 30ms/epoch - 3ms/step\n",
      "Epoch 474/2500\n",
      "11/11 - 0s - loss: 0.5276 - val_loss: 0.4608 - 30ms/epoch - 3ms/step\n",
      "Epoch 475/2500\n",
      "11/11 - 0s - loss: 0.4951 - val_loss: 0.4544 - 30ms/epoch - 3ms/step\n",
      "Epoch 476/2500\n",
      "11/11 - 0s - loss: 0.4962 - val_loss: 0.4543 - 30ms/epoch - 3ms/step\n",
      "Epoch 477/2500\n",
      "11/11 - 0s - loss: 0.5287 - val_loss: 0.4539 - 29ms/epoch - 3ms/step\n",
      "Epoch 478/2500\n",
      "11/11 - 0s - loss: 0.4959 - val_loss: 0.4536 - 32ms/epoch - 3ms/step\n",
      "Epoch 479/2500\n",
      "11/11 - 0s - loss: 0.4781 - val_loss: 0.4588 - 32ms/epoch - 3ms/step\n",
      "Epoch 480/2500\n",
      "11/11 - 0s - loss: 0.4823 - val_loss: 0.4604 - 30ms/epoch - 3ms/step\n",
      "Epoch 481/2500\n",
      "11/11 - 0s - loss: 0.5311 - val_loss: 0.4553 - 32ms/epoch - 3ms/step\n",
      "Epoch 482/2500\n",
      "11/11 - 0s - loss: 0.4840 - val_loss: 0.4476 - 39ms/epoch - 4ms/step\n",
      "Epoch 483/2500\n",
      "11/11 - 0s - loss: 0.4857 - val_loss: 0.4515 - 30ms/epoch - 3ms/step\n",
      "Epoch 484/2500\n",
      "11/11 - 0s - loss: 0.5392 - val_loss: 0.4605 - 31ms/epoch - 3ms/step\n",
      "Epoch 485/2500\n",
      "11/11 - 0s - loss: 0.5215 - val_loss: 0.4506 - 31ms/epoch - 3ms/step\n",
      "Epoch 486/2500\n",
      "11/11 - 0s - loss: 0.4916 - val_loss: 0.4507 - 32ms/epoch - 3ms/step\n",
      "Epoch 487/2500\n",
      "11/11 - 0s - loss: 0.4797 - val_loss: 0.4559 - 31ms/epoch - 3ms/step\n",
      "Epoch 488/2500\n",
      "11/11 - 0s - loss: 0.4885 - val_loss: 0.4545 - 30ms/epoch - 3ms/step\n",
      "Epoch 489/2500\n",
      "11/11 - 0s - loss: 0.5259 - val_loss: 0.4567 - 32ms/epoch - 3ms/step\n",
      "Epoch 490/2500\n",
      "11/11 - 0s - loss: 0.4748 - val_loss: 0.4562 - 30ms/epoch - 3ms/step\n",
      "Epoch 491/2500\n",
      "11/11 - 0s - loss: 0.4975 - val_loss: 0.4509 - 31ms/epoch - 3ms/step\n",
      "Epoch 492/2500\n",
      "11/11 - 0s - loss: 0.4953 - val_loss: 0.4461 - 31ms/epoch - 3ms/step\n",
      "Epoch 493/2500\n",
      "11/11 - 0s - loss: 0.4837 - val_loss: 0.4484 - 30ms/epoch - 3ms/step\n",
      "Epoch 494/2500\n",
      "11/11 - 0s - loss: 0.4828 - val_loss: 0.4513 - 29ms/epoch - 3ms/step\n",
      "Epoch 495/2500\n",
      "11/11 - 0s - loss: 0.4997 - val_loss: 0.4467 - 30ms/epoch - 3ms/step\n",
      "Epoch 496/2500\n",
      "11/11 - 0s - loss: 0.4706 - val_loss: 0.4441 - 30ms/epoch - 3ms/step\n",
      "Epoch 497/2500\n",
      "11/11 - 0s - loss: 0.4649 - val_loss: 0.4484 - 30ms/epoch - 3ms/step\n",
      "Epoch 498/2500\n",
      "11/11 - 0s - loss: 0.4921 - val_loss: 0.4571 - 31ms/epoch - 3ms/step\n",
      "Epoch 499/2500\n",
      "11/11 - 0s - loss: 0.4959 - val_loss: 0.4537 - 31ms/epoch - 3ms/step\n",
      "Epoch 500/2500\n",
      "11/11 - 0s - loss: 0.4825 - val_loss: 0.4502 - 31ms/epoch - 3ms/step\n",
      "Epoch 501/2500\n",
      "11/11 - 0s - loss: 0.4910 - val_loss: 0.4474 - 33ms/epoch - 3ms/step\n",
      "Epoch 502/2500\n",
      "11/11 - 0s - loss: 0.4860 - val_loss: 0.4548 - 30ms/epoch - 3ms/step\n",
      "Epoch 503/2500\n",
      "11/11 - 0s - loss: 0.4849 - val_loss: 0.4601 - 31ms/epoch - 3ms/step\n",
      "Epoch 504/2500\n",
      "11/11 - 0s - loss: 0.4782 - val_loss: 0.4530 - 30ms/epoch - 3ms/step\n",
      "Epoch 505/2500\n",
      "11/11 - 0s - loss: 0.4793 - val_loss: 0.4574 - 31ms/epoch - 3ms/step\n",
      "Epoch 506/2500\n",
      "11/11 - 0s - loss: 0.5165 - val_loss: 0.4587 - 31ms/epoch - 3ms/step\n",
      "Epoch 507/2500\n",
      "11/11 - 0s - loss: 0.4748 - val_loss: 0.4601 - 30ms/epoch - 3ms/step\n",
      "Epoch 508/2500\n",
      "11/11 - 0s - loss: 0.4889 - val_loss: 0.4644 - 30ms/epoch - 3ms/step\n",
      "Epoch 509/2500\n",
      "11/11 - 0s - loss: 0.4702 - val_loss: 0.4553 - 31ms/epoch - 3ms/step\n",
      "Epoch 510/2500\n",
      "11/11 - 0s - loss: 0.4898 - val_loss: 0.4522 - 30ms/epoch - 3ms/step\n",
      "Epoch 511/2500\n",
      "11/11 - 0s - loss: 0.4676 - val_loss: 0.4455 - 30ms/epoch - 3ms/step\n",
      "Epoch 512/2500\n",
      "11/11 - 0s - loss: 0.5045 - val_loss: 0.4492 - 30ms/epoch - 3ms/step\n",
      "Epoch 513/2500\n",
      "11/11 - 0s - loss: 0.4784 - val_loss: 0.4470 - 30ms/epoch - 3ms/step\n",
      "Epoch 514/2500\n",
      "11/11 - 0s - loss: 0.4810 - val_loss: 0.4406 - 30ms/epoch - 3ms/step\n",
      "Epoch 515/2500\n",
      "11/11 - 0s - loss: 0.5184 - val_loss: 0.4412 - 30ms/epoch - 3ms/step\n",
      "Epoch 516/2500\n",
      "11/11 - 0s - loss: 0.4900 - val_loss: 0.4451 - 31ms/epoch - 3ms/step\n",
      "Epoch 517/2500\n",
      "11/11 - 0s - loss: 0.4608 - val_loss: 0.4477 - 30ms/epoch - 3ms/step\n",
      "Epoch 518/2500\n",
      "11/11 - 0s - loss: 0.4798 - val_loss: 0.4507 - 30ms/epoch - 3ms/step\n",
      "Epoch 519/2500\n",
      "11/11 - 0s - loss: 0.4980 - val_loss: 0.4525 - 31ms/epoch - 3ms/step\n",
      "Epoch 520/2500\n",
      "11/11 - 0s - loss: 0.4900 - val_loss: 0.4455 - 31ms/epoch - 3ms/step\n",
      "Epoch 521/2500\n",
      "11/11 - 0s - loss: 0.5198 - val_loss: 0.4607 - 39ms/epoch - 4ms/step\n",
      "Epoch 522/2500\n",
      "11/11 - 0s - loss: 0.5175 - val_loss: 0.4562 - 33ms/epoch - 3ms/step\n",
      "Epoch 523/2500\n",
      "11/11 - 0s - loss: 0.4628 - val_loss: 0.4514 - 31ms/epoch - 3ms/step\n",
      "Epoch 524/2500\n",
      "11/11 - 0s - loss: 0.5073 - val_loss: 0.4464 - 34ms/epoch - 3ms/step\n",
      "Epoch 525/2500\n",
      "11/11 - 0s - loss: 0.5030 - val_loss: 0.4437 - 30ms/epoch - 3ms/step\n",
      "Epoch 526/2500\n",
      "11/11 - 0s - loss: 0.5020 - val_loss: 0.4452 - 32ms/epoch - 3ms/step\n",
      "Epoch 527/2500\n",
      "11/11 - 0s - loss: 0.4862 - val_loss: 0.4489 - 31ms/epoch - 3ms/step\n",
      "Epoch 528/2500\n",
      "11/11 - 0s - loss: 0.4857 - val_loss: 0.4543 - 31ms/epoch - 3ms/step\n",
      "Epoch 529/2500\n",
      "11/11 - 0s - loss: 0.4661 - val_loss: 0.4582 - 33ms/epoch - 3ms/step\n",
      "Epoch 530/2500\n",
      "11/11 - 0s - loss: 0.4996 - val_loss: 0.4638 - 32ms/epoch - 3ms/step\n",
      "Epoch 531/2500\n",
      "11/11 - 0s - loss: 0.4888 - val_loss: 0.4499 - 30ms/epoch - 3ms/step\n",
      "Epoch 532/2500\n",
      "11/11 - 0s - loss: 0.4686 - val_loss: 0.4539 - 31ms/epoch - 3ms/step\n",
      "Epoch 533/2500\n",
      "11/11 - 0s - loss: 0.4738 - val_loss: 0.4517 - 31ms/epoch - 3ms/step\n",
      "Epoch 534/2500\n",
      "11/11 - 0s - loss: 0.4765 - val_loss: 0.4569 - 31ms/epoch - 3ms/step\n",
      "Epoch 535/2500\n",
      "11/11 - 0s - loss: 0.4800 - val_loss: 0.4521 - 30ms/epoch - 3ms/step\n",
      "Epoch 536/2500\n",
      "11/11 - 0s - loss: 0.4954 - val_loss: 0.4504 - 30ms/epoch - 3ms/step\n",
      "Epoch 537/2500\n",
      "11/11 - 0s - loss: 0.4898 - val_loss: 0.4521 - 30ms/epoch - 3ms/step\n",
      "Epoch 538/2500\n",
      "11/11 - 0s - loss: 0.4903 - val_loss: 0.4538 - 30ms/epoch - 3ms/step\n",
      "Epoch 539/2500\n",
      "11/11 - 0s - loss: 0.4754 - val_loss: 0.4568 - 30ms/epoch - 3ms/step\n",
      "Epoch 540/2500\n",
      "11/11 - 0s - loss: 0.4898 - val_loss: 0.4591 - 31ms/epoch - 3ms/step\n",
      "Epoch 541/2500\n",
      "11/11 - 0s - loss: 0.4609 - val_loss: 0.4585 - 31ms/epoch - 3ms/step\n",
      "Epoch 542/2500\n",
      "11/11 - 0s - loss: 0.4890 - val_loss: 0.4562 - 31ms/epoch - 3ms/step\n",
      "Epoch 543/2500\n",
      "11/11 - 0s - loss: 0.4714 - val_loss: 0.4581 - 30ms/epoch - 3ms/step\n",
      "Epoch 544/2500\n",
      "11/11 - 0s - loss: 0.5019 - val_loss: 0.4630 - 30ms/epoch - 3ms/step\n",
      "Epoch 545/2500\n",
      "11/11 - 0s - loss: 0.4850 - val_loss: 0.4847 - 31ms/epoch - 3ms/step\n",
      "Epoch 546/2500\n",
      "11/11 - 0s - loss: 0.4762 - val_loss: 0.4830 - 33ms/epoch - 3ms/step\n",
      "Epoch 547/2500\n",
      "11/11 - 0s - loss: 0.5043 - val_loss: 0.4703 - 31ms/epoch - 3ms/step\n",
      "Epoch 548/2500\n",
      "11/11 - 0s - loss: 0.4705 - val_loss: 0.4541 - 30ms/epoch - 3ms/step\n",
      "Epoch 549/2500\n",
      "11/11 - 0s - loss: 0.4980 - val_loss: 0.4538 - 31ms/epoch - 3ms/step\n",
      "Epoch 550/2500\n",
      "11/11 - 0s - loss: 0.4749 - val_loss: 0.4588 - 31ms/epoch - 3ms/step\n",
      "Epoch 551/2500\n",
      "11/11 - 0s - loss: 0.4774 - val_loss: 0.4616 - 31ms/epoch - 3ms/step\n",
      "Epoch 552/2500\n",
      "11/11 - 0s - loss: 0.4479 - val_loss: 0.4528 - 37ms/epoch - 3ms/step\n",
      "Epoch 553/2500\n",
      "11/11 - 0s - loss: 0.4615 - val_loss: 0.4551 - 37ms/epoch - 3ms/step\n",
      "Epoch 554/2500\n",
      "11/11 - 0s - loss: 0.4841 - val_loss: 0.4573 - 31ms/epoch - 3ms/step\n",
      "Epoch 555/2500\n",
      "11/11 - 0s - loss: 0.4949 - val_loss: 0.4600 - 31ms/epoch - 3ms/step\n",
      "Epoch 556/2500\n",
      "11/11 - 0s - loss: 0.4644 - val_loss: 0.4507 - 30ms/epoch - 3ms/step\n",
      "Epoch 557/2500\n",
      "11/11 - 0s - loss: 0.5037 - val_loss: 0.4427 - 30ms/epoch - 3ms/step\n",
      "Epoch 558/2500\n",
      "11/11 - 0s - loss: 0.5190 - val_loss: 0.4415 - 31ms/epoch - 3ms/step\n",
      "Epoch 559/2500\n",
      "11/11 - 0s - loss: 0.4600 - val_loss: 0.4523 - 31ms/epoch - 3ms/step\n",
      "Epoch 560/2500\n",
      "11/11 - 0s - loss: 0.5017 - val_loss: 0.4474 - 30ms/epoch - 3ms/step\n",
      "Epoch 561/2500\n",
      "11/11 - 0s - loss: 0.4736 - val_loss: 0.4465 - 31ms/epoch - 3ms/step\n",
      "Epoch 562/2500\n",
      "11/11 - 0s - loss: 0.5034 - val_loss: 0.4440 - 29ms/epoch - 3ms/step\n",
      "Epoch 563/2500\n",
      "11/11 - 0s - loss: 0.4464 - val_loss: 0.4472 - 31ms/epoch - 3ms/step\n",
      "Epoch 564/2500\n",
      "11/11 - 0s - loss: 0.4985 - val_loss: 0.4473 - 30ms/epoch - 3ms/step\n",
      "Epoch 565/2500\n",
      "11/11 - 0s - loss: 0.4814 - val_loss: 0.4543 - 30ms/epoch - 3ms/step\n",
      "Epoch 566/2500\n",
      "11/11 - 0s - loss: 0.4660 - val_loss: 0.4632 - 34ms/epoch - 3ms/step\n",
      "Epoch 567/2500\n",
      "11/11 - 0s - loss: 0.4457 - val_loss: 0.4674 - 31ms/epoch - 3ms/step\n",
      "Epoch 568/2500\n",
      "11/11 - 0s - loss: 0.4824 - val_loss: 0.4637 - 30ms/epoch - 3ms/step\n",
      "Epoch 569/2500\n",
      "11/11 - 0s - loss: 0.4853 - val_loss: 0.4528 - 30ms/epoch - 3ms/step\n",
      "Epoch 570/2500\n",
      "11/11 - 0s - loss: 0.4754 - val_loss: 0.4534 - 31ms/epoch - 3ms/step\n",
      "Epoch 571/2500\n",
      "11/11 - 0s - loss: 0.4871 - val_loss: 0.4471 - 30ms/epoch - 3ms/step\n",
      "Epoch 572/2500\n",
      "11/11 - 0s - loss: 0.4529 - val_loss: 0.4600 - 31ms/epoch - 3ms/step\n",
      "Epoch 573/2500\n",
      "11/11 - 0s - loss: 0.4718 - val_loss: 0.4705 - 30ms/epoch - 3ms/step\n",
      "Epoch 574/2500\n",
      "11/11 - 0s - loss: 0.4685 - val_loss: 0.4647 - 30ms/epoch - 3ms/step\n",
      "Epoch 575/2500\n",
      "11/11 - 0s - loss: 0.4747 - val_loss: 0.4575 - 31ms/epoch - 3ms/step\n",
      "Epoch 576/2500\n",
      "11/11 - 0s - loss: 0.4651 - val_loss: 0.4606 - 30ms/epoch - 3ms/step\n",
      "Epoch 577/2500\n",
      "11/11 - 0s - loss: 0.4862 - val_loss: 0.4626 - 30ms/epoch - 3ms/step\n",
      "Epoch 578/2500\n",
      "11/11 - 0s - loss: 0.4849 - val_loss: 0.4563 - 30ms/epoch - 3ms/step\n",
      "Epoch 579/2500\n",
      "11/11 - 0s - loss: 0.4684 - val_loss: 0.4560 - 31ms/epoch - 3ms/step\n",
      "Epoch 580/2500\n",
      "11/11 - 0s - loss: 0.4897 - val_loss: 0.4501 - 30ms/epoch - 3ms/step\n",
      "Epoch 581/2500\n",
      "11/11 - 0s - loss: 0.4462 - val_loss: 0.4549 - 30ms/epoch - 3ms/step\n",
      "Epoch 582/2500\n",
      "11/11 - 0s - loss: 0.4875 - val_loss: 0.4540 - 31ms/epoch - 3ms/step\n",
      "Epoch 583/2500\n",
      "11/11 - 0s - loss: 0.4542 - val_loss: 0.4551 - 30ms/epoch - 3ms/step\n",
      "Epoch 584/2500\n",
      "11/11 - 0s - loss: 0.4698 - val_loss: 0.4581 - 31ms/epoch - 3ms/step\n",
      "Epoch 585/2500\n",
      "11/11 - 0s - loss: 0.4651 - val_loss: 0.4570 - 31ms/epoch - 3ms/step\n",
      "Epoch 586/2500\n",
      "11/11 - 0s - loss: 0.4620 - val_loss: 0.4578 - 36ms/epoch - 3ms/step\n",
      "Epoch 587/2500\n",
      "11/11 - 0s - loss: 0.4694 - val_loss: 0.4552 - 34ms/epoch - 3ms/step\n",
      "Epoch 588/2500\n",
      "11/11 - 0s - loss: 0.4817 - val_loss: 0.4563 - 31ms/epoch - 3ms/step\n",
      "Epoch 589/2500\n",
      "11/11 - 0s - loss: 0.4727 - val_loss: 0.4521 - 45ms/epoch - 4ms/step\n",
      "Epoch 590/2500\n",
      "11/11 - 0s - loss: 0.4629 - val_loss: 0.4456 - 31ms/epoch - 3ms/step\n",
      "Epoch 591/2500\n",
      "11/11 - 0s - loss: 0.4681 - val_loss: 0.4419 - 31ms/epoch - 3ms/step\n",
      "Epoch 592/2500\n",
      "11/11 - 0s - loss: 0.4668 - val_loss: 0.4404 - 30ms/epoch - 3ms/step\n",
      "Epoch 593/2500\n",
      "11/11 - 0s - loss: 0.4837 - val_loss: 0.4418 - 31ms/epoch - 3ms/step\n",
      "Epoch 594/2500\n",
      "11/11 - 0s - loss: 0.4439 - val_loss: 0.4438 - 31ms/epoch - 3ms/step\n",
      "Epoch 595/2500\n",
      "11/11 - 0s - loss: 0.4928 - val_loss: 0.4472 - 29ms/epoch - 3ms/step\n",
      "Epoch 596/2500\n",
      "11/11 - 0s - loss: 0.4806 - val_loss: 0.4516 - 31ms/epoch - 3ms/step\n",
      "Epoch 597/2500\n",
      "11/11 - 0s - loss: 0.4894 - val_loss: 0.4546 - 30ms/epoch - 3ms/step\n",
      "Epoch 598/2500\n",
      "11/11 - 0s - loss: 0.4788 - val_loss: 0.4559 - 30ms/epoch - 3ms/step\n",
      "Epoch 599/2500\n",
      "11/11 - 0s - loss: 0.4819 - val_loss: 0.4588 - 30ms/epoch - 3ms/step\n",
      "Epoch 600/2500\n",
      "11/11 - 0s - loss: 0.4835 - val_loss: 0.4530 - 30ms/epoch - 3ms/step\n",
      "Epoch 601/2500\n",
      "11/11 - 0s - loss: 0.4697 - val_loss: 0.4582 - 31ms/epoch - 3ms/step\n",
      "Epoch 602/2500\n",
      "11/11 - 0s - loss: 0.4663 - val_loss: 0.4642 - 30ms/epoch - 3ms/step\n",
      "Epoch 603/2500\n",
      "11/11 - 0s - loss: 0.4754 - val_loss: 0.4675 - 30ms/epoch - 3ms/step\n",
      "Epoch 604/2500\n",
      "11/11 - 0s - loss: 0.4425 - val_loss: 0.4594 - 30ms/epoch - 3ms/step\n",
      "Epoch 605/2500\n",
      "11/11 - 0s - loss: 0.4788 - val_loss: 0.4713 - 30ms/epoch - 3ms/step\n",
      "Epoch 606/2500\n",
      "11/11 - 0s - loss: 0.4822 - val_loss: 0.4776 - 34ms/epoch - 3ms/step\n",
      "Epoch 607/2500\n",
      "11/11 - 0s - loss: 0.4732 - val_loss: 0.4625 - 31ms/epoch - 3ms/step\n",
      "Epoch 608/2500\n",
      "11/11 - 0s - loss: 0.4697 - val_loss: 0.4650 - 31ms/epoch - 3ms/step\n",
      "Epoch 609/2500\n",
      "11/11 - 0s - loss: 0.4727 - val_loss: 0.4702 - 32ms/epoch - 3ms/step\n",
      "Epoch 610/2500\n",
      "11/11 - 0s - loss: 0.4760 - val_loss: 0.4654 - 33ms/epoch - 3ms/step\n",
      "Epoch 611/2500\n",
      "11/11 - 0s - loss: 0.4670 - val_loss: 0.4653 - 30ms/epoch - 3ms/step\n",
      "Epoch 612/2500\n",
      "11/11 - 0s - loss: 0.4673 - val_loss: 0.4562 - 30ms/epoch - 3ms/step\n",
      "Epoch 613/2500\n",
      "11/11 - 0s - loss: 0.4710 - val_loss: 0.4508 - 30ms/epoch - 3ms/step\n",
      "Epoch 614/2500\n",
      "11/11 - 0s - loss: 0.4855 - val_loss: 0.4467 - 31ms/epoch - 3ms/step\n",
      "Epoch 615/2500\n",
      "11/11 - 0s - loss: 0.4610 - val_loss: 0.4457 - 30ms/epoch - 3ms/step\n",
      "Epoch 616/2500\n",
      "11/11 - 0s - loss: 0.4820 - val_loss: 0.4479 - 30ms/epoch - 3ms/step\n",
      "Epoch 617/2500\n",
      "11/11 - 0s - loss: 0.4620 - val_loss: 0.4558 - 30ms/epoch - 3ms/step\n",
      "Epoch 618/2500\n",
      "11/11 - 0s - loss: 0.4684 - val_loss: 0.4569 - 30ms/epoch - 3ms/step\n",
      "Epoch 619/2500\n",
      "11/11 - 0s - loss: 0.4790 - val_loss: 0.4654 - 36ms/epoch - 3ms/step\n",
      "Epoch 620/2500\n",
      "11/11 - 0s - loss: 0.4641 - val_loss: 0.4676 - 34ms/epoch - 3ms/step\n",
      "Epoch 621/2500\n",
      "11/11 - 0s - loss: 0.4763 - val_loss: 0.4637 - 30ms/epoch - 3ms/step\n",
      "Epoch 622/2500\n",
      "11/11 - 0s - loss: 0.4719 - val_loss: 0.4680 - 30ms/epoch - 3ms/step\n",
      "Epoch 623/2500\n",
      "11/11 - 0s - loss: 0.4727 - val_loss: 0.4580 - 31ms/epoch - 3ms/step\n",
      "Epoch 624/2500\n",
      "11/11 - 0s - loss: 0.5010 - val_loss: 0.4483 - 30ms/epoch - 3ms/step\n",
      "Epoch 625/2500\n",
      "11/11 - 0s - loss: 0.4503 - val_loss: 0.4522 - 33ms/epoch - 3ms/step\n",
      "Epoch 626/2500\n",
      "11/11 - 0s - loss: 0.4729 - val_loss: 0.4514 - 32ms/epoch - 3ms/step\n",
      "Epoch 627/2500\n",
      "11/11 - 0s - loss: 0.4480 - val_loss: 0.4572 - 30ms/epoch - 3ms/step\n",
      "Epoch 628/2500\n",
      "11/11 - 0s - loss: 0.4649 - val_loss: 0.4539 - 33ms/epoch - 3ms/step\n",
      "Epoch 629/2500\n",
      "11/11 - 0s - loss: 0.4588 - val_loss: 0.4513 - 33ms/epoch - 3ms/step\n",
      "Epoch 630/2500\n",
      "11/11 - 0s - loss: 0.4687 - val_loss: 0.4514 - 30ms/epoch - 3ms/step\n",
      "Epoch 631/2500\n",
      "11/11 - 0s - loss: 0.4751 - val_loss: 0.4531 - 30ms/epoch - 3ms/step\n",
      "Epoch 632/2500\n",
      "11/11 - 0s - loss: 0.4635 - val_loss: 0.4538 - 31ms/epoch - 3ms/step\n",
      "Epoch 633/2500\n",
      "11/11 - 0s - loss: 0.4831 - val_loss: 0.4603 - 30ms/epoch - 3ms/step\n",
      "Epoch 634/2500\n",
      "11/11 - 0s - loss: 0.4710 - val_loss: 0.4604 - 31ms/epoch - 3ms/step\n",
      "Epoch 635/2500\n",
      "11/11 - 0s - loss: 0.4770 - val_loss: 0.4708 - 30ms/epoch - 3ms/step\n",
      "Epoch 636/2500\n",
      "11/11 - 0s - loss: 0.4690 - val_loss: 0.4719 - 30ms/epoch - 3ms/step\n",
      "Epoch 637/2500\n",
      "11/11 - 0s - loss: 0.4884 - val_loss: 0.4634 - 30ms/epoch - 3ms/step\n",
      "Epoch 638/2500\n",
      "11/11 - 0s - loss: 0.4729 - val_loss: 0.4573 - 31ms/epoch - 3ms/step\n",
      "Epoch 639/2500\n",
      "11/11 - 0s - loss: 0.4643 - val_loss: 0.4526 - 30ms/epoch - 3ms/step\n",
      "Epoch 640/2500\n",
      "11/11 - 0s - loss: 0.4569 - val_loss: 0.4547 - 31ms/epoch - 3ms/step\n",
      "Epoch 641/2500\n",
      "11/11 - 0s - loss: 0.4331 - val_loss: 0.4613 - 30ms/epoch - 3ms/step\n",
      "Epoch 642/2500\n",
      "11/11 - 0s - loss: 0.4979 - val_loss: 0.4609 - 30ms/epoch - 3ms/step\n",
      "Epoch 643/2500\n",
      "11/11 - 0s - loss: 0.4860 - val_loss: 0.4586 - 30ms/epoch - 3ms/step\n",
      "Epoch 644/2500\n",
      "11/11 - 0s - loss: 0.4653 - val_loss: 0.4667 - 33ms/epoch - 3ms/step\n",
      "Epoch 645/2500\n",
      "11/11 - 0s - loss: 0.4618 - val_loss: 0.4590 - 32ms/epoch - 3ms/step\n",
      "Epoch 646/2500\n",
      "11/11 - 0s - loss: 0.4596 - val_loss: 0.4571 - 30ms/epoch - 3ms/step\n",
      "Epoch 647/2500\n",
      "11/11 - 0s - loss: 0.4642 - val_loss: 0.4713 - 43ms/epoch - 4ms/step\n",
      "Epoch 648/2500\n",
      "11/11 - 0s - loss: 0.4608 - val_loss: 0.4633 - 31ms/epoch - 3ms/step\n",
      "Epoch 649/2500\n",
      "11/11 - 0s - loss: 0.4483 - val_loss: 0.4551 - 31ms/epoch - 3ms/step\n",
      "Epoch 650/2500\n",
      "11/11 - 0s - loss: 0.4638 - val_loss: 0.4623 - 30ms/epoch - 3ms/step\n",
      "Epoch 651/2500\n",
      "11/11 - 0s - loss: 0.4649 - val_loss: 0.4610 - 30ms/epoch - 3ms/step\n",
      "Epoch 652/2500\n",
      "11/11 - 0s - loss: 0.4557 - val_loss: 0.4735 - 31ms/epoch - 3ms/step\n",
      "Epoch 653/2500\n",
      "11/11 - 0s - loss: 0.4503 - val_loss: 0.4636 - 30ms/epoch - 3ms/step\n",
      "Epoch 654/2500\n",
      "11/11 - 0s - loss: 0.4548 - val_loss: 0.4629 - 30ms/epoch - 3ms/step\n",
      "Epoch 655/2500\n",
      "11/11 - 0s - loss: 0.4775 - val_loss: 0.4693 - 30ms/epoch - 3ms/step\n",
      "Epoch 656/2500\n",
      "11/11 - 0s - loss: 0.4828 - val_loss: 0.4706 - 31ms/epoch - 3ms/step\n",
      "Epoch 657/2500\n",
      "11/11 - 0s - loss: 0.4899 - val_loss: 0.4677 - 32ms/epoch - 3ms/step\n",
      "Epoch 658/2500\n",
      "11/11 - 0s - loss: 0.4805 - val_loss: 0.4579 - 31ms/epoch - 3ms/step\n",
      "Epoch 659/2500\n",
      "11/11 - 0s - loss: 0.4767 - val_loss: 0.4621 - 31ms/epoch - 3ms/step\n",
      "Epoch 660/2500\n",
      "11/11 - 0s - loss: 0.4504 - val_loss: 0.4771 - 32ms/epoch - 3ms/step\n",
      "Epoch 661/2500\n",
      "11/11 - 0s - loss: 0.4599 - val_loss: 0.4792 - 32ms/epoch - 3ms/step\n",
      "Epoch 662/2500\n",
      "11/11 - 0s - loss: 0.4597 - val_loss: 0.4698 - 34ms/epoch - 3ms/step\n",
      "Epoch 663/2500\n",
      "11/11 - 0s - loss: 0.4429 - val_loss: 0.4627 - 34ms/epoch - 3ms/step\n",
      "Epoch 664/2500\n",
      "11/11 - 0s - loss: 0.4571 - val_loss: 0.4607 - 34ms/epoch - 3ms/step\n",
      "Epoch 665/2500\n",
      "11/11 - 0s - loss: 0.4684 - val_loss: 0.4583 - 35ms/epoch - 3ms/step\n",
      "Epoch 666/2500\n",
      "11/11 - 0s - loss: 0.4520 - val_loss: 0.4529 - 35ms/epoch - 3ms/step\n",
      "Epoch 667/2500\n",
      "11/11 - 0s - loss: 0.4511 - val_loss: 0.4551 - 35ms/epoch - 3ms/step\n",
      "Epoch 668/2500\n",
      "11/11 - 0s - loss: 0.4556 - val_loss: 0.4647 - 33ms/epoch - 3ms/step\n",
      "Epoch 669/2500\n",
      "11/11 - 0s - loss: 0.4550 - val_loss: 0.4692 - 32ms/epoch - 3ms/step\n",
      "Epoch 670/2500\n",
      "11/11 - 0s - loss: 0.4861 - val_loss: 0.4595 - 32ms/epoch - 3ms/step\n",
      "Epoch 671/2500\n",
      "11/11 - 0s - loss: 0.4811 - val_loss: 0.4648 - 34ms/epoch - 3ms/step\n",
      "Epoch 672/2500\n",
      "11/11 - 0s - loss: 0.4706 - val_loss: 0.4615 - 31ms/epoch - 3ms/step\n",
      "Epoch 673/2500\n",
      "11/11 - 0s - loss: 0.4593 - val_loss: 0.4651 - 33ms/epoch - 3ms/step\n",
      "Epoch 674/2500\n",
      "11/11 - 0s - loss: 0.4617 - val_loss: 0.4699 - 32ms/epoch - 3ms/step\n",
      "Epoch 675/2500\n",
      "11/11 - 0s - loss: 0.4935 - val_loss: 0.4601 - 32ms/epoch - 3ms/step\n",
      "Epoch 676/2500\n",
      "11/11 - 0s - loss: 0.4613 - val_loss: 0.4606 - 32ms/epoch - 3ms/step\n",
      "Epoch 677/2500\n",
      "11/11 - 0s - loss: 0.4664 - val_loss: 0.4620 - 41ms/epoch - 4ms/step\n",
      "Epoch 678/2500\n",
      "11/11 - 0s - loss: 0.4637 - val_loss: 0.4683 - 38ms/epoch - 3ms/step\n",
      "Epoch 679/2500\n",
      "11/11 - 0s - loss: 0.4619 - val_loss: 0.4663 - 33ms/epoch - 3ms/step\n",
      "Epoch 680/2500\n",
      "11/11 - 0s - loss: 0.4762 - val_loss: 0.4691 - 36ms/epoch - 3ms/step\n",
      "Epoch 681/2500\n",
      "11/11 - 0s - loss: 0.4649 - val_loss: 0.4692 - 32ms/epoch - 3ms/step\n",
      "Epoch 682/2500\n",
      "11/11 - 0s - loss: 0.4329 - val_loss: 0.4668 - 32ms/epoch - 3ms/step\n",
      "Epoch 683/2500\n",
      "11/11 - 0s - loss: 0.4457 - val_loss: 0.4661 - 32ms/epoch - 3ms/step\n",
      "Epoch 684/2500\n",
      "11/11 - 0s - loss: 0.4575 - val_loss: 0.4629 - 32ms/epoch - 3ms/step\n",
      "Epoch 685/2500\n",
      "11/11 - 0s - loss: 0.4740 - val_loss: 0.4593 - 38ms/epoch - 3ms/step\n",
      "Epoch 686/2500\n",
      "11/11 - 0s - loss: 0.4594 - val_loss: 0.4628 - 36ms/epoch - 3ms/step\n",
      "Epoch 687/2500\n",
      "11/11 - 0s - loss: 0.4809 - val_loss: 0.4606 - 36ms/epoch - 3ms/step\n",
      "Epoch 688/2500\n",
      "11/11 - 0s - loss: 0.4792 - val_loss: 0.4699 - 31ms/epoch - 3ms/step\n",
      "Epoch 689/2500\n",
      "11/11 - 0s - loss: 0.4841 - val_loss: 0.4673 - 31ms/epoch - 3ms/step\n",
      "Epoch 690/2500\n",
      "11/11 - 0s - loss: 0.4786 - val_loss: 0.4550 - 32ms/epoch - 3ms/step\n",
      "Epoch 691/2500\n",
      "11/11 - 0s - loss: 0.4661 - val_loss: 0.4556 - 33ms/epoch - 3ms/step\n",
      "Epoch 692/2500\n",
      "11/11 - 0s - loss: 0.4929 - val_loss: 0.4482 - 34ms/epoch - 3ms/step\n",
      "Epoch 693/2500\n",
      "11/11 - 0s - loss: 0.4385 - val_loss: 0.4604 - 35ms/epoch - 3ms/step\n",
      "Epoch 694/2500\n",
      "11/11 - 0s - loss: 0.4746 - val_loss: 0.4625 - 34ms/epoch - 3ms/step\n",
      "Epoch 695/2500\n",
      "11/11 - 0s - loss: 0.4581 - val_loss: 0.4603 - 35ms/epoch - 3ms/step\n",
      "Epoch 696/2500\n",
      "11/11 - 0s - loss: 0.4635 - val_loss: 0.4668 - 35ms/epoch - 3ms/step\n",
      "Epoch 697/2500\n",
      "11/11 - 0s - loss: 0.4444 - val_loss: 0.4504 - 38ms/epoch - 3ms/step\n",
      "Epoch 698/2500\n",
      "11/11 - 0s - loss: 0.4550 - val_loss: 0.4495 - 35ms/epoch - 3ms/step\n",
      "Epoch 699/2500\n",
      "11/11 - 0s - loss: 0.4769 - val_loss: 0.4507 - 35ms/epoch - 3ms/step\n",
      "Epoch 700/2500\n",
      "11/11 - 0s - loss: 0.4858 - val_loss: 0.4630 - 34ms/epoch - 3ms/step\n",
      "Epoch 701/2500\n",
      "11/11 - 0s - loss: 0.4652 - val_loss: 0.4682 - 34ms/epoch - 3ms/step\n",
      "Epoch 702/2500\n",
      "11/11 - 0s - loss: 0.4537 - val_loss: 0.4582 - 33ms/epoch - 3ms/step\n",
      "Epoch 703/2500\n",
      "11/11 - 0s - loss: 0.4543 - val_loss: 0.4613 - 34ms/epoch - 3ms/step\n",
      "Epoch 704/2500\n",
      "11/11 - 0s - loss: 0.4375 - val_loss: 0.4585 - 45ms/epoch - 4ms/step\n",
      "Epoch 705/2500\n",
      "11/11 - 0s - loss: 0.4172 - val_loss: 0.4489 - 38ms/epoch - 3ms/step\n",
      "Epoch 706/2500\n",
      "11/11 - 0s - loss: 0.4907 - val_loss: 0.4548 - 34ms/epoch - 3ms/step\n",
      "Epoch 707/2500\n",
      "11/11 - 0s - loss: 0.4672 - val_loss: 0.4528 - 34ms/epoch - 3ms/step\n",
      "Epoch 708/2500\n",
      "11/11 - 0s - loss: 0.4715 - val_loss: 0.4582 - 34ms/epoch - 3ms/step\n",
      "Epoch 709/2500\n",
      "11/11 - 0s - loss: 0.4555 - val_loss: 0.4662 - 33ms/epoch - 3ms/step\n",
      "Epoch 710/2500\n",
      "11/11 - 0s - loss: 0.4467 - val_loss: 0.4865 - 33ms/epoch - 3ms/step\n",
      "Epoch 711/2500\n",
      "11/11 - 0s - loss: 0.4505 - val_loss: 0.4754 - 33ms/epoch - 3ms/step\n",
      "Epoch 712/2500\n",
      "11/11 - 0s - loss: 0.4576 - val_loss: 0.4615 - 34ms/epoch - 3ms/step\n",
      "Epoch 713/2500\n",
      "11/11 - 0s - loss: 0.4414 - val_loss: 0.4569 - 36ms/epoch - 3ms/step\n",
      "Epoch 714/2500\n",
      "11/11 - 0s - loss: 0.4546 - val_loss: 0.4531 - 34ms/epoch - 3ms/step\n",
      "Epoch 715/2500\n",
      "11/11 - 0s - loss: 0.4464 - val_loss: 0.4597 - 32ms/epoch - 3ms/step\n",
      "Epoch 716/2500\n",
      "11/11 - 0s - loss: 0.4377 - val_loss: 0.4555 - 33ms/epoch - 3ms/step\n",
      "Epoch 717/2500\n",
      "11/11 - 0s - loss: 0.4570 - val_loss: 0.4595 - 32ms/epoch - 3ms/step\n",
      "Epoch 718/2500\n",
      "11/11 - 0s - loss: 0.4412 - val_loss: 0.4563 - 32ms/epoch - 3ms/step\n",
      "Epoch 719/2500\n",
      "11/11 - 0s - loss: 0.4501 - val_loss: 0.4637 - 32ms/epoch - 3ms/step\n",
      "Epoch 720/2500\n",
      "11/11 - 0s - loss: 0.4708 - val_loss: 0.4597 - 33ms/epoch - 3ms/step\n",
      "Epoch 721/2500\n",
      "11/11 - 0s - loss: 0.4389 - val_loss: 0.4580 - 33ms/epoch - 3ms/step\n",
      "Epoch 722/2500\n",
      "11/11 - 0s - loss: 0.4369 - val_loss: 0.4574 - 31ms/epoch - 3ms/step\n",
      "Epoch 723/2500\n",
      "11/11 - 0s - loss: 0.4595 - val_loss: 0.4581 - 31ms/epoch - 3ms/step\n",
      "Epoch 724/2500\n",
      "11/11 - 0s - loss: 0.4526 - val_loss: 0.4577 - 31ms/epoch - 3ms/step\n",
      "Epoch 725/2500\n",
      "11/11 - 0s - loss: 0.4493 - val_loss: 0.4503 - 31ms/epoch - 3ms/step\n",
      "Epoch 726/2500\n",
      "11/11 - 0s - loss: 0.4515 - val_loss: 0.4464 - 32ms/epoch - 3ms/step\n",
      "Epoch 727/2500\n",
      "11/11 - 0s - loss: 0.4662 - val_loss: 0.4507 - 33ms/epoch - 3ms/step\n",
      "Epoch 728/2500\n",
      "11/11 - 0s - loss: 0.4361 - val_loss: 0.4470 - 33ms/epoch - 3ms/step\n",
      "Epoch 729/2500\n",
      "11/11 - 0s - loss: 0.4558 - val_loss: 0.4543 - 44ms/epoch - 4ms/step\n",
      "Epoch 730/2500\n",
      "11/11 - 0s - loss: 0.4541 - val_loss: 0.4719 - 37ms/epoch - 3ms/step\n",
      "Epoch 731/2500\n",
      "11/11 - 0s - loss: 0.4536 - val_loss: 0.4892 - 32ms/epoch - 3ms/step\n",
      "Epoch 732/2500\n",
      "11/11 - 0s - loss: 0.4611 - val_loss: 0.4651 - 31ms/epoch - 3ms/step\n",
      "Epoch 733/2500\n",
      "11/11 - 0s - loss: 0.4468 - val_loss: 0.4619 - 32ms/epoch - 3ms/step\n",
      "Epoch 734/2500\n",
      "11/11 - 0s - loss: 0.4459 - val_loss: 0.4611 - 31ms/epoch - 3ms/step\n",
      "Epoch 735/2500\n",
      "11/11 - 0s - loss: 0.4616 - val_loss: 0.4570 - 44ms/epoch - 4ms/step\n",
      "Epoch 736/2500\n",
      "11/11 - 0s - loss: 0.4550 - val_loss: 0.4533 - 31ms/epoch - 3ms/step\n",
      "Epoch 737/2500\n",
      "11/11 - 0s - loss: 0.4410 - val_loss: 0.4572 - 32ms/epoch - 3ms/step\n",
      "Epoch 738/2500\n",
      "11/11 - 0s - loss: 0.4600 - val_loss: 0.4583 - 31ms/epoch - 3ms/step\n",
      "Epoch 739/2500\n",
      "11/11 - 0s - loss: 0.4361 - val_loss: 0.4515 - 32ms/epoch - 3ms/step\n",
      "Epoch 740/2500\n",
      "11/11 - 0s - loss: 0.4353 - val_loss: 0.4543 - 32ms/epoch - 3ms/step\n",
      "Epoch 741/2500\n",
      "11/11 - 0s - loss: 0.4383 - val_loss: 0.4598 - 32ms/epoch - 3ms/step\n",
      "Epoch 742/2500\n",
      "11/11 - 0s - loss: 0.4664 - val_loss: 0.4674 - 32ms/epoch - 3ms/step\n",
      "Epoch 743/2500\n",
      "11/11 - 0s - loss: 0.4409 - val_loss: 0.4620 - 31ms/epoch - 3ms/step\n",
      "Epoch 744/2500\n",
      "11/11 - 0s - loss: 0.4592 - val_loss: 0.4640 - 31ms/epoch - 3ms/step\n",
      "Epoch 745/2500\n",
      "11/11 - 0s - loss: 0.4481 - val_loss: 0.4641 - 31ms/epoch - 3ms/step\n",
      "Epoch 746/2500\n",
      "11/11 - 0s - loss: 0.4460 - val_loss: 0.4596 - 35ms/epoch - 3ms/step\n",
      "Epoch 747/2500\n",
      "11/11 - 0s - loss: 0.4727 - val_loss: 0.4603 - 30ms/epoch - 3ms/step\n",
      "Epoch 748/2500\n",
      "11/11 - 0s - loss: 0.4738 - val_loss: 0.4621 - 33ms/epoch - 3ms/step\n",
      "Epoch 749/2500\n",
      "11/11 - 0s - loss: 0.4419 - val_loss: 0.4611 - 34ms/epoch - 3ms/step\n",
      "Epoch 750/2500\n",
      "11/11 - 0s - loss: 0.4572 - val_loss: 0.4660 - 34ms/epoch - 3ms/step\n",
      "Epoch 751/2500\n",
      "11/11 - 0s - loss: 0.4368 - val_loss: 0.4723 - 34ms/epoch - 3ms/step\n",
      "Epoch 752/2500\n",
      "11/11 - 0s - loss: 0.4423 - val_loss: 0.4676 - 33ms/epoch - 3ms/step\n",
      "Epoch 753/2500\n",
      "11/11 - 0s - loss: 0.4678 - val_loss: 0.4558 - 36ms/epoch - 3ms/step\n",
      "Epoch 754/2500\n",
      "11/11 - 0s - loss: 0.4495 - val_loss: 0.4592 - 33ms/epoch - 3ms/step\n",
      "Epoch 755/2500\n",
      "11/11 - 0s - loss: 0.4456 - val_loss: 0.4475 - 34ms/epoch - 3ms/step\n",
      "Epoch 756/2500\n",
      "11/11 - 0s - loss: 0.4487 - val_loss: 0.4444 - 33ms/epoch - 3ms/step\n",
      "Epoch 757/2500\n",
      "11/11 - 0s - loss: 0.4602 - val_loss: 0.4470 - 34ms/epoch - 3ms/step\n",
      "Epoch 758/2500\n",
      "11/11 - 0s - loss: 0.4349 - val_loss: 0.4548 - 34ms/epoch - 3ms/step\n",
      "Epoch 759/2500\n",
      "11/11 - 0s - loss: 0.4419 - val_loss: 0.4655 - 32ms/epoch - 3ms/step\n",
      "Epoch 760/2500\n",
      "11/11 - 0s - loss: 0.4255 - val_loss: 0.4565 - 34ms/epoch - 3ms/step\n",
      "Epoch 761/2500\n",
      "11/11 - 0s - loss: 0.4340 - val_loss: 0.4573 - 43ms/epoch - 4ms/step\n",
      "Epoch 762/2500\n",
      "11/11 - 0s - loss: 0.4488 - val_loss: 0.4564 - 41ms/epoch - 4ms/step\n",
      "Epoch 763/2500\n",
      "11/11 - 0s - loss: 0.4560 - val_loss: 0.4512 - 33ms/epoch - 3ms/step\n",
      "Epoch 764/2500\n",
      "11/11 - 0s - loss: 0.4548 - val_loss: 0.4569 - 34ms/epoch - 3ms/step\n",
      "Epoch 765/2500\n",
      "11/11 - 0s - loss: 0.4510 - val_loss: 0.4585 - 32ms/epoch - 3ms/step\n",
      "Epoch 766/2500\n",
      "11/11 - 0s - loss: 0.4456 - val_loss: 0.4617 - 32ms/epoch - 3ms/step\n",
      "Epoch 767/2500\n",
      "11/11 - 0s - loss: 0.4706 - val_loss: 0.4681 - 32ms/epoch - 3ms/step\n",
      "Epoch 768/2500\n",
      "11/11 - 0s - loss: 0.4576 - val_loss: 0.4640 - 31ms/epoch - 3ms/step\n",
      "Epoch 769/2500\n",
      "11/11 - 0s - loss: 0.4402 - val_loss: 0.4503 - 32ms/epoch - 3ms/step\n",
      "Epoch 770/2500\n",
      "11/11 - 0s - loss: 0.4564 - val_loss: 0.4524 - 31ms/epoch - 3ms/step\n",
      "Epoch 771/2500\n",
      "11/11 - 0s - loss: 0.4444 - val_loss: 0.4567 - 32ms/epoch - 3ms/step\n",
      "Epoch 772/2500\n",
      "11/11 - 0s - loss: 0.4590 - val_loss: 0.4617 - 32ms/epoch - 3ms/step\n",
      "Epoch 773/2500\n",
      "11/11 - 0s - loss: 0.4274 - val_loss: 0.4613 - 32ms/epoch - 3ms/step\n",
      "Epoch 774/2500\n",
      "11/11 - 0s - loss: 0.4474 - val_loss: 0.4563 - 32ms/epoch - 3ms/step\n",
      "Epoch 775/2500\n",
      "11/11 - 0s - loss: 0.4414 - val_loss: 0.4546 - 31ms/epoch - 3ms/step\n",
      "Epoch 776/2500\n",
      "11/11 - 0s - loss: 0.4954 - val_loss: 0.4534 - 32ms/epoch - 3ms/step\n",
      "Epoch 777/2500\n",
      "11/11 - 0s - loss: 0.4276 - val_loss: 0.4527 - 32ms/epoch - 3ms/step\n",
      "Epoch 778/2500\n",
      "11/11 - 0s - loss: 0.4422 - val_loss: 0.4562 - 34ms/epoch - 3ms/step\n",
      "Epoch 779/2500\n",
      "11/11 - 0s - loss: 0.4573 - val_loss: 0.4533 - 33ms/epoch - 3ms/step\n",
      "Epoch 780/2500\n",
      "11/11 - 0s - loss: 0.4585 - val_loss: 0.4547 - 31ms/epoch - 3ms/step\n",
      "Epoch 781/2500\n",
      "11/11 - 0s - loss: 0.4501 - val_loss: 0.4593 - 32ms/epoch - 3ms/step\n",
      "Epoch 782/2500\n",
      "11/11 - 0s - loss: 0.4445 - val_loss: 0.4484 - 32ms/epoch - 3ms/step\n",
      "Epoch 783/2500\n",
      "11/11 - 0s - loss: 0.4454 - val_loss: 0.4494 - 32ms/epoch - 3ms/step\n",
      "Epoch 784/2500\n",
      "11/11 - 0s - loss: 0.4711 - val_loss: 0.4508 - 32ms/epoch - 3ms/step\n",
      "Epoch 785/2500\n",
      "11/11 - 0s - loss: 0.4532 - val_loss: 0.4535 - 33ms/epoch - 3ms/step\n",
      "Epoch 786/2500\n",
      "11/11 - 0s - loss: 0.4433 - val_loss: 0.4554 - 31ms/epoch - 3ms/step\n",
      "Epoch 787/2500\n",
      "11/11 - 0s - loss: 0.4346 - val_loss: 0.4599 - 31ms/epoch - 3ms/step\n",
      "Epoch 788/2500\n",
      "11/11 - 0s - loss: 0.4354 - val_loss: 0.4596 - 31ms/epoch - 3ms/step\n",
      "Epoch 789/2500\n",
      "11/11 - 0s - loss: 0.4423 - val_loss: 0.4707 - 32ms/epoch - 3ms/step\n",
      "Epoch 790/2500\n",
      "11/11 - 0s - loss: 0.4338 - val_loss: 0.4593 - 31ms/epoch - 3ms/step\n",
      "Epoch 791/2500\n",
      "11/11 - 0s - loss: 0.4391 - val_loss: 0.4494 - 31ms/epoch - 3ms/step\n",
      "Epoch 792/2500\n",
      "11/11 - 0s - loss: 0.4329 - val_loss: 0.4621 - 32ms/epoch - 3ms/step\n",
      "Epoch 793/2500\n",
      "11/11 - 0s - loss: 0.4520 - val_loss: 0.4579 - 51ms/epoch - 5ms/step\n",
      "Epoch 794/2500\n",
      "11/11 - 0s - loss: 0.4294 - val_loss: 0.4606 - 31ms/epoch - 3ms/step\n",
      "Epoch 795/2500\n",
      "11/11 - 0s - loss: 0.4226 - val_loss: 0.4574 - 33ms/epoch - 3ms/step\n",
      "Epoch 796/2500\n",
      "11/11 - 0s - loss: 0.4408 - val_loss: 0.4580 - 32ms/epoch - 3ms/step\n",
      "Epoch 797/2500\n",
      "11/11 - 0s - loss: 0.4618 - val_loss: 0.4523 - 31ms/epoch - 3ms/step\n",
      "Epoch 798/2500\n",
      "11/11 - 0s - loss: 0.4535 - val_loss: 0.4440 - 31ms/epoch - 3ms/step\n",
      "Epoch 799/2500\n",
      "11/11 - 0s - loss: 0.4148 - val_loss: 0.4406 - 32ms/epoch - 3ms/step\n",
      "Epoch 800/2500\n",
      "11/11 - 0s - loss: 0.4559 - val_loss: 0.4456 - 31ms/epoch - 3ms/step\n",
      "Epoch 801/2500\n",
      "11/11 - 0s - loss: 0.4461 - val_loss: 0.4445 - 31ms/epoch - 3ms/step\n",
      "Epoch 802/2500\n",
      "11/11 - 0s - loss: 0.4444 - val_loss: 0.4516 - 32ms/epoch - 3ms/step\n",
      "Epoch 803/2500\n",
      "11/11 - 0s - loss: 0.4391 - val_loss: 0.4470 - 31ms/epoch - 3ms/step\n",
      "Epoch 804/2500\n",
      "11/11 - 0s - loss: 0.4499 - val_loss: 0.4480 - 33ms/epoch - 3ms/step\n",
      "Epoch 805/2500\n",
      "11/11 - 0s - loss: 0.4716 - val_loss: 0.4443 - 31ms/epoch - 3ms/step\n",
      "Epoch 806/2500\n",
      "11/11 - 0s - loss: 0.4381 - val_loss: 0.4460 - 32ms/epoch - 3ms/step\n",
      "Epoch 807/2500\n",
      "11/11 - 0s - loss: 0.4388 - val_loss: 0.4511 - 32ms/epoch - 3ms/step\n",
      "Epoch 808/2500\n",
      "11/11 - 0s - loss: 0.4570 - val_loss: 0.4591 - 35ms/epoch - 3ms/step\n",
      "Epoch 809/2500\n",
      "11/11 - 0s - loss: 0.4422 - val_loss: 0.4553 - 32ms/epoch - 3ms/step\n",
      "Epoch 810/2500\n",
      "11/11 - 0s - loss: 0.4647 - val_loss: 0.4584 - 31ms/epoch - 3ms/step\n",
      "Epoch 811/2500\n",
      "11/11 - 0s - loss: 0.4376 - val_loss: 0.4566 - 31ms/epoch - 3ms/step\n",
      "Epoch 812/2500\n",
      "11/11 - 0s - loss: 0.4074 - val_loss: 0.4619 - 31ms/epoch - 3ms/step\n",
      "Epoch 813/2500\n",
      "11/11 - 0s - loss: 0.4315 - val_loss: 0.4509 - 31ms/epoch - 3ms/step\n",
      "Epoch 814/2500\n",
      "11/11 - 0s - loss: 0.4246 - val_loss: 0.4486 - 31ms/epoch - 3ms/step\n",
      "Epoch 815/2500\n",
      "11/11 - 0s - loss: 0.4489 - val_loss: 0.4541 - 31ms/epoch - 3ms/step\n",
      "Epoch 816/2500\n",
      "11/11 - 0s - loss: 0.4605 - val_loss: 0.4576 - 31ms/epoch - 3ms/step\n",
      "Epoch 817/2500\n",
      "11/11 - 0s - loss: 0.4541 - val_loss: 0.4545 - 32ms/epoch - 3ms/step\n",
      "Epoch 818/2500\n",
      "11/11 - 0s - loss: 0.4313 - val_loss: 0.4570 - 36ms/epoch - 3ms/step\n",
      "Epoch 819/2500\n",
      "11/11 - 0s - loss: 0.4509 - val_loss: 0.4593 - 37ms/epoch - 3ms/step\n",
      "Epoch 820/2500\n",
      "11/11 - 0s - loss: 0.4253 - val_loss: 0.4588 - 31ms/epoch - 3ms/step\n",
      "Epoch 821/2500\n",
      "11/11 - 0s - loss: 0.4472 - val_loss: 0.4617 - 32ms/epoch - 3ms/step\n",
      "Epoch 822/2500\n",
      "11/11 - 0s - loss: 0.4467 - val_loss: 0.4602 - 31ms/epoch - 3ms/step\n",
      "Epoch 823/2500\n",
      "11/11 - 0s - loss: 0.4339 - val_loss: 0.4570 - 35ms/epoch - 3ms/step\n",
      "Epoch 824/2500\n",
      "11/11 - 0s - loss: 0.4501 - val_loss: 0.4660 - 32ms/epoch - 3ms/step\n",
      "Epoch 825/2500\n",
      "11/11 - 0s - loss: 0.4287 - val_loss: 0.4641 - 31ms/epoch - 3ms/step\n",
      "Epoch 826/2500\n",
      "11/11 - 0s - loss: 0.4516 - val_loss: 0.4617 - 32ms/epoch - 3ms/step\n",
      "Epoch 827/2500\n",
      "11/11 - 0s - loss: 0.4369 - val_loss: 0.4597 - 32ms/epoch - 3ms/step\n",
      "Epoch 828/2500\n",
      "11/11 - 0s - loss: 0.4547 - val_loss: 0.4646 - 31ms/epoch - 3ms/step\n",
      "Epoch 829/2500\n",
      "11/11 - 0s - loss: 0.4256 - val_loss: 0.4693 - 31ms/epoch - 3ms/step\n",
      "Epoch 830/2500\n",
      "11/11 - 0s - loss: 0.4384 - val_loss: 0.4639 - 32ms/epoch - 3ms/step\n",
      "Epoch 831/2500\n",
      "11/11 - 0s - loss: 0.4282 - val_loss: 0.4624 - 32ms/epoch - 3ms/step\n",
      "Epoch 832/2500\n",
      "11/11 - 0s - loss: 0.4167 - val_loss: 0.4672 - 32ms/epoch - 3ms/step\n",
      "Epoch 833/2500\n",
      "11/11 - 0s - loss: 0.4206 - val_loss: 0.4652 - 31ms/epoch - 3ms/step\n",
      "Epoch 834/2500\n",
      "11/11 - 0s - loss: 0.4164 - val_loss: 0.4589 - 31ms/epoch - 3ms/step\n",
      "Epoch 835/2500\n",
      "11/11 - 0s - loss: 0.4440 - val_loss: 0.4532 - 32ms/epoch - 3ms/step\n",
      "Epoch 836/2500\n",
      "11/11 - 0s - loss: 0.4512 - val_loss: 0.4545 - 32ms/epoch - 3ms/step\n",
      "Epoch 837/2500\n",
      "11/11 - 0s - loss: 0.4274 - val_loss: 0.4532 - 30ms/epoch - 3ms/step\n",
      "Epoch 838/2500\n",
      "11/11 - 0s - loss: 0.4313 - val_loss: 0.4716 - 34ms/epoch - 3ms/step\n",
      "Epoch 839/2500\n",
      "11/11 - 0s - loss: 0.4329 - val_loss: 0.4745 - 32ms/epoch - 3ms/step\n",
      "Epoch 840/2500\n",
      "11/11 - 0s - loss: 0.4550 - val_loss: 0.4568 - 32ms/epoch - 3ms/step\n",
      "Epoch 841/2500\n",
      "11/11 - 0s - loss: 0.4293 - val_loss: 0.4572 - 31ms/epoch - 3ms/step\n",
      "Epoch 842/2500\n",
      "11/11 - 0s - loss: 0.4346 - val_loss: 0.4620 - 32ms/epoch - 3ms/step\n",
      "Epoch 843/2500\n",
      "11/11 - 0s - loss: 0.4442 - val_loss: 0.4540 - 31ms/epoch - 3ms/step\n",
      "Epoch 844/2500\n",
      "11/11 - 0s - loss: 0.4131 - val_loss: 0.4591 - 31ms/epoch - 3ms/step\n",
      "Epoch 845/2500\n",
      "11/11 - 0s - loss: 0.4360 - val_loss: 0.4636 - 32ms/epoch - 3ms/step\n",
      "Epoch 846/2500\n",
      "11/11 - 0s - loss: 0.4268 - val_loss: 0.4657 - 31ms/epoch - 3ms/step\n",
      "Epoch 847/2500\n",
      "11/11 - 0s - loss: 0.4432 - val_loss: 0.4652 - 31ms/epoch - 3ms/step\n",
      "Epoch 848/2500\n",
      "11/11 - 0s - loss: 0.4263 - val_loss: 0.4637 - 32ms/epoch - 3ms/step\n",
      "Epoch 849/2500\n",
      "11/11 - 0s - loss: 0.4425 - val_loss: 0.4738 - 31ms/epoch - 3ms/step\n",
      "Epoch 850/2500\n",
      "11/11 - 0s - loss: 0.4454 - val_loss: 0.4601 - 37ms/epoch - 3ms/step\n",
      "Epoch 851/2500\n",
      "11/11 - 0s - loss: 0.4376 - val_loss: 0.4571 - 38ms/epoch - 3ms/step\n",
      "Epoch 852/2500\n",
      "11/11 - 0s - loss: 0.4241 - val_loss: 0.4710 - 33ms/epoch - 3ms/step\n",
      "Epoch 853/2500\n",
      "11/11 - 0s - loss: 0.4165 - val_loss: 0.4734 - 33ms/epoch - 3ms/step\n",
      "Epoch 854/2500\n",
      "11/11 - 0s - loss: 0.4256 - val_loss: 0.4704 - 31ms/epoch - 3ms/step\n",
      "Epoch 855/2500\n",
      "11/11 - 0s - loss: 0.4381 - val_loss: 0.4647 - 33ms/epoch - 3ms/step\n",
      "Epoch 856/2500\n",
      "11/11 - 0s - loss: 0.4441 - val_loss: 0.4718 - 31ms/epoch - 3ms/step\n",
      "Epoch 857/2500\n",
      "11/11 - 0s - loss: 0.4203 - val_loss: 0.4712 - 32ms/epoch - 3ms/step\n",
      "Epoch 858/2500\n",
      "11/11 - 0s - loss: 0.4199 - val_loss: 0.4632 - 31ms/epoch - 3ms/step\n",
      "Epoch 859/2500\n",
      "11/11 - 0s - loss: 0.4462 - val_loss: 0.4655 - 31ms/epoch - 3ms/step\n",
      "Epoch 860/2500\n",
      "11/11 - 0s - loss: 0.4159 - val_loss: 0.4706 - 31ms/epoch - 3ms/step\n",
      "Epoch 861/2500\n",
      "11/11 - 0s - loss: 0.4544 - val_loss: 0.4726 - 32ms/epoch - 3ms/step\n",
      "Epoch 862/2500\n",
      "11/11 - 0s - loss: 0.4088 - val_loss: 0.4611 - 32ms/epoch - 3ms/step\n",
      "Epoch 863/2500\n",
      "11/11 - 0s - loss: 0.4330 - val_loss: 0.4571 - 32ms/epoch - 3ms/step\n",
      "Epoch 864/2500\n",
      "11/11 - 0s - loss: 0.4496 - val_loss: 0.4605 - 31ms/epoch - 3ms/step\n",
      "Epoch 865/2500\n",
      "11/11 - 0s - loss: 0.4400 - val_loss: 0.4600 - 31ms/epoch - 3ms/step\n",
      "Epoch 866/2500\n",
      "11/11 - 0s - loss: 0.4368 - val_loss: 0.4642 - 33ms/epoch - 3ms/step\n",
      "Epoch 867/2500\n",
      "11/11 - 0s - loss: 0.4434 - val_loss: 0.4620 - 33ms/epoch - 3ms/step\n",
      "Epoch 868/2500\n",
      "11/11 - 0s - loss: 0.4180 - val_loss: 0.4667 - 41ms/epoch - 4ms/step\n",
      "Epoch 869/2500\n",
      "11/11 - 0s - loss: 0.4391 - val_loss: 0.4660 - 38ms/epoch - 3ms/step\n",
      "Epoch 870/2500\n",
      "11/11 - 0s - loss: 0.4405 - val_loss: 0.4596 - 32ms/epoch - 3ms/step\n",
      "Epoch 871/2500\n",
      "11/11 - 0s - loss: 0.4615 - val_loss: 0.4542 - 31ms/epoch - 3ms/step\n",
      "Epoch 872/2500\n",
      "11/11 - 0s - loss: 0.4409 - val_loss: 0.4646 - 31ms/epoch - 3ms/step\n",
      "Epoch 873/2500\n",
      "11/11 - 0s - loss: 0.4382 - val_loss: 0.4557 - 30ms/epoch - 3ms/step\n",
      "Epoch 874/2500\n",
      "11/11 - 0s - loss: 0.4360 - val_loss: 0.4566 - 31ms/epoch - 3ms/step\n",
      "Epoch 875/2500\n",
      "11/11 - 0s - loss: 0.4394 - val_loss: 0.4639 - 31ms/epoch - 3ms/step\n",
      "Epoch 876/2500\n",
      "11/11 - 0s - loss: 0.4492 - val_loss: 0.4637 - 31ms/epoch - 3ms/step\n",
      "Epoch 877/2500\n",
      "11/11 - 0s - loss: 0.4391 - val_loss: 0.4643 - 36ms/epoch - 3ms/step\n",
      "Epoch 878/2500\n",
      "11/11 - 0s - loss: 0.4305 - val_loss: 0.4679 - 44ms/epoch - 4ms/step\n",
      "Epoch 879/2500\n",
      "11/11 - 0s - loss: 0.4571 - val_loss: 0.4484 - 36ms/epoch - 3ms/step\n",
      "Epoch 880/2500\n",
      "11/11 - 0s - loss: 0.4399 - val_loss: 0.4467 - 39ms/epoch - 4ms/step\n",
      "Epoch 881/2500\n",
      "11/11 - 0s - loss: 0.4355 - val_loss: 0.4569 - 37ms/epoch - 3ms/step\n",
      "Epoch 882/2500\n",
      "11/11 - 0s - loss: 0.4252 - val_loss: 0.4561 - 36ms/epoch - 3ms/step\n",
      "Epoch 883/2500\n",
      "11/11 - 0s - loss: 0.4344 - val_loss: 0.4489 - 34ms/epoch - 3ms/step\n",
      "Epoch 884/2500\n",
      "11/11 - 0s - loss: 0.4457 - val_loss: 0.4506 - 33ms/epoch - 3ms/step\n",
      "Epoch 885/2500\n",
      "11/11 - 0s - loss: 0.4348 - val_loss: 0.4580 - 41ms/epoch - 4ms/step\n",
      "Epoch 886/2500\n",
      "11/11 - 0s - loss: 0.4313 - val_loss: 0.4574 - 36ms/epoch - 3ms/step\n",
      "Epoch 887/2500\n",
      "11/11 - 0s - loss: 0.4256 - val_loss: 0.4542 - 34ms/epoch - 3ms/step\n",
      "Epoch 888/2500\n",
      "11/11 - 0s - loss: 0.4239 - val_loss: 0.4657 - 34ms/epoch - 3ms/step\n",
      "Epoch 889/2500\n",
      "11/11 - 0s - loss: 0.4144 - val_loss: 0.4685 - 33ms/epoch - 3ms/step\n",
      "Epoch 890/2500\n",
      "11/11 - 0s - loss: 0.4191 - val_loss: 0.4588 - 34ms/epoch - 3ms/step\n",
      "Epoch 891/2500\n",
      "11/11 - 0s - loss: 0.4385 - val_loss: 0.4593 - 32ms/epoch - 3ms/step\n",
      "Epoch 892/2500\n",
      "11/11 - 0s - loss: 0.4370 - val_loss: 0.4771 - 32ms/epoch - 3ms/step\n",
      "Epoch 893/2500\n",
      "11/11 - 0s - loss: 0.4380 - val_loss: 0.4610 - 32ms/epoch - 3ms/step\n",
      "Epoch 894/2500\n",
      "11/11 - 0s - loss: 0.4401 - val_loss: 0.4620 - 37ms/epoch - 3ms/step\n",
      "Epoch 895/2500\n",
      "11/11 - 0s - loss: 0.4257 - val_loss: 0.4547 - 33ms/epoch - 3ms/step\n",
      "Epoch 896/2500\n",
      "11/11 - 0s - loss: 0.4336 - val_loss: 0.4557 - 33ms/epoch - 3ms/step\n",
      "Epoch 897/2500\n",
      "11/11 - 0s - loss: 0.4278 - val_loss: 0.4599 - 33ms/epoch - 3ms/step\n",
      "Epoch 898/2500\n",
      "11/11 - 0s - loss: 0.4100 - val_loss: 0.4794 - 33ms/epoch - 3ms/step\n",
      "Epoch 899/2500\n",
      "11/11 - 0s - loss: 0.4493 - val_loss: 0.4765 - 32ms/epoch - 3ms/step\n",
      "Epoch 900/2500\n",
      "11/11 - 0s - loss: 0.4247 - val_loss: 0.4674 - 32ms/epoch - 3ms/step\n",
      "Epoch 901/2500\n",
      "11/11 - 0s - loss: 0.4228 - val_loss: 0.4644 - 32ms/epoch - 3ms/step\n",
      "Epoch 902/2500\n",
      "11/11 - 0s - loss: 0.4224 - val_loss: 0.4616 - 43ms/epoch - 4ms/step\n",
      "Epoch 903/2500\n",
      "11/11 - 0s - loss: 0.4295 - val_loss: 0.4657 - 34ms/epoch - 3ms/step\n",
      "Epoch 904/2500\n",
      "11/11 - 0s - loss: 0.4295 - val_loss: 0.4619 - 32ms/epoch - 3ms/step\n",
      "Epoch 905/2500\n",
      "11/11 - 0s - loss: 0.4167 - val_loss: 0.4660 - 31ms/epoch - 3ms/step\n",
      "Epoch 906/2500\n",
      "11/11 - 0s - loss: 0.4240 - val_loss: 0.4666 - 30ms/epoch - 3ms/step\n",
      "Epoch 907/2500\n",
      "11/11 - 0s - loss: 0.4136 - val_loss: 0.4624 - 30ms/epoch - 3ms/step\n",
      "Epoch 908/2500\n",
      "11/11 - 0s - loss: 0.4149 - val_loss: 0.4651 - 33ms/epoch - 3ms/step\n",
      "Epoch 909/2500\n",
      "11/11 - 0s - loss: 0.4468 - val_loss: 0.4656 - 30ms/epoch - 3ms/step\n",
      "Epoch 910/2500\n",
      "11/11 - 0s - loss: 0.4206 - val_loss: 0.4690 - 32ms/epoch - 3ms/step\n",
      "Epoch 911/2500\n",
      "11/11 - 0s - loss: 0.4226 - val_loss: 0.4646 - 32ms/epoch - 3ms/step\n",
      "Epoch 912/2500\n",
      "11/11 - 0s - loss: 0.4426 - val_loss: 0.4679 - 30ms/epoch - 3ms/step\n",
      "Epoch 913/2500\n",
      "11/11 - 0s - loss: 0.4240 - val_loss: 0.4682 - 31ms/epoch - 3ms/step\n",
      "Epoch 914/2500\n",
      "11/11 - 0s - loss: 0.4294 - val_loss: 0.4582 - 31ms/epoch - 3ms/step\n",
      "Epoch 915/2500\n",
      "11/11 - 0s - loss: 0.4402 - val_loss: 0.4531 - 30ms/epoch - 3ms/step\n",
      "Epoch 916/2500\n",
      "11/11 - 0s - loss: 0.4406 - val_loss: 0.4599 - 30ms/epoch - 3ms/step\n",
      "Epoch 917/2500\n",
      "11/11 - 0s - loss: 0.4119 - val_loss: 0.4581 - 30ms/epoch - 3ms/step\n",
      "Epoch 918/2500\n",
      "11/11 - 0s - loss: 0.4378 - val_loss: 0.4605 - 31ms/epoch - 3ms/step\n",
      "Epoch 919/2500\n",
      "11/11 - 0s - loss: 0.4173 - val_loss: 0.4539 - 30ms/epoch - 3ms/step\n",
      "Epoch 920/2500\n",
      "11/11 - 0s - loss: 0.4349 - val_loss: 0.4631 - 30ms/epoch - 3ms/step\n",
      "Epoch 921/2500\n",
      "11/11 - 0s - loss: 0.4454 - val_loss: 0.4617 - 35ms/epoch - 3ms/step\n",
      "Epoch 922/2500\n",
      "11/11 - 0s - loss: 0.4043 - val_loss: 0.4621 - 30ms/epoch - 3ms/step\n",
      "Epoch 923/2500\n",
      "11/11 - 0s - loss: 0.4348 - val_loss: 0.4675 - 31ms/epoch - 3ms/step\n",
      "Epoch 924/2500\n",
      "11/11 - 0s - loss: 0.4216 - val_loss: 0.4714 - 33ms/epoch - 3ms/step\n",
      "Epoch 925/2500\n",
      "11/11 - 0s - loss: 0.4242 - val_loss: 0.4673 - 33ms/epoch - 3ms/step\n",
      "Epoch 926/2500\n",
      "11/11 - 0s - loss: 0.4339 - val_loss: 0.4686 - 35ms/epoch - 3ms/step\n",
      "Epoch 927/2500\n",
      "11/11 - 0s - loss: 0.4137 - val_loss: 0.4695 - 37ms/epoch - 3ms/step\n",
      "Epoch 928/2500\n",
      "11/11 - 0s - loss: 0.4334 - val_loss: 0.4694 - 32ms/epoch - 3ms/step\n",
      "Epoch 929/2500\n",
      "11/11 - 0s - loss: 0.4370 - val_loss: 0.4710 - 32ms/epoch - 3ms/step\n",
      "Epoch 930/2500\n",
      "11/11 - 0s - loss: 0.4268 - val_loss: 0.4776 - 30ms/epoch - 3ms/step\n",
      "Epoch 931/2500\n",
      "11/11 - 0s - loss: 0.4271 - val_loss: 0.4591 - 30ms/epoch - 3ms/step\n",
      "Epoch 932/2500\n",
      "11/11 - 0s - loss: 0.4330 - val_loss: 0.4607 - 29ms/epoch - 3ms/step\n",
      "Epoch 933/2500\n",
      "11/11 - 0s - loss: 0.4142 - val_loss: 0.4636 - 31ms/epoch - 3ms/step\n",
      "Epoch 934/2500\n",
      "11/11 - 0s - loss: 0.4323 - val_loss: 0.4604 - 33ms/epoch - 3ms/step\n",
      "Epoch 935/2500\n",
      "11/11 - 0s - loss: 0.4179 - val_loss: 0.4615 - 33ms/epoch - 3ms/step\n",
      "Epoch 936/2500\n",
      "11/11 - 0s - loss: 0.4223 - val_loss: 0.4745 - 31ms/epoch - 3ms/step\n",
      "Epoch 937/2500\n",
      "11/11 - 0s - loss: 0.4258 - val_loss: 0.4651 - 29ms/epoch - 3ms/step\n",
      "Epoch 938/2500\n",
      "11/11 - 0s - loss: 0.4205 - val_loss: 0.4582 - 31ms/epoch - 3ms/step\n",
      "Epoch 939/2500\n",
      "11/11 - 0s - loss: 0.4422 - val_loss: 0.4709 - 30ms/epoch - 3ms/step\n",
      "Epoch 940/2500\n",
      "11/11 - 0s - loss: 0.4564 - val_loss: 0.4751 - 30ms/epoch - 3ms/step\n",
      "Epoch 941/2500\n",
      "11/11 - 0s - loss: 0.4279 - val_loss: 0.4778 - 31ms/epoch - 3ms/step\n",
      "Epoch 942/2500\n",
      "11/11 - 0s - loss: 0.4395 - val_loss: 0.4712 - 30ms/epoch - 3ms/step\n",
      "Epoch 943/2500\n",
      "11/11 - 0s - loss: 0.4349 - val_loss: 0.4701 - 31ms/epoch - 3ms/step\n",
      "Epoch 944/2500\n",
      "11/11 - 0s - loss: 0.4440 - val_loss: 0.4670 - 30ms/epoch - 3ms/step\n",
      "Epoch 945/2500\n",
      "11/11 - 0s - loss: 0.4328 - val_loss: 0.4713 - 30ms/epoch - 3ms/step\n",
      "Epoch 946/2500\n",
      "11/11 - 0s - loss: 0.4208 - val_loss: 0.4656 - 30ms/epoch - 3ms/step\n",
      "Epoch 947/2500\n",
      "11/11 - 0s - loss: 0.4296 - val_loss: 0.4684 - 32ms/epoch - 3ms/step\n",
      "Epoch 948/2500\n",
      "11/11 - 0s - loss: 0.4284 - val_loss: 0.4685 - 32ms/epoch - 3ms/step\n",
      "Epoch 949/2500\n",
      "11/11 - 0s - loss: 0.4344 - val_loss: 0.4664 - 30ms/epoch - 3ms/step\n",
      "Epoch 950/2500\n",
      "11/11 - 0s - loss: 0.4362 - val_loss: 0.4636 - 31ms/epoch - 3ms/step\n",
      "Epoch 951/2500\n",
      "11/11 - 0s - loss: 0.4346 - val_loss: 0.4693 - 30ms/epoch - 3ms/step\n",
      "Epoch 952/2500\n",
      "11/11 - 0s - loss: 0.4326 - val_loss: 0.4729 - 30ms/epoch - 3ms/step\n",
      "Epoch 953/2500\n",
      "11/11 - 0s - loss: 0.4319 - val_loss: 0.4705 - 30ms/epoch - 3ms/step\n",
      "Epoch 954/2500\n",
      "11/11 - 0s - loss: 0.4325 - val_loss: 0.4708 - 30ms/epoch - 3ms/step\n",
      "Epoch 955/2500\n",
      "11/11 - 0s - loss: 0.4238 - val_loss: 0.4720 - 30ms/epoch - 3ms/step\n",
      "Epoch 956/2500\n",
      "11/11 - 0s - loss: 0.4348 - val_loss: 0.4639 - 30ms/epoch - 3ms/step\n",
      "Epoch 957/2500\n",
      "11/11 - 0s - loss: 0.4194 - val_loss: 0.4688 - 31ms/epoch - 3ms/step\n",
      "Epoch 958/2500\n",
      "11/11 - 0s - loss: 0.4307 - val_loss: 0.4658 - 30ms/epoch - 3ms/step\n",
      "Epoch 959/2500\n",
      "11/11 - 0s - loss: 0.4285 - val_loss: 0.4614 - 44ms/epoch - 4ms/step\n",
      "Epoch 960/2500\n",
      "11/11 - 0s - loss: 0.4257 - val_loss: 0.4642 - 34ms/epoch - 3ms/step\n",
      "Epoch 961/2500\n",
      "11/11 - 0s - loss: 0.4161 - val_loss: 0.4693 - 31ms/epoch - 3ms/step\n",
      "Epoch 962/2500\n",
      "11/11 - 0s - loss: 0.4447 - val_loss: 0.4613 - 30ms/epoch - 3ms/step\n",
      "Epoch 963/2500\n",
      "11/11 - 0s - loss: 0.4289 - val_loss: 0.4596 - 32ms/epoch - 3ms/step\n",
      "Epoch 964/2500\n",
      "11/11 - 0s - loss: 0.4278 - val_loss: 0.4690 - 32ms/epoch - 3ms/step\n",
      "Epoch 965/2500\n",
      "11/11 - 0s - loss: 0.4174 - val_loss: 0.4741 - 31ms/epoch - 3ms/step\n",
      "Epoch 966/2500\n",
      "11/11 - 0s - loss: 0.4118 - val_loss: 0.4684 - 31ms/epoch - 3ms/step\n",
      "Epoch 967/2500\n",
      "11/11 - 0s - loss: 0.4173 - val_loss: 0.4653 - 30ms/epoch - 3ms/step\n",
      "Epoch 968/2500\n",
      "11/11 - 0s - loss: 0.4203 - val_loss: 0.4600 - 30ms/epoch - 3ms/step\n",
      "Epoch 969/2500\n",
      "11/11 - 0s - loss: 0.4110 - val_loss: 0.4570 - 30ms/epoch - 3ms/step\n",
      "Epoch 970/2500\n",
      "11/11 - 0s - loss: 0.4181 - val_loss: 0.4648 - 31ms/epoch - 3ms/step\n",
      "Epoch 971/2500\n",
      "11/11 - 0s - loss: 0.4192 - val_loss: 0.4699 - 30ms/epoch - 3ms/step\n",
      "Epoch 972/2500\n",
      "11/11 - 0s - loss: 0.4364 - val_loss: 0.4684 - 31ms/epoch - 3ms/step\n",
      "Epoch 973/2500\n",
      "11/11 - 0s - loss: 0.4328 - val_loss: 0.4611 - 35ms/epoch - 3ms/step\n",
      "Epoch 974/2500\n",
      "11/11 - 0s - loss: 0.4107 - val_loss: 0.4604 - 31ms/epoch - 3ms/step\n",
      "Epoch 975/2500\n",
      "11/11 - 0s - loss: 0.4363 - val_loss: 0.4717 - 30ms/epoch - 3ms/step\n",
      "Epoch 976/2500\n",
      "11/11 - 0s - loss: 0.4433 - val_loss: 0.4967 - 30ms/epoch - 3ms/step\n",
      "Epoch 977/2500\n",
      "11/11 - 0s - loss: 0.4329 - val_loss: 0.4705 - 30ms/epoch - 3ms/step\n",
      "Epoch 978/2500\n",
      "11/11 - 0s - loss: 0.4124 - val_loss: 0.4625 - 30ms/epoch - 3ms/step\n",
      "Epoch 979/2500\n",
      "11/11 - 0s - loss: 0.4339 - val_loss: 0.4611 - 31ms/epoch - 3ms/step\n",
      "Epoch 980/2500\n",
      "11/11 - 0s - loss: 0.4083 - val_loss: 0.4616 - 31ms/epoch - 3ms/step\n",
      "Epoch 981/2500\n",
      "11/11 - 0s - loss: 0.4231 - val_loss: 0.4700 - 30ms/epoch - 3ms/step\n",
      "Epoch 982/2500\n",
      "11/11 - 0s - loss: 0.4180 - val_loss: 0.4677 - 30ms/epoch - 3ms/step\n",
      "Epoch 983/2500\n",
      "11/11 - 0s - loss: 0.4192 - val_loss: 0.4669 - 30ms/epoch - 3ms/step\n",
      "Epoch 984/2500\n",
      "11/11 - 0s - loss: 0.4224 - val_loss: 0.4613 - 31ms/epoch - 3ms/step\n",
      "Epoch 985/2500\n",
      "11/11 - 0s - loss: 0.4195 - val_loss: 0.4661 - 40ms/epoch - 4ms/step\n",
      "Epoch 986/2500\n",
      "11/11 - 0s - loss: 0.4242 - val_loss: 0.4779 - 33ms/epoch - 3ms/step\n",
      "Epoch 987/2500\n",
      "11/11 - 0s - loss: 0.4233 - val_loss: 0.4736 - 30ms/epoch - 3ms/step\n",
      "Epoch 988/2500\n",
      "11/11 - 0s - loss: 0.4397 - val_loss: 0.4730 - 32ms/epoch - 3ms/step\n",
      "Epoch 989/2500\n",
      "11/11 - 0s - loss: 0.4200 - val_loss: 0.4660 - 32ms/epoch - 3ms/step\n",
      "Epoch 990/2500\n",
      "11/11 - 0s - loss: 0.4170 - val_loss: 0.4670 - 32ms/epoch - 3ms/step\n",
      "Epoch 991/2500\n",
      "11/11 - 0s - loss: 0.4153 - val_loss: 0.4640 - 30ms/epoch - 3ms/step\n",
      "Epoch 992/2500\n",
      "11/11 - 0s - loss: 0.4279 - val_loss: 0.4607 - 31ms/epoch - 3ms/step\n",
      "Epoch 993/2500\n",
      "11/11 - 0s - loss: 0.4186 - val_loss: 0.4661 - 31ms/epoch - 3ms/step\n",
      "Epoch 994/2500\n",
      "11/11 - 0s - loss: 0.4293 - val_loss: 0.4637 - 30ms/epoch - 3ms/step\n",
      "Epoch 995/2500\n",
      "11/11 - 0s - loss: 0.4193 - val_loss: 0.4606 - 30ms/epoch - 3ms/step\n",
      "Epoch 996/2500\n",
      "11/11 - 0s - loss: 0.4414 - val_loss: 0.4590 - 30ms/epoch - 3ms/step\n",
      "Epoch 997/2500\n",
      "11/11 - 0s - loss: 0.4335 - val_loss: 0.4554 - 30ms/epoch - 3ms/step\n",
      "Epoch 998/2500\n",
      "11/11 - 0s - loss: 0.4306 - val_loss: 0.4618 - 31ms/epoch - 3ms/step\n",
      "Epoch 999/2500\n",
      "11/11 - 0s - loss: 0.4213 - val_loss: 0.4702 - 34ms/epoch - 3ms/step\n",
      "Epoch 1000/2500\n",
      "11/11 - 0s - loss: 0.4353 - val_loss: 0.4544 - 31ms/epoch - 3ms/step\n",
      "Epoch 1001/2500\n",
      "11/11 - 0s - loss: 0.4391 - val_loss: 0.4454 - 32ms/epoch - 3ms/step\n",
      "Epoch 1002/2500\n",
      "11/11 - 0s - loss: 0.4135 - val_loss: 0.4546 - 31ms/epoch - 3ms/step\n",
      "Epoch 1003/2500\n",
      "11/11 - 0s - loss: 0.4121 - val_loss: 0.4589 - 30ms/epoch - 3ms/step\n",
      "Epoch 1004/2500\n",
      "11/11 - 0s - loss: 0.4234 - val_loss: 0.4668 - 31ms/epoch - 3ms/step\n",
      "Epoch 1005/2500\n",
      "11/11 - 0s - loss: 0.4167 - val_loss: 0.4655 - 32ms/epoch - 3ms/step\n",
      "Epoch 1006/2500\n",
      "11/11 - 0s - loss: 0.4142 - val_loss: 0.4603 - 30ms/epoch - 3ms/step\n",
      "Epoch 1007/2500\n",
      "11/11 - 0s - loss: 0.4212 - val_loss: 0.4577 - 30ms/epoch - 3ms/step\n",
      "Epoch 1008/2500\n",
      "11/11 - 0s - loss: 0.4143 - val_loss: 0.4681 - 31ms/epoch - 3ms/step\n",
      "Epoch 1009/2500\n",
      "11/11 - 0s - loss: 0.4313 - val_loss: 0.4719 - 31ms/epoch - 3ms/step\n",
      "Epoch 1010/2500\n",
      "11/11 - 0s - loss: 0.4144 - val_loss: 0.4716 - 31ms/epoch - 3ms/step\n",
      "Epoch 1011/2500\n",
      "11/11 - 0s - loss: 0.4071 - val_loss: 0.4710 - 47ms/epoch - 4ms/step\n",
      "Epoch 1012/2500\n",
      "11/11 - 0s - loss: 0.4162 - val_loss: 0.4652 - 30ms/epoch - 3ms/step\n",
      "Epoch 1013/2500\n",
      "11/11 - 0s - loss: 0.4140 - val_loss: 0.4587 - 30ms/epoch - 3ms/step\n",
      "Epoch 1014/2500\n",
      "11/11 - 0s - loss: 0.4339 - val_loss: 0.4589 - 30ms/epoch - 3ms/step\n",
      "Epoch 1015/2500\n",
      "11/11 - 0s - loss: 0.4394 - val_loss: 0.4535 - 30ms/epoch - 3ms/step\n",
      "Epoch 1016/2500\n",
      "11/11 - 0s - loss: 0.4395 - val_loss: 0.4465 - 31ms/epoch - 3ms/step\n",
      "Epoch 1017/2500\n",
      "11/11 - 0s - loss: 0.4395 - val_loss: 0.4515 - 32ms/epoch - 3ms/step\n",
      "Epoch 1018/2500\n",
      "11/11 - 0s - loss: 0.4445 - val_loss: 0.4523 - 31ms/epoch - 3ms/step\n",
      "Epoch 1019/2500\n",
      "11/11 - 0s - loss: 0.3965 - val_loss: 0.4519 - 31ms/epoch - 3ms/step\n",
      "Epoch 1020/2500\n",
      "11/11 - 0s - loss: 0.4273 - val_loss: 0.4592 - 32ms/epoch - 3ms/step\n",
      "Epoch 1021/2500\n",
      "11/11 - 0s - loss: 0.4171 - val_loss: 0.4643 - 31ms/epoch - 3ms/step\n",
      "Epoch 1022/2500\n",
      "11/11 - 0s - loss: 0.4378 - val_loss: 0.4567 - 30ms/epoch - 3ms/step\n",
      "Epoch 1023/2500\n",
      "11/11 - 0s - loss: 0.4276 - val_loss: 0.4559 - 33ms/epoch - 3ms/step\n",
      "Epoch 1024/2500\n",
      "11/11 - 0s - loss: 0.4415 - val_loss: 0.4622 - 32ms/epoch - 3ms/step\n",
      "Epoch 1025/2500\n",
      "11/11 - 0s - loss: 0.4268 - val_loss: 0.4545 - 31ms/epoch - 3ms/step\n",
      "Epoch 1026/2500\n",
      "11/11 - 0s - loss: 0.4323 - val_loss: 0.4515 - 36ms/epoch - 3ms/step\n",
      "Epoch 1027/2500\n",
      "11/11 - 0s - loss: 0.4040 - val_loss: 0.4549 - 30ms/epoch - 3ms/step\n",
      "Epoch 1028/2500\n",
      "11/11 - 0s - loss: 0.4320 - val_loss: 0.4569 - 30ms/epoch - 3ms/step\n",
      "Epoch 1029/2500\n",
      "11/11 - 0s - loss: 0.4234 - val_loss: 0.4512 - 30ms/epoch - 3ms/step\n",
      "Epoch 1030/2500\n",
      "11/11 - 0s - loss: 0.4179 - val_loss: 0.4541 - 30ms/epoch - 3ms/step\n",
      "Epoch 1031/2500\n",
      "11/11 - 0s - loss: 0.3966 - val_loss: 0.4493 - 31ms/epoch - 3ms/step\n",
      "Epoch 1032/2500\n",
      "11/11 - 0s - loss: 0.4246 - val_loss: 0.4536 - 30ms/epoch - 3ms/step\n",
      "Epoch 1033/2500\n",
      "11/11 - 0s - loss: 0.4069 - val_loss: 0.4703 - 31ms/epoch - 3ms/step\n",
      "Epoch 1034/2500\n",
      "11/11 - 0s - loss: 0.4233 - val_loss: 0.4651 - 30ms/epoch - 3ms/step\n",
      "Epoch 1035/2500\n",
      "11/11 - 0s - loss: 0.4236 - val_loss: 0.4646 - 34ms/epoch - 3ms/step\n",
      "Epoch 1036/2500\n",
      "11/11 - 0s - loss: 0.4348 - val_loss: 0.4732 - 31ms/epoch - 3ms/step\n",
      "Epoch 1037/2500\n",
      "11/11 - 0s - loss: 0.4104 - val_loss: 0.4702 - 40ms/epoch - 4ms/step\n",
      "Epoch 1038/2500\n",
      "11/11 - 0s - loss: 0.4268 - val_loss: 0.4907 - 33ms/epoch - 3ms/step\n",
      "Epoch 1039/2500\n",
      "11/11 - 0s - loss: 0.4296 - val_loss: 0.4871 - 30ms/epoch - 3ms/step\n",
      "Epoch 1040/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4702 - 30ms/epoch - 3ms/step\n",
      "Epoch 1041/2500\n",
      "11/11 - 0s - loss: 0.4167 - val_loss: 0.4661 - 30ms/epoch - 3ms/step\n",
      "Epoch 1042/2500\n",
      "11/11 - 0s - loss: 0.4285 - val_loss: 0.4619 - 30ms/epoch - 3ms/step\n",
      "Epoch 1043/2500\n",
      "11/11 - 0s - loss: 0.4189 - val_loss: 0.4663 - 30ms/epoch - 3ms/step\n",
      "Epoch 1044/2500\n",
      "11/11 - 0s - loss: 0.4230 - val_loss: 0.4539 - 29ms/epoch - 3ms/step\n",
      "Epoch 1045/2500\n",
      "11/11 - 0s - loss: 0.4245 - val_loss: 0.4571 - 31ms/epoch - 3ms/step\n",
      "Epoch 1046/2500\n",
      "11/11 - 0s - loss: 0.4156 - val_loss: 0.4808 - 31ms/epoch - 3ms/step\n",
      "Epoch 1047/2500\n",
      "11/11 - 0s - loss: 0.4222 - val_loss: 0.4730 - 34ms/epoch - 3ms/step\n",
      "Epoch 1048/2500\n",
      "11/11 - 0s - loss: 0.4044 - val_loss: 0.4496 - 30ms/epoch - 3ms/step\n",
      "Epoch 1049/2500\n",
      "11/11 - 0s - loss: 0.4180 - val_loss: 0.4488 - 30ms/epoch - 3ms/step\n",
      "Epoch 1050/2500\n",
      "11/11 - 0s - loss: 0.4256 - val_loss: 0.4610 - 31ms/epoch - 3ms/step\n",
      "Epoch 1051/2500\n",
      "11/11 - 0s - loss: 0.4053 - val_loss: 0.4678 - 32ms/epoch - 3ms/step\n",
      "Epoch 1052/2500\n",
      "11/11 - 0s - loss: 0.4304 - val_loss: 0.4730 - 30ms/epoch - 3ms/step\n",
      "Epoch 1053/2500\n",
      "11/11 - 0s - loss: 0.4227 - val_loss: 0.4657 - 31ms/epoch - 3ms/step\n",
      "Epoch 1054/2500\n",
      "11/11 - 0s - loss: 0.4204 - val_loss: 0.4629 - 30ms/epoch - 3ms/step\n",
      "Epoch 1055/2500\n",
      "11/11 - 0s - loss: 0.4152 - val_loss: 0.4754 - 31ms/epoch - 3ms/step\n",
      "Epoch 1056/2500\n",
      "11/11 - 0s - loss: 0.4297 - val_loss: 0.4647 - 30ms/epoch - 3ms/step\n",
      "Epoch 1057/2500\n",
      "11/11 - 0s - loss: 0.4300 - val_loss: 0.4542 - 31ms/epoch - 3ms/step\n",
      "Epoch 1058/2500\n",
      "11/11 - 0s - loss: 0.4372 - val_loss: 0.4552 - 31ms/epoch - 3ms/step\n",
      "Epoch 1059/2500\n",
      "11/11 - 0s - loss: 0.4097 - val_loss: 0.4544 - 33ms/epoch - 3ms/step\n",
      "Epoch 1060/2500\n",
      "11/11 - 0s - loss: 0.4319 - val_loss: 0.4619 - 31ms/epoch - 3ms/step\n",
      "Epoch 1061/2500\n",
      "11/11 - 0s - loss: 0.4105 - val_loss: 0.4587 - 30ms/epoch - 3ms/step\n",
      "Epoch 1062/2500\n",
      "11/11 - 0s - loss: 0.4332 - val_loss: 0.4588 - 43ms/epoch - 4ms/step\n",
      "Epoch 1063/2500\n",
      "11/11 - 0s - loss: 0.4241 - val_loss: 0.4580 - 31ms/epoch - 3ms/step\n",
      "Epoch 1064/2500\n",
      "11/11 - 0s - loss: 0.4232 - val_loss: 0.4566 - 32ms/epoch - 3ms/step\n",
      "Epoch 1065/2500\n",
      "11/11 - 0s - loss: 0.4342 - val_loss: 0.4500 - 32ms/epoch - 3ms/step\n",
      "Epoch 1066/2500\n",
      "11/11 - 0s - loss: 0.4069 - val_loss: 0.4513 - 32ms/epoch - 3ms/step\n",
      "Epoch 1067/2500\n",
      "11/11 - 0s - loss: 0.4330 - val_loss: 0.4514 - 31ms/epoch - 3ms/step\n",
      "Epoch 1068/2500\n",
      "11/11 - 0s - loss: 0.4188 - val_loss: 0.4598 - 31ms/epoch - 3ms/step\n",
      "Epoch 1069/2500\n",
      "11/11 - 0s - loss: 0.4215 - val_loss: 0.4616 - 30ms/epoch - 3ms/step\n",
      "Epoch 1070/2500\n",
      "11/11 - 0s - loss: 0.4228 - val_loss: 0.4662 - 31ms/epoch - 3ms/step\n",
      "Epoch 1071/2500\n",
      "11/11 - 0s - loss: 0.4077 - val_loss: 0.4749 - 31ms/epoch - 3ms/step\n",
      "Epoch 1072/2500\n",
      "11/11 - 0s - loss: 0.4220 - val_loss: 0.4672 - 32ms/epoch - 3ms/step\n",
      "Epoch 1073/2500\n",
      "11/11 - 0s - loss: 0.4159 - val_loss: 0.4643 - 30ms/epoch - 3ms/step\n",
      "Epoch 1074/2500\n",
      "11/11 - 0s - loss: 0.4379 - val_loss: 0.4784 - 31ms/epoch - 3ms/step\n",
      "Epoch 1075/2500\n",
      "11/11 - 0s - loss: 0.4139 - val_loss: 0.4773 - 30ms/epoch - 3ms/step\n",
      "Epoch 1076/2500\n",
      "11/11 - 0s - loss: 0.4159 - val_loss: 0.4673 - 30ms/epoch - 3ms/step\n",
      "Epoch 1077/2500\n",
      "11/11 - 0s - loss: 0.4256 - val_loss: 0.4818 - 30ms/epoch - 3ms/step\n",
      "Epoch 1078/2500\n",
      "11/11 - 0s - loss: 0.4075 - val_loss: 0.4723 - 31ms/epoch - 3ms/step\n",
      "Epoch 1079/2500\n",
      "11/11 - 0s - loss: 0.4012 - val_loss: 0.4644 - 31ms/epoch - 3ms/step\n",
      "Epoch 1080/2500\n",
      "11/11 - 0s - loss: 0.4263 - val_loss: 0.4638 - 30ms/epoch - 3ms/step\n",
      "Epoch 1081/2500\n",
      "11/11 - 0s - loss: 0.4083 - val_loss: 0.4736 - 30ms/epoch - 3ms/step\n",
      "Epoch 1082/2500\n",
      "11/11 - 0s - loss: 0.4152 - val_loss: 0.4661 - 31ms/epoch - 3ms/step\n",
      "Epoch 1083/2500\n",
      "11/11 - 0s - loss: 0.4158 - val_loss: 0.4631 - 34ms/epoch - 3ms/step\n",
      "Epoch 1084/2500\n",
      "11/11 - 0s - loss: 0.4016 - val_loss: 0.4685 - 29ms/epoch - 3ms/step\n",
      "Epoch 1085/2500\n",
      "11/11 - 0s - loss: 0.4152 - val_loss: 0.4674 - 30ms/epoch - 3ms/step\n",
      "Epoch 1086/2500\n",
      "11/11 - 0s - loss: 0.4167 - val_loss: 0.4652 - 30ms/epoch - 3ms/step\n",
      "Epoch 1087/2500\n",
      "11/11 - 0s - loss: 0.4216 - val_loss: 0.4659 - 41ms/epoch - 4ms/step\n",
      "Epoch 1088/2500\n",
      "11/11 - 0s - loss: 0.4035 - val_loss: 0.4739 - 33ms/epoch - 3ms/step\n",
      "Epoch 1089/2500\n",
      "11/11 - 0s - loss: 0.4371 - val_loss: 0.4637 - 31ms/epoch - 3ms/step\n",
      "Epoch 1090/2500\n",
      "11/11 - 0s - loss: 0.4171 - val_loss: 0.4641 - 31ms/epoch - 3ms/step\n",
      "Epoch 1091/2500\n",
      "11/11 - 0s - loss: 0.4110 - val_loss: 0.4659 - 33ms/epoch - 3ms/step\n",
      "Epoch 1092/2500\n",
      "11/11 - 0s - loss: 0.4139 - val_loss: 0.4777 - 31ms/epoch - 3ms/step\n",
      "Epoch 1093/2500\n",
      "11/11 - 0s - loss: 0.4128 - val_loss: 0.4832 - 30ms/epoch - 3ms/step\n",
      "Epoch 1094/2500\n",
      "11/11 - 0s - loss: 0.4177 - val_loss: 0.4839 - 33ms/epoch - 3ms/step\n",
      "Epoch 1095/2500\n",
      "11/11 - 0s - loss: 0.4261 - val_loss: 0.4876 - 30ms/epoch - 3ms/step\n",
      "Epoch 1096/2500\n",
      "11/11 - 0s - loss: 0.4283 - val_loss: 0.4819 - 30ms/epoch - 3ms/step\n",
      "Epoch 1097/2500\n",
      "11/11 - 0s - loss: 0.4116 - val_loss: 0.4719 - 30ms/epoch - 3ms/step\n",
      "Epoch 1098/2500\n",
      "11/11 - 0s - loss: 0.4289 - val_loss: 0.4750 - 30ms/epoch - 3ms/step\n",
      "Epoch 1099/2500\n",
      "11/11 - 0s - loss: 0.3999 - val_loss: 0.4707 - 30ms/epoch - 3ms/step\n",
      "Epoch 1100/2500\n",
      "11/11 - 0s - loss: 0.4107 - val_loss: 0.4712 - 31ms/epoch - 3ms/step\n",
      "Epoch 1101/2500\n",
      "11/11 - 0s - loss: 0.4020 - val_loss: 0.4646 - 30ms/epoch - 3ms/step\n",
      "Epoch 1102/2500\n",
      "11/11 - 0s - loss: 0.4140 - val_loss: 0.4679 - 30ms/epoch - 3ms/step\n",
      "Epoch 1103/2500\n",
      "11/11 - 0s - loss: 0.4142 - val_loss: 0.4750 - 30ms/epoch - 3ms/step\n",
      "Epoch 1104/2500\n",
      "11/11 - 0s - loss: 0.4173 - val_loss: 0.4776 - 31ms/epoch - 3ms/step\n",
      "Epoch 1105/2500\n",
      "11/11 - 0s - loss: 0.4155 - val_loss: 0.4660 - 34ms/epoch - 3ms/step\n",
      "Epoch 1106/2500\n",
      "11/11 - 0s - loss: 0.4039 - val_loss: 0.4657 - 30ms/epoch - 3ms/step\n",
      "Epoch 1107/2500\n",
      "11/11 - 0s - loss: 0.4177 - val_loss: 0.4668 - 30ms/epoch - 3ms/step\n",
      "Epoch 1108/2500\n",
      "11/11 - 0s - loss: 0.4324 - val_loss: 0.4735 - 30ms/epoch - 3ms/step\n",
      "Epoch 1109/2500\n",
      "11/11 - 0s - loss: 0.4195 - val_loss: 0.4685 - 30ms/epoch - 3ms/step\n",
      "Epoch 1110/2500\n",
      "11/11 - 0s - loss: 0.4186 - val_loss: 0.4717 - 31ms/epoch - 3ms/step\n",
      "Epoch 1111/2500\n",
      "11/11 - 0s - loss: 0.4039 - val_loss: 0.4822 - 36ms/epoch - 3ms/step\n",
      "Epoch 1112/2500\n",
      "11/11 - 0s - loss: 0.4060 - val_loss: 0.4750 - 39ms/epoch - 4ms/step\n",
      "Epoch 1113/2500\n",
      "11/11 - 0s - loss: 0.4035 - val_loss: 0.4773 - 30ms/epoch - 3ms/step\n",
      "Epoch 1114/2500\n",
      "11/11 - 0s - loss: 0.4242 - val_loss: 0.4813 - 31ms/epoch - 3ms/step\n",
      "Epoch 1115/2500\n",
      "11/11 - 0s - loss: 0.4012 - val_loss: 0.4692 - 30ms/epoch - 3ms/step\n",
      "Epoch 1116/2500\n",
      "11/11 - 0s - loss: 0.4299 - val_loss: 0.4614 - 30ms/epoch - 3ms/step\n",
      "Epoch 1117/2500\n",
      "11/11 - 0s - loss: 0.4148 - val_loss: 0.4609 - 33ms/epoch - 3ms/step\n",
      "Epoch 1118/2500\n",
      "11/11 - 0s - loss: 0.4244 - val_loss: 0.4610 - 30ms/epoch - 3ms/step\n",
      "Epoch 1119/2500\n",
      "11/11 - 0s - loss: 0.4367 - val_loss: 0.4625 - 31ms/epoch - 3ms/step\n",
      "Epoch 1120/2500\n",
      "11/11 - 0s - loss: 0.4212 - val_loss: 0.4643 - 33ms/epoch - 3ms/step\n",
      "Epoch 1121/2500\n",
      "11/11 - 0s - loss: 0.4211 - val_loss: 0.4767 - 30ms/epoch - 3ms/step\n",
      "Epoch 1122/2500\n",
      "11/11 - 0s - loss: 0.4185 - val_loss: 0.4668 - 31ms/epoch - 3ms/step\n",
      "Epoch 1123/2500\n",
      "11/11 - 0s - loss: 0.3915 - val_loss: 0.4745 - 30ms/epoch - 3ms/step\n",
      "Epoch 1124/2500\n",
      "11/11 - 0s - loss: 0.4158 - val_loss: 0.4724 - 30ms/epoch - 3ms/step\n",
      "Epoch 1125/2500\n",
      "11/11 - 0s - loss: 0.4159 - val_loss: 0.4555 - 30ms/epoch - 3ms/step\n",
      "Epoch 1126/2500\n",
      "11/11 - 0s - loss: 0.4203 - val_loss: 0.4573 - 33ms/epoch - 3ms/step\n",
      "Epoch 1127/2500\n",
      "11/11 - 0s - loss: 0.4063 - val_loss: 0.4595 - 31ms/epoch - 3ms/step\n",
      "Epoch 1128/2500\n",
      "11/11 - 0s - loss: 0.4104 - val_loss: 0.4615 - 31ms/epoch - 3ms/step\n",
      "Epoch 1129/2500\n",
      "11/11 - 0s - loss: 0.4249 - val_loss: 0.4673 - 30ms/epoch - 3ms/step\n",
      "Epoch 1130/2500\n",
      "11/11 - 0s - loss: 0.4023 - val_loss: 0.4750 - 30ms/epoch - 3ms/step\n",
      "Epoch 1131/2500\n",
      "11/11 - 0s - loss: 0.4170 - val_loss: 0.4708 - 31ms/epoch - 3ms/step\n",
      "Epoch 1132/2500\n",
      "11/11 - 0s - loss: 0.4120 - val_loss: 0.4675 - 30ms/epoch - 3ms/step\n",
      "Epoch 1133/2500\n",
      "11/11 - 0s - loss: 0.4165 - val_loss: 0.4750 - 30ms/epoch - 3ms/step\n",
      "Epoch 1134/2500\n",
      "11/11 - 0s - loss: 0.4228 - val_loss: 0.4726 - 30ms/epoch - 3ms/step\n",
      "Epoch 1135/2500\n",
      "11/11 - 0s - loss: 0.4176 - val_loss: 0.4701 - 30ms/epoch - 3ms/step\n",
      "Epoch 1136/2500\n",
      "11/11 - 0s - loss: 0.4429 - val_loss: 0.4785 - 30ms/epoch - 3ms/step\n",
      "Epoch 1137/2500\n",
      "11/11 - 0s - loss: 0.4084 - val_loss: 0.4689 - 31ms/epoch - 3ms/step\n",
      "Epoch 1138/2500\n",
      "11/11 - 0s - loss: 0.4063 - val_loss: 0.4843 - 32ms/epoch - 3ms/step\n",
      "Epoch 1139/2500\n",
      "11/11 - 0s - loss: 0.4245 - val_loss: 0.4676 - 47ms/epoch - 4ms/step\n",
      "Epoch 1140/2500\n",
      "11/11 - 0s - loss: 0.4177 - val_loss: 0.4643 - 30ms/epoch - 3ms/step\n",
      "Epoch 1141/2500\n",
      "11/11 - 0s - loss: 0.4135 - val_loss: 0.4796 - 31ms/epoch - 3ms/step\n",
      "Epoch 1142/2500\n",
      "11/11 - 0s - loss: 0.4213 - val_loss: 0.4851 - 30ms/epoch - 3ms/step\n",
      "Epoch 1143/2500\n",
      "11/11 - 0s - loss: 0.4084 - val_loss: 0.4726 - 30ms/epoch - 3ms/step\n",
      "Epoch 1144/2500\n",
      "11/11 - 0s - loss: 0.3949 - val_loss: 0.4638 - 31ms/epoch - 3ms/step\n",
      "Epoch 1145/2500\n",
      "11/11 - 0s - loss: 0.4032 - val_loss: 0.4663 - 30ms/epoch - 3ms/step\n",
      "Epoch 1146/2500\n",
      "11/11 - 0s - loss: 0.4288 - val_loss: 0.4668 - 31ms/epoch - 3ms/step\n",
      "Epoch 1147/2500\n",
      "11/11 - 0s - loss: 0.4146 - val_loss: 0.4664 - 33ms/epoch - 3ms/step\n",
      "Epoch 1148/2500\n",
      "11/11 - 0s - loss: 0.4112 - val_loss: 0.4640 - 30ms/epoch - 3ms/step\n",
      "Epoch 1149/2500\n",
      "11/11 - 0s - loss: 0.4249 - val_loss: 0.4735 - 31ms/epoch - 3ms/step\n",
      "Epoch 1150/2500\n",
      "11/11 - 0s - loss: 0.4148 - val_loss: 0.4907 - 31ms/epoch - 3ms/step\n",
      "Epoch 1151/2500\n",
      "11/11 - 0s - loss: 0.4103 - val_loss: 0.4715 - 30ms/epoch - 3ms/step\n",
      "Epoch 1152/2500\n",
      "11/11 - 0s - loss: 0.4032 - val_loss: 0.4607 - 30ms/epoch - 3ms/step\n",
      "Epoch 1153/2500\n",
      "11/11 - 0s - loss: 0.4173 - val_loss: 0.4657 - 30ms/epoch - 3ms/step\n",
      "Epoch 1154/2500\n",
      "11/11 - 0s - loss: 0.4112 - val_loss: 0.4733 - 30ms/epoch - 3ms/step\n",
      "Epoch 1155/2500\n",
      "11/11 - 0s - loss: 0.4186 - val_loss: 0.4686 - 30ms/epoch - 3ms/step\n",
      "Epoch 1156/2500\n",
      "11/11 - 0s - loss: 0.4112 - val_loss: 0.4761 - 31ms/epoch - 3ms/step\n",
      "Epoch 1157/2500\n",
      "11/11 - 0s - loss: 0.4252 - val_loss: 0.4735 - 30ms/epoch - 3ms/step\n",
      "Epoch 1158/2500\n",
      "11/11 - 0s - loss: 0.4193 - val_loss: 0.4561 - 32ms/epoch - 3ms/step\n",
      "Epoch 1159/2500\n",
      "11/11 - 0s - loss: 0.3956 - val_loss: 0.4578 - 30ms/epoch - 3ms/step\n",
      "Epoch 1160/2500\n",
      "11/11 - 0s - loss: 0.4227 - val_loss: 0.4642 - 31ms/epoch - 3ms/step\n",
      "Epoch 1161/2500\n",
      "11/11 - 0s - loss: 0.4069 - val_loss: 0.4707 - 39ms/epoch - 4ms/step\n",
      "Epoch 1162/2500\n",
      "11/11 - 0s - loss: 0.4207 - val_loss: 0.4806 - 33ms/epoch - 3ms/step\n",
      "Epoch 1163/2500\n",
      "11/11 - 0s - loss: 0.3996 - val_loss: 0.4677 - 32ms/epoch - 3ms/step\n",
      "Epoch 1164/2500\n",
      "11/11 - 0s - loss: 0.4263 - val_loss: 0.4625 - 31ms/epoch - 3ms/step\n",
      "Epoch 1165/2500\n",
      "11/11 - 0s - loss: 0.4172 - val_loss: 0.4534 - 31ms/epoch - 3ms/step\n",
      "Epoch 1166/2500\n",
      "11/11 - 0s - loss: 0.3933 - val_loss: 0.4569 - 30ms/epoch - 3ms/step\n",
      "Epoch 1167/2500\n",
      "11/11 - 0s - loss: 0.4118 - val_loss: 0.4861 - 30ms/epoch - 3ms/step\n",
      "Epoch 1168/2500\n",
      "11/11 - 0s - loss: 0.4209 - val_loss: 0.4734 - 33ms/epoch - 3ms/step\n",
      "Epoch 1169/2500\n",
      "11/11 - 0s - loss: 0.4091 - val_loss: 0.4675 - 31ms/epoch - 3ms/step\n",
      "Epoch 1170/2500\n",
      "11/11 - 0s - loss: 0.4008 - val_loss: 0.4638 - 31ms/epoch - 3ms/step\n",
      "Epoch 1171/2500\n",
      "11/11 - 0s - loss: 0.4060 - val_loss: 0.4616 - 30ms/epoch - 3ms/step\n",
      "Epoch 1172/2500\n",
      "11/11 - 0s - loss: 0.4072 - val_loss: 0.4615 - 30ms/epoch - 3ms/step\n",
      "Epoch 1173/2500\n",
      "11/11 - 0s - loss: 0.4150 - val_loss: 0.4795 - 31ms/epoch - 3ms/step\n",
      "Epoch 1174/2500\n",
      "11/11 - 0s - loss: 0.4099 - val_loss: 0.4792 - 31ms/epoch - 3ms/step\n",
      "Epoch 1175/2500\n",
      "11/11 - 0s - loss: 0.4094 - val_loss: 0.4607 - 30ms/epoch - 3ms/step\n",
      "Epoch 1176/2500\n",
      "11/11 - 0s - loss: 0.4154 - val_loss: 0.4638 - 31ms/epoch - 3ms/step\n",
      "Epoch 1177/2500\n",
      "11/11 - 0s - loss: 0.4168 - val_loss: 0.4680 - 30ms/epoch - 3ms/step\n",
      "Epoch 1178/2500\n",
      "11/11 - 0s - loss: 0.4169 - val_loss: 0.4841 - 30ms/epoch - 3ms/step\n",
      "Epoch 1179/2500\n",
      "11/11 - 0s - loss: 0.4022 - val_loss: 0.4784 - 32ms/epoch - 3ms/step\n",
      "Epoch 1180/2500\n",
      "11/11 - 0s - loss: 0.4160 - val_loss: 0.4748 - 31ms/epoch - 3ms/step\n",
      "Epoch 1181/2500\n",
      "11/11 - 0s - loss: 0.4106 - val_loss: 0.4635 - 30ms/epoch - 3ms/step\n",
      "Epoch 1182/2500\n",
      "11/11 - 0s - loss: 0.4077 - val_loss: 0.4637 - 31ms/epoch - 3ms/step\n",
      "Epoch 1183/2500\n",
      "11/11 - 0s - loss: 0.4107 - val_loss: 0.4599 - 42ms/epoch - 4ms/step\n",
      "Epoch 1184/2500\n",
      "11/11 - 0s - loss: 0.4083 - val_loss: 0.4563 - 30ms/epoch - 3ms/step\n",
      "Epoch 1185/2500\n",
      "11/11 - 0s - loss: 0.4025 - val_loss: 0.4645 - 30ms/epoch - 3ms/step\n",
      "Epoch 1186/2500\n",
      "11/11 - 0s - loss: 0.4159 - val_loss: 0.4664 - 30ms/epoch - 3ms/step\n",
      "Epoch 1187/2500\n",
      "11/11 - 0s - loss: 0.4008 - val_loss: 0.4624 - 30ms/epoch - 3ms/step\n",
      "Epoch 1188/2500\n",
      "11/11 - 0s - loss: 0.4296 - val_loss: 0.4698 - 31ms/epoch - 3ms/step\n",
      "Epoch 1189/2500\n",
      "11/11 - 0s - loss: 0.4091 - val_loss: 0.4679 - 30ms/epoch - 3ms/step\n",
      "Epoch 1190/2500\n",
      "11/11 - 0s - loss: 0.4090 - val_loss: 0.4651 - 35ms/epoch - 3ms/step\n",
      "Epoch 1191/2500\n",
      "11/11 - 0s - loss: 0.4111 - val_loss: 0.4717 - 31ms/epoch - 3ms/step\n",
      "Epoch 1192/2500\n",
      "11/11 - 0s - loss: 0.4231 - val_loss: 0.4727 - 30ms/epoch - 3ms/step\n",
      "Epoch 1193/2500\n",
      "11/11 - 0s - loss: 0.3998 - val_loss: 0.4682 - 30ms/epoch - 3ms/step\n",
      "Epoch 1194/2500\n",
      "11/11 - 0s - loss: 0.4142 - val_loss: 0.4681 - 30ms/epoch - 3ms/step\n",
      "Epoch 1195/2500\n",
      "11/11 - 0s - loss: 0.3984 - val_loss: 0.4731 - 32ms/epoch - 3ms/step\n",
      "Epoch 1196/2500\n",
      "11/11 - 0s - loss: 0.4149 - val_loss: 0.4748 - 33ms/epoch - 3ms/step\n",
      "Epoch 1197/2500\n",
      "11/11 - 0s - loss: 0.4125 - val_loss: 0.4773 - 31ms/epoch - 3ms/step\n",
      "Epoch 1198/2500\n",
      "11/11 - 0s - loss: 0.3990 - val_loss: 0.4936 - 30ms/epoch - 3ms/step\n",
      "Epoch 1199/2500\n",
      "11/11 - 0s - loss: 0.4090 - val_loss: 0.4830 - 31ms/epoch - 3ms/step\n",
      "Epoch 1200/2500\n",
      "11/11 - 0s - loss: 0.4020 - val_loss: 0.4853 - 34ms/epoch - 3ms/step\n",
      "Epoch 1201/2500\n",
      "11/11 - 0s - loss: 0.4100 - val_loss: 0.4915 - 31ms/epoch - 3ms/step\n",
      "Epoch 1202/2500\n",
      "11/11 - 0s - loss: 0.4097 - val_loss: 0.4754 - 30ms/epoch - 3ms/step\n",
      "Epoch 1203/2500\n",
      "11/11 - 0s - loss: 0.4134 - val_loss: 0.4672 - 30ms/epoch - 3ms/step\n",
      "Epoch 1204/2500\n",
      "11/11 - 0s - loss: 0.4068 - val_loss: 0.4631 - 30ms/epoch - 3ms/step\n",
      "Epoch 1205/2500\n",
      "11/11 - 0s - loss: 0.4123 - val_loss: 0.4697 - 30ms/epoch - 3ms/step\n",
      "Epoch 1206/2500\n",
      "11/11 - 0s - loss: 0.4116 - val_loss: 0.4771 - 31ms/epoch - 3ms/step\n",
      "Epoch 1207/2500\n",
      "11/11 - 0s - loss: 0.4105 - val_loss: 0.4718 - 30ms/epoch - 3ms/step\n",
      "Epoch 1208/2500\n",
      "11/11 - 0s - loss: 0.4121 - val_loss: 0.4659 - 30ms/epoch - 3ms/step\n",
      "Epoch 1209/2500\n",
      "11/11 - 0s - loss: 0.4266 - val_loss: 0.4724 - 41ms/epoch - 4ms/step\n",
      "Epoch 1210/2500\n",
      "11/11 - 0s - loss: 0.4128 - val_loss: 0.4662 - 32ms/epoch - 3ms/step\n",
      "Epoch 1211/2500\n",
      "11/11 - 0s - loss: 0.4050 - val_loss: 0.4658 - 34ms/epoch - 3ms/step\n",
      "Epoch 1212/2500\n",
      "11/11 - 0s - loss: 0.4242 - val_loss: 0.4689 - 31ms/epoch - 3ms/step\n",
      "Epoch 1213/2500\n",
      "11/11 - 0s - loss: 0.3952 - val_loss: 0.4650 - 35ms/epoch - 3ms/step\n",
      "Epoch 1214/2500\n",
      "11/11 - 0s - loss: 0.4148 - val_loss: 0.4718 - 33ms/epoch - 3ms/step\n",
      "Epoch 1215/2500\n",
      "11/11 - 0s - loss: 0.4168 - val_loss: 0.4772 - 32ms/epoch - 3ms/step\n",
      "Epoch 1216/2500\n",
      "11/11 - 0s - loss: 0.4050 - val_loss: 0.4708 - 31ms/epoch - 3ms/step\n",
      "Epoch 1217/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4604 - 33ms/epoch - 3ms/step\n",
      "Epoch 1218/2500\n",
      "11/11 - 0s - loss: 0.4155 - val_loss: 0.4669 - 32ms/epoch - 3ms/step\n",
      "Epoch 1219/2500\n",
      "11/11 - 0s - loss: 0.3998 - val_loss: 0.4723 - 33ms/epoch - 3ms/step\n",
      "Epoch 1220/2500\n",
      "11/11 - 0s - loss: 0.4157 - val_loss: 0.4705 - 35ms/epoch - 3ms/step\n",
      "Epoch 1221/2500\n",
      "11/11 - 0s - loss: 0.4181 - val_loss: 0.4599 - 38ms/epoch - 3ms/step\n",
      "Epoch 1222/2500\n",
      "11/11 - 0s - loss: 0.4203 - val_loss: 0.4770 - 34ms/epoch - 3ms/step\n",
      "Epoch 1223/2500\n",
      "11/11 - 0s - loss: 0.4279 - val_loss: 0.4678 - 33ms/epoch - 3ms/step\n",
      "Epoch 1224/2500\n",
      "11/11 - 0s - loss: 0.3989 - val_loss: 0.4588 - 33ms/epoch - 3ms/step\n",
      "Epoch 1225/2500\n",
      "11/11 - 0s - loss: 0.3949 - val_loss: 0.4658 - 32ms/epoch - 3ms/step\n",
      "Epoch 1226/2500\n",
      "11/11 - 0s - loss: 0.3829 - val_loss: 0.4858 - 32ms/epoch - 3ms/step\n",
      "Epoch 1227/2500\n",
      "11/11 - 0s - loss: 0.4181 - val_loss: 0.4772 - 32ms/epoch - 3ms/step\n",
      "Epoch 1228/2500\n",
      "11/11 - 0s - loss: 0.4189 - val_loss: 0.4683 - 30ms/epoch - 3ms/step\n",
      "Epoch 1229/2500\n",
      "11/11 - 0s - loss: 0.3964 - val_loss: 0.4677 - 38ms/epoch - 3ms/step\n",
      "Epoch 1230/2500\n",
      "11/11 - 0s - loss: 0.4088 - val_loss: 0.4867 - 33ms/epoch - 3ms/step\n",
      "Epoch 1231/2500\n",
      "11/11 - 0s - loss: 0.4144 - val_loss: 0.4746 - 32ms/epoch - 3ms/step\n",
      "Epoch 1232/2500\n",
      "11/11 - 0s - loss: 0.4254 - val_loss: 0.4915 - 31ms/epoch - 3ms/step\n",
      "Epoch 1233/2500\n",
      "11/11 - 0s - loss: 0.4216 - val_loss: 0.4800 - 30ms/epoch - 3ms/step\n",
      "Epoch 1234/2500\n",
      "11/11 - 0s - loss: 0.4128 - val_loss: 0.4640 - 44ms/epoch - 4ms/step\n",
      "Epoch 1235/2500\n",
      "11/11 - 0s - loss: 0.4037 - val_loss: 0.4664 - 31ms/epoch - 3ms/step\n",
      "Epoch 1236/2500\n",
      "11/11 - 0s - loss: 0.4137 - val_loss: 0.4805 - 31ms/epoch - 3ms/step\n",
      "Epoch 1237/2500\n",
      "11/11 - 0s - loss: 0.4075 - val_loss: 0.4674 - 31ms/epoch - 3ms/step\n",
      "Epoch 1238/2500\n",
      "11/11 - 0s - loss: 0.4013 - val_loss: 0.4716 - 31ms/epoch - 3ms/step\n",
      "Epoch 1239/2500\n",
      "11/11 - 0s - loss: 0.4041 - val_loss: 0.4630 - 31ms/epoch - 3ms/step\n",
      "Epoch 1240/2500\n",
      "11/11 - 0s - loss: 0.4029 - val_loss: 0.4793 - 33ms/epoch - 3ms/step\n",
      "Epoch 1241/2500\n",
      "11/11 - 0s - loss: 0.4326 - val_loss: 0.4751 - 31ms/epoch - 3ms/step\n",
      "Epoch 1242/2500\n",
      "11/11 - 0s - loss: 0.4047 - val_loss: 0.4750 - 30ms/epoch - 3ms/step\n",
      "Epoch 1243/2500\n",
      "11/11 - 0s - loss: 0.4060 - val_loss: 0.4709 - 31ms/epoch - 3ms/step\n",
      "Epoch 1244/2500\n",
      "11/11 - 0s - loss: 0.4035 - val_loss: 0.4747 - 30ms/epoch - 3ms/step\n",
      "Epoch 1245/2500\n",
      "11/11 - 0s - loss: 0.4198 - val_loss: 0.4714 - 31ms/epoch - 3ms/step\n",
      "Epoch 1246/2500\n",
      "11/11 - 0s - loss: 0.4295 - val_loss: 0.4590 - 30ms/epoch - 3ms/step\n",
      "Epoch 1247/2500\n",
      "11/11 - 0s - loss: 0.3906 - val_loss: 0.4572 - 30ms/epoch - 3ms/step\n",
      "Epoch 1248/2500\n",
      "11/11 - 0s - loss: 0.4179 - val_loss: 0.4667 - 30ms/epoch - 3ms/step\n",
      "Epoch 1249/2500\n",
      "11/11 - 0s - loss: 0.4103 - val_loss: 0.4831 - 30ms/epoch - 3ms/step\n",
      "Epoch 1250/2500\n",
      "11/11 - 0s - loss: 0.4284 - val_loss: 0.4622 - 33ms/epoch - 3ms/step\n",
      "Epoch 1251/2500\n",
      "11/11 - 0s - loss: 0.4012 - val_loss: 0.4643 - 30ms/epoch - 3ms/step\n",
      "Epoch 1252/2500\n",
      "11/11 - 0s - loss: 0.4096 - val_loss: 0.4711 - 31ms/epoch - 3ms/step\n",
      "Epoch 1253/2500\n",
      "11/11 - 0s - loss: 0.4063 - val_loss: 0.4574 - 30ms/epoch - 3ms/step\n",
      "Epoch 1254/2500\n",
      "11/11 - 0s - loss: 0.4112 - val_loss: 0.4557 - 30ms/epoch - 3ms/step\n",
      "Epoch 1255/2500\n",
      "11/11 - 0s - loss: 0.3753 - val_loss: 0.4755 - 33ms/epoch - 3ms/step\n",
      "Epoch 1256/2500\n",
      "11/11 - 0s - loss: 0.4112 - val_loss: 0.4717 - 30ms/epoch - 3ms/step\n",
      "Epoch 1257/2500\n",
      "11/11 - 0s - loss: 0.4123 - val_loss: 0.4704 - 47ms/epoch - 4ms/step\n",
      "Epoch 1258/2500\n",
      "11/11 - 0s - loss: 0.4013 - val_loss: 0.4729 - 37ms/epoch - 3ms/step\n",
      "Epoch 1259/2500\n",
      "11/11 - 0s - loss: 0.4105 - val_loss: 0.4931 - 36ms/epoch - 3ms/step\n",
      "Epoch 1260/2500\n",
      "11/11 - 0s - loss: 0.4152 - val_loss: 0.4662 - 39ms/epoch - 4ms/step\n",
      "Epoch 1261/2500\n",
      "11/11 - 0s - loss: 0.3998 - val_loss: 0.4711 - 31ms/epoch - 3ms/step\n",
      "Epoch 1262/2500\n",
      "11/11 - 0s - loss: 0.4070 - val_loss: 0.4691 - 31ms/epoch - 3ms/step\n",
      "Epoch 1263/2500\n",
      "11/11 - 0s - loss: 0.4045 - val_loss: 0.4700 - 30ms/epoch - 3ms/step\n",
      "Epoch 1264/2500\n",
      "11/11 - 0s - loss: 0.4086 - val_loss: 0.4738 - 30ms/epoch - 3ms/step\n",
      "Epoch 1265/2500\n",
      "11/11 - 0s - loss: 0.3929 - val_loss: 0.4715 - 30ms/epoch - 3ms/step\n",
      "Epoch 1266/2500\n",
      "11/11 - 0s - loss: 0.4054 - val_loss: 0.4670 - 30ms/epoch - 3ms/step\n",
      "Epoch 1267/2500\n",
      "11/11 - 0s - loss: 0.4035 - val_loss: 0.4772 - 30ms/epoch - 3ms/step\n",
      "Epoch 1268/2500\n",
      "11/11 - 0s - loss: 0.3975 - val_loss: 0.4732 - 31ms/epoch - 3ms/step\n",
      "Epoch 1269/2500\n",
      "11/11 - 0s - loss: 0.3883 - val_loss: 0.4689 - 32ms/epoch - 3ms/step\n",
      "Epoch 1270/2500\n",
      "11/11 - 0s - loss: 0.3891 - val_loss: 0.4681 - 37ms/epoch - 3ms/step\n",
      "Epoch 1271/2500\n",
      "11/11 - 0s - loss: 0.4060 - val_loss: 0.4685 - 33ms/epoch - 3ms/step\n",
      "Epoch 1272/2500\n",
      "11/11 - 0s - loss: 0.4105 - val_loss: 0.4646 - 32ms/epoch - 3ms/step\n",
      "Epoch 1273/2500\n",
      "11/11 - 0s - loss: 0.4022 - val_loss: 0.4674 - 32ms/epoch - 3ms/step\n",
      "Epoch 1274/2500\n",
      "11/11 - 0s - loss: 0.4010 - val_loss: 0.4694 - 33ms/epoch - 3ms/step\n",
      "Epoch 1275/2500\n",
      "11/11 - 0s - loss: 0.4078 - val_loss: 0.4739 - 32ms/epoch - 3ms/step\n",
      "Epoch 1276/2500\n",
      "11/11 - 0s - loss: 0.4231 - val_loss: 0.4599 - 32ms/epoch - 3ms/step\n",
      "Epoch 1277/2500\n",
      "11/11 - 0s - loss: 0.3976 - val_loss: 0.4669 - 32ms/epoch - 3ms/step\n",
      "Epoch 1278/2500\n",
      "11/11 - 0s - loss: 0.4098 - val_loss: 0.4715 - 32ms/epoch - 3ms/step\n",
      "Epoch 1279/2500\n",
      "11/11 - 0s - loss: 0.4112 - val_loss: 0.4693 - 32ms/epoch - 3ms/step\n",
      "Epoch 1280/2500\n",
      "11/11 - 0s - loss: 0.4124 - val_loss: 0.4640 - 57ms/epoch - 5ms/step\n",
      "Epoch 1281/2500\n",
      "11/11 - 0s - loss: 0.4036 - val_loss: 0.4770 - 32ms/epoch - 3ms/step\n",
      "Epoch 1282/2500\n",
      "11/11 - 0s - loss: 0.3956 - val_loss: 0.4768 - 37ms/epoch - 3ms/step\n",
      "Epoch 1283/2500\n",
      "11/11 - 0s - loss: 0.4213 - val_loss: 0.4725 - 32ms/epoch - 3ms/step\n",
      "Epoch 1284/2500\n",
      "11/11 - 0s - loss: 0.4037 - val_loss: 0.4658 - 35ms/epoch - 3ms/step\n",
      "Epoch 1285/2500\n",
      "11/11 - 0s - loss: 0.3897 - val_loss: 0.4677 - 37ms/epoch - 3ms/step\n",
      "Epoch 1286/2500\n",
      "11/11 - 0s - loss: 0.4070 - val_loss: 0.4721 - 35ms/epoch - 3ms/step\n",
      "Epoch 1287/2500\n",
      "11/11 - 0s - loss: 0.3904 - val_loss: 0.4771 - 35ms/epoch - 3ms/step\n",
      "Epoch 1288/2500\n",
      "11/11 - 0s - loss: 0.3963 - val_loss: 0.4791 - 35ms/epoch - 3ms/step\n",
      "Epoch 1289/2500\n",
      "11/11 - 0s - loss: 0.3999 - val_loss: 0.4909 - 35ms/epoch - 3ms/step\n",
      "Epoch 1290/2500\n",
      "11/11 - 0s - loss: 0.4002 - val_loss: 0.4785 - 37ms/epoch - 3ms/step\n",
      "Epoch 1291/2500\n",
      "11/11 - 0s - loss: 0.4178 - val_loss: 0.4789 - 35ms/epoch - 3ms/step\n",
      "Epoch 1292/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.4771 - 33ms/epoch - 3ms/step\n",
      "Epoch 1293/2500\n",
      "11/11 - 0s - loss: 0.4194 - val_loss: 0.4699 - 32ms/epoch - 3ms/step\n",
      "Epoch 1294/2500\n",
      "11/11 - 0s - loss: 0.4091 - val_loss: 0.4868 - 33ms/epoch - 3ms/step\n",
      "Epoch 1295/2500\n",
      "11/11 - 0s - loss: 0.3965 - val_loss: 0.4884 - 32ms/epoch - 3ms/step\n",
      "Epoch 1296/2500\n",
      "11/11 - 0s - loss: 0.4091 - val_loss: 0.4762 - 32ms/epoch - 3ms/step\n",
      "Epoch 1297/2500\n",
      "11/11 - 0s - loss: 0.3993 - val_loss: 0.4732 - 33ms/epoch - 3ms/step\n",
      "Epoch 1298/2500\n",
      "11/11 - 0s - loss: 0.4067 - val_loss: 0.4620 - 33ms/epoch - 3ms/step\n",
      "Epoch 1299/2500\n",
      "11/11 - 0s - loss: 0.4134 - val_loss: 0.4662 - 34ms/epoch - 3ms/step\n",
      "Epoch 1300/2500\n",
      "11/11 - 0s - loss: 0.4019 - val_loss: 0.4646 - 38ms/epoch - 3ms/step\n",
      "Epoch 1301/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4645 - 40ms/epoch - 4ms/step\n",
      "Epoch 1302/2500\n",
      "11/11 - 0s - loss: 0.3987 - val_loss: 0.4703 - 36ms/epoch - 3ms/step\n",
      "Epoch 1303/2500\n",
      "11/11 - 0s - loss: 0.4195 - val_loss: 0.4727 - 31ms/epoch - 3ms/step\n",
      "Epoch 1304/2500\n",
      "11/11 - 0s - loss: 0.3940 - val_loss: 0.4675 - 32ms/epoch - 3ms/step\n",
      "Epoch 1305/2500\n",
      "11/11 - 0s - loss: 0.4182 - val_loss: 0.4699 - 32ms/epoch - 3ms/step\n",
      "Epoch 1306/2500\n",
      "11/11 - 0s - loss: 0.4119 - val_loss: 0.4822 - 31ms/epoch - 3ms/step\n",
      "Epoch 1307/2500\n",
      "11/11 - 0s - loss: 0.4073 - val_loss: 0.4774 - 31ms/epoch - 3ms/step\n",
      "Epoch 1308/2500\n",
      "11/11 - 0s - loss: 0.4049 - val_loss: 0.4886 - 32ms/epoch - 3ms/step\n",
      "Epoch 1309/2500\n",
      "11/11 - 0s - loss: 0.3977 - val_loss: 0.5068 - 31ms/epoch - 3ms/step\n",
      "Epoch 1310/2500\n",
      "11/11 - 0s - loss: 0.3927 - val_loss: 0.4800 - 33ms/epoch - 3ms/step\n",
      "Epoch 1311/2500\n",
      "11/11 - 0s - loss: 0.4021 - val_loss: 0.4762 - 32ms/epoch - 3ms/step\n",
      "Epoch 1312/2500\n",
      "11/11 - 0s - loss: 0.4023 - val_loss: 0.4771 - 31ms/epoch - 3ms/step\n",
      "Epoch 1313/2500\n",
      "11/11 - 0s - loss: 0.4140 - val_loss: 0.4810 - 32ms/epoch - 3ms/step\n",
      "Epoch 1314/2500\n",
      "11/11 - 0s - loss: 0.3989 - val_loss: 0.4774 - 32ms/epoch - 3ms/step\n",
      "Epoch 1315/2500\n",
      "11/11 - 0s - loss: 0.4104 - val_loss: 0.4806 - 30ms/epoch - 3ms/step\n",
      "Epoch 1316/2500\n",
      "11/11 - 0s - loss: 0.3963 - val_loss: 0.4865 - 32ms/epoch - 3ms/step\n",
      "Epoch 1317/2500\n",
      "11/11 - 0s - loss: 0.4070 - val_loss: 0.4752 - 33ms/epoch - 3ms/step\n",
      "Epoch 1318/2500\n",
      "11/11 - 0s - loss: 0.4138 - val_loss: 0.4740 - 32ms/epoch - 3ms/step\n",
      "Epoch 1319/2500\n",
      "11/11 - 0s - loss: 0.3926 - val_loss: 0.4740 - 37ms/epoch - 3ms/step\n",
      "Epoch 1320/2500\n",
      "11/11 - 0s - loss: 0.4031 - val_loss: 0.4767 - 33ms/epoch - 3ms/step\n",
      "Epoch 1321/2500\n",
      "11/11 - 0s - loss: 0.3985 - val_loss: 0.4682 - 33ms/epoch - 3ms/step\n",
      "Epoch 1322/2500\n",
      "11/11 - 0s - loss: 0.4137 - val_loss: 0.4718 - 44ms/epoch - 4ms/step\n",
      "Epoch 1323/2500\n",
      "11/11 - 0s - loss: 0.4209 - val_loss: 0.4742 - 40ms/epoch - 4ms/step\n",
      "Epoch 1324/2500\n",
      "11/11 - 0s - loss: 0.3995 - val_loss: 0.4723 - 36ms/epoch - 3ms/step\n",
      "Epoch 1325/2500\n",
      "11/11 - 0s - loss: 0.4059 - val_loss: 0.4753 - 35ms/epoch - 3ms/step\n",
      "Epoch 1326/2500\n",
      "11/11 - 0s - loss: 0.4033 - val_loss: 0.4686 - 36ms/epoch - 3ms/step\n",
      "Epoch 1327/2500\n",
      "11/11 - 0s - loss: 0.4011 - val_loss: 0.4797 - 34ms/epoch - 3ms/step\n",
      "Epoch 1328/2500\n",
      "11/11 - 0s - loss: 0.4128 - val_loss: 0.4654 - 35ms/epoch - 3ms/step\n",
      "Epoch 1329/2500\n",
      "11/11 - 0s - loss: 0.4137 - val_loss: 0.4594 - 36ms/epoch - 3ms/step\n",
      "Epoch 1330/2500\n",
      "11/11 - 0s - loss: 0.3990 - val_loss: 0.4675 - 32ms/epoch - 3ms/step\n",
      "Epoch 1331/2500\n",
      "11/11 - 0s - loss: 0.4187 - val_loss: 0.4662 - 32ms/epoch - 3ms/step\n",
      "Epoch 1332/2500\n",
      "11/11 - 0s - loss: 0.4226 - val_loss: 0.4666 - 32ms/epoch - 3ms/step\n",
      "Epoch 1333/2500\n",
      "11/11 - 0s - loss: 0.4053 - val_loss: 0.4709 - 30ms/epoch - 3ms/step\n",
      "Epoch 1334/2500\n",
      "11/11 - 0s - loss: 0.4028 - val_loss: 0.4772 - 30ms/epoch - 3ms/step\n",
      "Epoch 1335/2500\n",
      "11/11 - 0s - loss: 0.4048 - val_loss: 0.4790 - 30ms/epoch - 3ms/step\n",
      "Epoch 1336/2500\n",
      "11/11 - 0s - loss: 0.4089 - val_loss: 0.4605 - 31ms/epoch - 3ms/step\n",
      "Epoch 1337/2500\n",
      "11/11 - 0s - loss: 0.4161 - val_loss: 0.4704 - 33ms/epoch - 3ms/step\n",
      "Epoch 1338/2500\n",
      "11/11 - 0s - loss: 0.4197 - val_loss: 0.4765 - 32ms/epoch - 3ms/step\n",
      "Epoch 1339/2500\n",
      "11/11 - 0s - loss: 0.3927 - val_loss: 0.4838 - 35ms/epoch - 3ms/step\n",
      "Epoch 1340/2500\n",
      "11/11 - 0s - loss: 0.4109 - val_loss: 0.4704 - 33ms/epoch - 3ms/step\n",
      "Epoch 1341/2500\n",
      "11/11 - 0s - loss: 0.4085 - val_loss: 0.4669 - 31ms/epoch - 3ms/step\n",
      "Epoch 1342/2500\n",
      "11/11 - 0s - loss: 0.3900 - val_loss: 0.4865 - 31ms/epoch - 3ms/step\n",
      "Epoch 1343/2500\n",
      "11/11 - 0s - loss: 0.4223 - val_loss: 0.4763 - 30ms/epoch - 3ms/step\n",
      "Epoch 1344/2500\n",
      "11/11 - 0s - loss: 0.4000 - val_loss: 0.4722 - 34ms/epoch - 3ms/step\n",
      "Epoch 1345/2500\n",
      "11/11 - 0s - loss: 0.4011 - val_loss: 0.4726 - 40ms/epoch - 4ms/step\n",
      "Epoch 1346/2500\n",
      "11/11 - 0s - loss: 0.4145 - val_loss: 0.4670 - 33ms/epoch - 3ms/step\n",
      "Epoch 1347/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4729 - 34ms/epoch - 3ms/step\n",
      "Epoch 1348/2500\n",
      "11/11 - 0s - loss: 0.4141 - val_loss: 0.4758 - 36ms/epoch - 3ms/step\n",
      "Epoch 1349/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4915 - 39ms/epoch - 4ms/step\n",
      "Epoch 1350/2500\n",
      "11/11 - 0s - loss: 0.4173 - val_loss: 0.4841 - 34ms/epoch - 3ms/step\n",
      "Epoch 1351/2500\n",
      "11/11 - 0s - loss: 0.3972 - val_loss: 0.4626 - 31ms/epoch - 3ms/step\n",
      "Epoch 1352/2500\n",
      "11/11 - 0s - loss: 0.3873 - val_loss: 0.4711 - 30ms/epoch - 3ms/step\n",
      "Epoch 1353/2500\n",
      "11/11 - 0s - loss: 0.3812 - val_loss: 0.4608 - 30ms/epoch - 3ms/step\n",
      "Epoch 1354/2500\n",
      "11/11 - 0s - loss: 0.4048 - val_loss: 0.4661 - 31ms/epoch - 3ms/step\n",
      "Epoch 1355/2500\n",
      "11/11 - 0s - loss: 0.3966 - val_loss: 0.4764 - 30ms/epoch - 3ms/step\n",
      "Epoch 1356/2500\n",
      "11/11 - 0s - loss: 0.4070 - val_loss: 0.4785 - 31ms/epoch - 3ms/step\n",
      "Epoch 1357/2500\n",
      "11/11 - 0s - loss: 0.3960 - val_loss: 0.4718 - 31ms/epoch - 3ms/step\n",
      "Epoch 1358/2500\n",
      "11/11 - 0s - loss: 0.4063 - val_loss: 0.4696 - 40ms/epoch - 4ms/step\n",
      "Epoch 1359/2500\n",
      "11/11 - 0s - loss: 0.3981 - val_loss: 0.4637 - 33ms/epoch - 3ms/step\n",
      "Epoch 1360/2500\n",
      "11/11 - 0s - loss: 0.4129 - val_loss: 0.4761 - 34ms/epoch - 3ms/step\n",
      "Epoch 1361/2500\n",
      "11/11 - 0s - loss: 0.4149 - val_loss: 0.4786 - 33ms/epoch - 3ms/step\n",
      "Epoch 1362/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4736 - 32ms/epoch - 3ms/step\n",
      "Epoch 1363/2500\n",
      "11/11 - 0s - loss: 0.3936 - val_loss: 0.4763 - 30ms/epoch - 3ms/step\n",
      "Epoch 1364/2500\n",
      "11/11 - 0s - loss: 0.3916 - val_loss: 0.4749 - 31ms/epoch - 3ms/step\n",
      "Epoch 1365/2500\n",
      "11/11 - 0s - loss: 0.3988 - val_loss: 0.4647 - 31ms/epoch - 3ms/step\n",
      "Epoch 1366/2500\n",
      "11/11 - 0s - loss: 0.4027 - val_loss: 0.4699 - 30ms/epoch - 3ms/step\n",
      "Epoch 1367/2500\n",
      "11/11 - 0s - loss: 0.4095 - val_loss: 0.4785 - 53ms/epoch - 5ms/step\n",
      "Epoch 1368/2500\n",
      "11/11 - 0s - loss: 0.4044 - val_loss: 0.4641 - 31ms/epoch - 3ms/step\n",
      "Epoch 1369/2500\n",
      "11/11 - 0s - loss: 0.3860 - val_loss: 0.4764 - 30ms/epoch - 3ms/step\n",
      "Epoch 1370/2500\n",
      "11/11 - 0s - loss: 0.4031 - val_loss: 0.4817 - 31ms/epoch - 3ms/step\n",
      "Epoch 1371/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.4746 - 30ms/epoch - 3ms/step\n",
      "Epoch 1372/2500\n",
      "11/11 - 0s - loss: 0.4039 - val_loss: 0.4644 - 30ms/epoch - 3ms/step\n",
      "Epoch 1373/2500\n",
      "11/11 - 0s - loss: 0.3909 - val_loss: 0.4811 - 32ms/epoch - 3ms/step\n",
      "Epoch 1374/2500\n",
      "11/11 - 0s - loss: 0.4149 - val_loss: 0.4746 - 31ms/epoch - 3ms/step\n",
      "Epoch 1375/2500\n",
      "11/11 - 0s - loss: 0.4043 - val_loss: 0.4682 - 30ms/epoch - 3ms/step\n",
      "Epoch 1376/2500\n",
      "11/11 - 0s - loss: 0.4080 - val_loss: 0.4801 - 34ms/epoch - 3ms/step\n",
      "Epoch 1377/2500\n",
      "11/11 - 0s - loss: 0.3963 - val_loss: 0.4734 - 31ms/epoch - 3ms/step\n",
      "Epoch 1378/2500\n",
      "11/11 - 0s - loss: 0.3988 - val_loss: 0.4766 - 30ms/epoch - 3ms/step\n",
      "Epoch 1379/2500\n",
      "11/11 - 0s - loss: 0.3970 - val_loss: 0.4746 - 30ms/epoch - 3ms/step\n",
      "Epoch 1380/2500\n",
      "11/11 - 0s - loss: 0.3861 - val_loss: 0.4688 - 30ms/epoch - 3ms/step\n",
      "Epoch 1381/2500\n",
      "11/11 - 0s - loss: 0.4107 - val_loss: 0.4671 - 31ms/epoch - 3ms/step\n",
      "Epoch 1382/2500\n",
      "11/11 - 0s - loss: 0.3941 - val_loss: 0.4893 - 30ms/epoch - 3ms/step\n",
      "Epoch 1383/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4946 - 31ms/epoch - 3ms/step\n",
      "Epoch 1384/2500\n",
      "11/11 - 0s - loss: 0.4033 - val_loss: 0.4708 - 30ms/epoch - 3ms/step\n",
      "Epoch 1385/2500\n",
      "11/11 - 0s - loss: 0.4004 - val_loss: 0.4631 - 32ms/epoch - 3ms/step\n",
      "Epoch 1386/2500\n",
      "11/11 - 0s - loss: 0.4028 - val_loss: 0.4781 - 31ms/epoch - 3ms/step\n",
      "Epoch 1387/2500\n",
      "11/11 - 0s - loss: 0.4120 - val_loss: 0.4633 - 31ms/epoch - 3ms/step\n",
      "Epoch 1388/2500\n",
      "11/11 - 0s - loss: 0.4172 - val_loss: 0.4625 - 31ms/epoch - 3ms/step\n",
      "Epoch 1389/2500\n",
      "11/11 - 0s - loss: 0.4092 - val_loss: 0.4788 - 29ms/epoch - 3ms/step\n",
      "Epoch 1390/2500\n",
      "11/11 - 0s - loss: 0.3955 - val_loss: 0.4742 - 30ms/epoch - 3ms/step\n",
      "Epoch 1391/2500\n",
      "11/11 - 0s - loss: 0.4154 - val_loss: 0.4863 - 40ms/epoch - 4ms/step\n",
      "Epoch 1392/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4873 - 33ms/epoch - 3ms/step\n",
      "Epoch 1393/2500\n",
      "11/11 - 0s - loss: 0.4063 - val_loss: 0.4832 - 31ms/epoch - 3ms/step\n",
      "Epoch 1394/2500\n",
      "11/11 - 0s - loss: 0.4234 - val_loss: 0.4809 - 33ms/epoch - 3ms/step\n",
      "Epoch 1395/2500\n",
      "11/11 - 0s - loss: 0.4008 - val_loss: 0.4850 - 38ms/epoch - 3ms/step\n",
      "Epoch 1396/2500\n",
      "11/11 - 0s - loss: 0.3962 - val_loss: 0.4711 - 34ms/epoch - 3ms/step\n",
      "Epoch 1397/2500\n",
      "11/11 - 0s - loss: 0.3943 - val_loss: 0.4715 - 35ms/epoch - 3ms/step\n",
      "Epoch 1398/2500\n",
      "11/11 - 0s - loss: 0.4069 - val_loss: 0.4737 - 35ms/epoch - 3ms/step\n",
      "Epoch 1399/2500\n",
      "11/11 - 0s - loss: 0.4080 - val_loss: 0.4682 - 35ms/epoch - 3ms/step\n",
      "Epoch 1400/2500\n",
      "11/11 - 0s - loss: 0.4070 - val_loss: 0.4704 - 34ms/epoch - 3ms/step\n",
      "Epoch 1401/2500\n",
      "11/11 - 0s - loss: 0.3952 - val_loss: 0.4827 - 35ms/epoch - 3ms/step\n",
      "Epoch 1402/2500\n",
      "11/11 - 0s - loss: 0.4067 - val_loss: 0.4714 - 32ms/epoch - 3ms/step\n",
      "Epoch 1403/2500\n",
      "11/11 - 0s - loss: 0.4022 - val_loss: 0.4673 - 37ms/epoch - 3ms/step\n",
      "Epoch 1404/2500\n",
      "11/11 - 0s - loss: 0.4002 - val_loss: 0.4860 - 35ms/epoch - 3ms/step\n",
      "Epoch 1405/2500\n",
      "11/11 - 0s - loss: 0.4158 - val_loss: 0.4859 - 34ms/epoch - 3ms/step\n",
      "Epoch 1406/2500\n",
      "11/11 - 0s - loss: 0.4125 - val_loss: 0.4728 - 33ms/epoch - 3ms/step\n",
      "Epoch 1407/2500\n",
      "11/11 - 0s - loss: 0.3992 - val_loss: 0.4714 - 33ms/epoch - 3ms/step\n",
      "Epoch 1408/2500\n",
      "11/11 - 0s - loss: 0.4151 - val_loss: 0.4775 - 34ms/epoch - 3ms/step\n",
      "Epoch 1409/2500\n",
      "11/11 - 0s - loss: 0.3821 - val_loss: 0.4633 - 33ms/epoch - 3ms/step\n",
      "Epoch 1410/2500\n",
      "11/11 - 0s - loss: 0.4000 - val_loss: 0.4742 - 33ms/epoch - 3ms/step\n",
      "Epoch 1411/2500\n",
      "11/11 - 0s - loss: 0.4180 - val_loss: 0.4902 - 41ms/epoch - 4ms/step\n",
      "Epoch 1412/2500\n",
      "11/11 - 0s - loss: 0.3914 - val_loss: 0.4797 - 42ms/epoch - 4ms/step\n",
      "Epoch 1413/2500\n",
      "11/11 - 0s - loss: 0.3933 - val_loss: 0.4797 - 32ms/epoch - 3ms/step\n",
      "Epoch 1414/2500\n",
      "11/11 - 0s - loss: 0.4004 - val_loss: 0.4745 - 33ms/epoch - 3ms/step\n",
      "Epoch 1415/2500\n",
      "11/11 - 0s - loss: 0.4042 - val_loss: 0.4808 - 32ms/epoch - 3ms/step\n",
      "Epoch 1416/2500\n",
      "11/11 - 0s - loss: 0.4064 - val_loss: 0.4939 - 31ms/epoch - 3ms/step\n",
      "Epoch 1417/2500\n",
      "11/11 - 0s - loss: 0.4146 - val_loss: 0.4785 - 31ms/epoch - 3ms/step\n",
      "Epoch 1418/2500\n",
      "11/11 - 0s - loss: 0.4130 - val_loss: 0.4767 - 32ms/epoch - 3ms/step\n",
      "Epoch 1419/2500\n",
      "11/11 - 0s - loss: 0.3869 - val_loss: 0.4858 - 32ms/epoch - 3ms/step\n",
      "Epoch 1420/2500\n",
      "11/11 - 0s - loss: 0.3989 - val_loss: 0.4824 - 40ms/epoch - 4ms/step\n",
      "Epoch 1421/2500\n",
      "11/11 - 0s - loss: 0.4034 - val_loss: 0.4902 - 36ms/epoch - 3ms/step\n",
      "Epoch 1422/2500\n",
      "11/11 - 0s - loss: 0.3956 - val_loss: 0.4809 - 38ms/epoch - 3ms/step\n",
      "Epoch 1423/2500\n",
      "11/11 - 0s - loss: 0.3714 - val_loss: 0.4712 - 31ms/epoch - 3ms/step\n",
      "Epoch 1424/2500\n",
      "11/11 - 0s - loss: 0.4085 - val_loss: 0.4740 - 32ms/epoch - 3ms/step\n",
      "Epoch 1425/2500\n",
      "11/11 - 0s - loss: 0.3811 - val_loss: 0.4764 - 32ms/epoch - 3ms/step\n",
      "Epoch 1426/2500\n",
      "11/11 - 0s - loss: 0.3909 - val_loss: 0.4730 - 32ms/epoch - 3ms/step\n",
      "Epoch 1427/2500\n",
      "11/11 - 0s - loss: 0.4026 - val_loss: 0.4745 - 32ms/epoch - 3ms/step\n",
      "Epoch 1428/2500\n",
      "11/11 - 0s - loss: 0.4094 - val_loss: 0.4867 - 34ms/epoch - 3ms/step\n",
      "Epoch 1429/2500\n",
      "11/11 - 0s - loss: 0.3893 - val_loss: 0.4819 - 31ms/epoch - 3ms/step\n",
      "Epoch 1430/2500\n",
      "11/11 - 0s - loss: 0.3987 - val_loss: 0.4845 - 30ms/epoch - 3ms/step\n",
      "Epoch 1431/2500\n",
      "11/11 - 0s - loss: 0.3968 - val_loss: 0.4713 - 31ms/epoch - 3ms/step\n",
      "Epoch 1432/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.4624 - 32ms/epoch - 3ms/step\n",
      "Epoch 1433/2500\n",
      "11/11 - 0s - loss: 0.3941 - val_loss: 0.4644 - 39ms/epoch - 4ms/step\n",
      "Epoch 1434/2500\n",
      "11/11 - 0s - loss: 0.3962 - val_loss: 0.4720 - 73ms/epoch - 7ms/step\n",
      "Epoch 1435/2500\n",
      "11/11 - 0s - loss: 0.3963 - val_loss: 0.4753 - 43ms/epoch - 4ms/step\n",
      "Epoch 1436/2500\n",
      "11/11 - 0s - loss: 0.4129 - val_loss: 0.4786 - 44ms/epoch - 4ms/step\n",
      "Epoch 1437/2500\n",
      "11/11 - 0s - loss: 0.3930 - val_loss: 0.4728 - 44ms/epoch - 4ms/step\n",
      "Epoch 1438/2500\n",
      "11/11 - 0s - loss: 0.3948 - val_loss: 0.4809 - 39ms/epoch - 4ms/step\n",
      "Epoch 1439/2500\n",
      "11/11 - 0s - loss: 0.3975 - val_loss: 0.4906 - 40ms/epoch - 4ms/step\n",
      "Epoch 1440/2500\n",
      "11/11 - 0s - loss: 0.3902 - val_loss: 0.4814 - 37ms/epoch - 3ms/step\n",
      "Epoch 1441/2500\n",
      "11/11 - 0s - loss: 0.3952 - val_loss: 0.4741 - 35ms/epoch - 3ms/step\n",
      "Epoch 1442/2500\n",
      "11/11 - 0s - loss: 0.3952 - val_loss: 0.4684 - 37ms/epoch - 3ms/step\n",
      "Epoch 1443/2500\n",
      "11/11 - 0s - loss: 0.3941 - val_loss: 0.4743 - 35ms/epoch - 3ms/step\n",
      "Epoch 1444/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4766 - 40ms/epoch - 4ms/step\n",
      "Epoch 1445/2500\n",
      "11/11 - 0s - loss: 0.3961 - val_loss: 0.4727 - 37ms/epoch - 3ms/step\n",
      "Epoch 1446/2500\n",
      "11/11 - 0s - loss: 0.3973 - val_loss: 0.4768 - 39ms/epoch - 4ms/step\n",
      "Epoch 1447/2500\n",
      "11/11 - 0s - loss: 0.4170 - val_loss: 0.4796 - 40ms/epoch - 4ms/step\n",
      "Epoch 1448/2500\n",
      "11/11 - 0s - loss: 0.3900 - val_loss: 0.4807 - 38ms/epoch - 3ms/step\n",
      "Epoch 1449/2500\n",
      "11/11 - 0s - loss: 0.4041 - val_loss: 0.4713 - 38ms/epoch - 3ms/step\n",
      "Epoch 1450/2500\n",
      "11/11 - 0s - loss: 0.4058 - val_loss: 0.4848 - 38ms/epoch - 3ms/step\n",
      "Epoch 1451/2500\n",
      "11/11 - 0s - loss: 0.4087 - val_loss: 0.4832 - 41ms/epoch - 4ms/step\n",
      "Epoch 1452/2500\n",
      "11/11 - 0s - loss: 0.4142 - val_loss: 0.4690 - 34ms/epoch - 3ms/step\n",
      "Epoch 1453/2500\n",
      "11/11 - 0s - loss: 0.3957 - val_loss: 0.4784 - 38ms/epoch - 3ms/step\n",
      "Epoch 1454/2500\n",
      "11/11 - 0s - loss: 0.4024 - val_loss: 0.4876 - 35ms/epoch - 3ms/step\n",
      "Epoch 1455/2500\n",
      "11/11 - 0s - loss: 0.3966 - val_loss: 0.4825 - 33ms/epoch - 3ms/step\n",
      "Epoch 1456/2500\n",
      "11/11 - 0s - loss: 0.3957 - val_loss: 0.4876 - 42ms/epoch - 4ms/step\n",
      "Epoch 1457/2500\n",
      "11/11 - 0s - loss: 0.3866 - val_loss: 0.4885 - 52ms/epoch - 5ms/step\n",
      "Epoch 1458/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4748 - 35ms/epoch - 3ms/step\n",
      "Epoch 1459/2500\n",
      "11/11 - 0s - loss: 0.3935 - val_loss: 0.4718 - 34ms/epoch - 3ms/step\n",
      "Epoch 1460/2500\n",
      "11/11 - 0s - loss: 0.4012 - val_loss: 0.4689 - 36ms/epoch - 3ms/step\n",
      "Epoch 1461/2500\n",
      "11/11 - 0s - loss: 0.4045 - val_loss: 0.4771 - 37ms/epoch - 3ms/step\n",
      "Epoch 1462/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4673 - 42ms/epoch - 4ms/step\n",
      "Epoch 1463/2500\n",
      "11/11 - 0s - loss: 0.3900 - val_loss: 0.4690 - 41ms/epoch - 4ms/step\n",
      "Epoch 1464/2500\n",
      "11/11 - 0s - loss: 0.3994 - val_loss: 0.4833 - 36ms/epoch - 3ms/step\n",
      "Epoch 1465/2500\n",
      "11/11 - 0s - loss: 0.3990 - val_loss: 0.4837 - 40ms/epoch - 4ms/step\n",
      "Epoch 1466/2500\n",
      "11/11 - 0s - loss: 0.4000 - val_loss: 0.4664 - 36ms/epoch - 3ms/step\n",
      "Epoch 1467/2500\n",
      "11/11 - 0s - loss: 0.4048 - val_loss: 0.4660 - 37ms/epoch - 3ms/step\n",
      "Epoch 1468/2500\n",
      "11/11 - 0s - loss: 0.3898 - val_loss: 0.4726 - 36ms/epoch - 3ms/step\n",
      "Epoch 1469/2500\n",
      "11/11 - 0s - loss: 0.4074 - val_loss: 0.4742 - 47ms/epoch - 4ms/step\n",
      "Epoch 1470/2500\n",
      "11/11 - 0s - loss: 0.3894 - val_loss: 0.4797 - 41ms/epoch - 4ms/step\n",
      "Epoch 1471/2500\n",
      "11/11 - 0s - loss: 0.3895 - val_loss: 0.4733 - 34ms/epoch - 3ms/step\n",
      "Epoch 1472/2500\n",
      "11/11 - 0s - loss: 0.3956 - val_loss: 0.4672 - 34ms/epoch - 3ms/step\n",
      "Epoch 1473/2500\n",
      "11/11 - 0s - loss: 0.4058 - val_loss: 0.4732 - 35ms/epoch - 3ms/step\n",
      "Epoch 1474/2500\n",
      "11/11 - 0s - loss: 0.4043 - val_loss: 0.4717 - 33ms/epoch - 3ms/step\n",
      "Epoch 1475/2500\n",
      "11/11 - 0s - loss: 0.3993 - val_loss: 0.4691 - 34ms/epoch - 3ms/step\n",
      "Epoch 1476/2500\n",
      "11/11 - 0s - loss: 0.4016 - val_loss: 0.4697 - 34ms/epoch - 3ms/step\n",
      "Epoch 1477/2500\n",
      "11/11 - 0s - loss: 0.4048 - val_loss: 0.4768 - 36ms/epoch - 3ms/step\n",
      "Epoch 1478/2500\n",
      "11/11 - 0s - loss: 0.3942 - val_loss: 0.4763 - 31ms/epoch - 3ms/step\n",
      "Epoch 1479/2500\n",
      "11/11 - 0s - loss: 0.4049 - val_loss: 0.4738 - 34ms/epoch - 3ms/step\n",
      "Epoch 1480/2500\n",
      "11/11 - 0s - loss: 0.4070 - val_loss: 0.4814 - 32ms/epoch - 3ms/step\n",
      "Epoch 1481/2500\n",
      "11/11 - 0s - loss: 0.4075 - val_loss: 0.4784 - 32ms/epoch - 3ms/step\n",
      "Epoch 1482/2500\n",
      "11/11 - 0s - loss: 0.4101 - val_loss: 0.4778 - 31ms/epoch - 3ms/step\n",
      "Epoch 1483/2500\n",
      "11/11 - 0s - loss: 0.4024 - val_loss: 0.4739 - 33ms/epoch - 3ms/step\n",
      "Epoch 1484/2500\n",
      "11/11 - 0s - loss: 0.4052 - val_loss: 0.4662 - 33ms/epoch - 3ms/step\n",
      "Epoch 1485/2500\n",
      "11/11 - 0s - loss: 0.3964 - val_loss: 0.4743 - 31ms/epoch - 3ms/step\n",
      "Epoch 1486/2500\n",
      "11/11 - 0s - loss: 0.3890 - val_loss: 0.4686 - 31ms/epoch - 3ms/step\n",
      "Epoch 1487/2500\n",
      "11/11 - 0s - loss: 0.3894 - val_loss: 0.4775 - 31ms/epoch - 3ms/step\n",
      "Epoch 1488/2500\n",
      "11/11 - 0s - loss: 0.4082 - val_loss: 0.4685 - 31ms/epoch - 3ms/step\n",
      "Epoch 1489/2500\n",
      "11/11 - 0s - loss: 0.3847 - val_loss: 0.4721 - 31ms/epoch - 3ms/step\n",
      "Epoch 1490/2500\n",
      "11/11 - 0s - loss: 0.3970 - val_loss: 0.4820 - 31ms/epoch - 3ms/step\n",
      "Epoch 1491/2500\n",
      "11/11 - 0s - loss: 0.3925 - val_loss: 0.4769 - 31ms/epoch - 3ms/step\n",
      "Epoch 1492/2500\n",
      "11/11 - 0s - loss: 0.3965 - val_loss: 0.4760 - 31ms/epoch - 3ms/step\n",
      "Epoch 1493/2500\n",
      "11/11 - 0s - loss: 0.3814 - val_loss: 0.4843 - 31ms/epoch - 3ms/step\n",
      "Epoch 1494/2500\n",
      "11/11 - 0s - loss: 0.3771 - val_loss: 0.4804 - 32ms/epoch - 3ms/step\n",
      "Epoch 1495/2500\n",
      "11/11 - 0s - loss: 0.4081 - val_loss: 0.4785 - 31ms/epoch - 3ms/step\n",
      "Epoch 1496/2500\n",
      "11/11 - 0s - loss: 0.4098 - val_loss: 0.4939 - 31ms/epoch - 3ms/step\n",
      "Epoch 1497/2500\n",
      "11/11 - 0s - loss: 0.4131 - val_loss: 0.4755 - 31ms/epoch - 3ms/step\n",
      "Epoch 1498/2500\n",
      "11/11 - 0s - loss: 0.3975 - val_loss: 0.4650 - 32ms/epoch - 3ms/step\n",
      "Epoch 1499/2500\n",
      "11/11 - 0s - loss: 0.3916 - val_loss: 0.4833 - 41ms/epoch - 4ms/step\n",
      "Epoch 1500/2500\n",
      "11/11 - 0s - loss: 0.4062 - val_loss: 0.4667 - 37ms/epoch - 3ms/step\n",
      "Epoch 1501/2500\n",
      "11/11 - 0s - loss: 0.4039 - val_loss: 0.4762 - 36ms/epoch - 3ms/step\n",
      "Epoch 1502/2500\n",
      "11/11 - 0s - loss: 0.3840 - val_loss: 0.4762 - 34ms/epoch - 3ms/step\n",
      "Epoch 1503/2500\n",
      "11/11 - 0s - loss: 0.3893 - val_loss: 0.4779 - 37ms/epoch - 3ms/step\n",
      "Epoch 1504/2500\n",
      "11/11 - 0s - loss: 0.4067 - val_loss: 0.4951 - 33ms/epoch - 3ms/step\n",
      "Epoch 1505/2500\n",
      "11/11 - 0s - loss: 0.3917 - val_loss: 0.4897 - 34ms/epoch - 3ms/step\n",
      "Epoch 1506/2500\n",
      "11/11 - 0s - loss: 0.3967 - val_loss: 0.4823 - 34ms/epoch - 3ms/step\n",
      "Epoch 1507/2500\n",
      "11/11 - 0s - loss: 0.3957 - val_loss: 0.4903 - 32ms/epoch - 3ms/step\n",
      "Epoch 1508/2500\n",
      "11/11 - 0s - loss: 0.3867 - val_loss: 0.4770 - 32ms/epoch - 3ms/step\n",
      "Epoch 1509/2500\n",
      "11/11 - 0s - loss: 0.3938 - val_loss: 0.4892 - 33ms/epoch - 3ms/step\n",
      "Epoch 1510/2500\n",
      "11/11 - 0s - loss: 0.3902 - val_loss: 0.4919 - 31ms/epoch - 3ms/step\n",
      "Epoch 1511/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4991 - 31ms/epoch - 3ms/step\n",
      "Epoch 1512/2500\n",
      "11/11 - 0s - loss: 0.4090 - val_loss: 0.4732 - 30ms/epoch - 3ms/step\n",
      "Epoch 1513/2500\n",
      "11/11 - 0s - loss: 0.3946 - val_loss: 0.4818 - 33ms/epoch - 3ms/step\n",
      "Epoch 1514/2500\n",
      "11/11 - 0s - loss: 0.3938 - val_loss: 0.4913 - 33ms/epoch - 3ms/step\n",
      "Epoch 1515/2500\n",
      "11/11 - 0s - loss: 0.3922 - val_loss: 0.4772 - 34ms/epoch - 3ms/step\n",
      "Epoch 1516/2500\n",
      "11/11 - 0s - loss: 0.4044 - val_loss: 0.4697 - 34ms/epoch - 3ms/step\n",
      "Epoch 1517/2500\n",
      "11/11 - 0s - loss: 0.4030 - val_loss: 0.4798 - 34ms/epoch - 3ms/step\n",
      "Epoch 1518/2500\n",
      "11/11 - 0s - loss: 0.4020 - val_loss: 0.4853 - 34ms/epoch - 3ms/step\n",
      "Epoch 1519/2500\n",
      "11/11 - 0s - loss: 0.4062 - val_loss: 0.4811 - 49ms/epoch - 4ms/step\n",
      "Epoch 1520/2500\n",
      "11/11 - 0s - loss: 0.3958 - val_loss: 0.4741 - 34ms/epoch - 3ms/step\n",
      "Epoch 1521/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4889 - 33ms/epoch - 3ms/step\n",
      "Epoch 1522/2500\n",
      "11/11 - 0s - loss: 0.4025 - val_loss: 0.4800 - 35ms/epoch - 3ms/step\n",
      "Epoch 1523/2500\n",
      "11/11 - 0s - loss: 0.4080 - val_loss: 0.4978 - 34ms/epoch - 3ms/step\n",
      "Epoch 1524/2500\n",
      "11/11 - 0s - loss: 0.4040 - val_loss: 0.4924 - 35ms/epoch - 3ms/step\n",
      "Epoch 1525/2500\n",
      "11/11 - 0s - loss: 0.4082 - val_loss: 0.4773 - 34ms/epoch - 3ms/step\n",
      "Epoch 1526/2500\n",
      "11/11 - 0s - loss: 0.3823 - val_loss: 0.4705 - 39ms/epoch - 4ms/step\n",
      "Epoch 1527/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4800 - 35ms/epoch - 3ms/step\n",
      "Epoch 1528/2500\n",
      "11/11 - 0s - loss: 0.3978 - val_loss: 0.4687 - 33ms/epoch - 3ms/step\n",
      "Epoch 1529/2500\n",
      "11/11 - 0s - loss: 0.4131 - val_loss: 0.4578 - 33ms/epoch - 3ms/step\n",
      "Epoch 1530/2500\n",
      "11/11 - 0s - loss: 0.3924 - val_loss: 0.4652 - 31ms/epoch - 3ms/step\n",
      "Epoch 1531/2500\n",
      "11/11 - 0s - loss: 0.4044 - val_loss: 0.4680 - 33ms/epoch - 3ms/step\n",
      "Epoch 1532/2500\n",
      "11/11 - 0s - loss: 0.3899 - val_loss: 0.4746 - 35ms/epoch - 3ms/step\n",
      "Epoch 1533/2500\n",
      "11/11 - 0s - loss: 0.3914 - val_loss: 0.4711 - 41ms/epoch - 4ms/step\n",
      "Epoch 1534/2500\n",
      "11/11 - 0s - loss: 0.3894 - val_loss: 0.4702 - 33ms/epoch - 3ms/step\n",
      "Epoch 1535/2500\n",
      "11/11 - 0s - loss: 0.3883 - val_loss: 0.4707 - 32ms/epoch - 3ms/step\n",
      "Epoch 1536/2500\n",
      "11/11 - 0s - loss: 0.3772 - val_loss: 0.4730 - 33ms/epoch - 3ms/step\n",
      "Epoch 1537/2500\n",
      "11/11 - 0s - loss: 0.3971 - val_loss: 0.4689 - 34ms/epoch - 3ms/step\n",
      "Epoch 1538/2500\n",
      "11/11 - 0s - loss: 0.3908 - val_loss: 0.4717 - 34ms/epoch - 3ms/step\n",
      "Epoch 1539/2500\n",
      "11/11 - 0s - loss: 0.3885 - val_loss: 0.4831 - 33ms/epoch - 3ms/step\n",
      "Epoch 1540/2500\n",
      "11/11 - 0s - loss: 0.3994 - val_loss: 0.4668 - 31ms/epoch - 3ms/step\n",
      "Epoch 1541/2500\n",
      "11/11 - 0s - loss: 0.3843 - val_loss: 0.4631 - 35ms/epoch - 3ms/step\n",
      "Epoch 1542/2500\n",
      "11/11 - 0s - loss: 0.3917 - val_loss: 0.4670 - 39ms/epoch - 4ms/step\n",
      "Epoch 1543/2500\n",
      "11/11 - 0s - loss: 0.4011 - val_loss: 0.4619 - 31ms/epoch - 3ms/step\n",
      "Epoch 1544/2500\n",
      "11/11 - 0s - loss: 0.4028 - val_loss: 0.4655 - 33ms/epoch - 3ms/step\n",
      "Epoch 1545/2500\n",
      "11/11 - 0s - loss: 0.4029 - val_loss: 0.4718 - 31ms/epoch - 3ms/step\n",
      "Epoch 1546/2500\n",
      "11/11 - 0s - loss: 0.3809 - val_loss: 0.4729 - 31ms/epoch - 3ms/step\n",
      "Epoch 1547/2500\n",
      "11/11 - 0s - loss: 0.4034 - val_loss: 0.4707 - 31ms/epoch - 3ms/step\n",
      "Epoch 1548/2500\n",
      "11/11 - 0s - loss: 0.3934 - val_loss: 0.4561 - 31ms/epoch - 3ms/step\n",
      "Epoch 1549/2500\n",
      "11/11 - 0s - loss: 0.3914 - val_loss: 0.4713 - 31ms/epoch - 3ms/step\n",
      "Epoch 1550/2500\n",
      "11/11 - 0s - loss: 0.3823 - val_loss: 0.4822 - 34ms/epoch - 3ms/step\n",
      "Epoch 1551/2500\n",
      "11/11 - 0s - loss: 0.3942 - val_loss: 0.4819 - 37ms/epoch - 3ms/step\n",
      "Epoch 1552/2500\n",
      "11/11 - 0s - loss: 0.3998 - val_loss: 0.4831 - 34ms/epoch - 3ms/step\n",
      "Epoch 1553/2500\n",
      "11/11 - 0s - loss: 0.3960 - val_loss: 0.4756 - 33ms/epoch - 3ms/step\n",
      "Epoch 1554/2500\n",
      "11/11 - 0s - loss: 0.4109 - val_loss: 0.4701 - 32ms/epoch - 3ms/step\n",
      "Epoch 1555/2500\n",
      "11/11 - 0s - loss: 0.3967 - val_loss: 0.4754 - 31ms/epoch - 3ms/step\n",
      "Epoch 1556/2500\n",
      "11/11 - 0s - loss: 0.4038 - val_loss: 0.4683 - 31ms/epoch - 3ms/step\n",
      "Epoch 1557/2500\n",
      "11/11 - 0s - loss: 0.3918 - val_loss: 0.4688 - 31ms/epoch - 3ms/step\n",
      "Epoch 1558/2500\n",
      "11/11 - 0s - loss: 0.4117 - val_loss: 0.4650 - 32ms/epoch - 3ms/step\n",
      "Epoch 1559/2500\n",
      "11/11 - 0s - loss: 0.3875 - val_loss: 0.4744 - 31ms/epoch - 3ms/step\n",
      "Epoch 1560/2500\n",
      "11/11 - 0s - loss: 0.3978 - val_loss: 0.4637 - 32ms/epoch - 3ms/step\n",
      "Epoch 1561/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4635 - 31ms/epoch - 3ms/step\n",
      "Epoch 1562/2500\n",
      "11/11 - 0s - loss: 0.3940 - val_loss: 0.4625 - 31ms/epoch - 3ms/step\n",
      "Epoch 1563/2500\n",
      "11/11 - 0s - loss: 0.3850 - val_loss: 0.4703 - 31ms/epoch - 3ms/step\n",
      "Epoch 1564/2500\n",
      "11/11 - 0s - loss: 0.4002 - val_loss: 0.4716 - 40ms/epoch - 4ms/step\n",
      "Epoch 1565/2500\n",
      "11/11 - 0s - loss: 0.3884 - val_loss: 0.4708 - 37ms/epoch - 3ms/step\n",
      "Epoch 1566/2500\n",
      "11/11 - 0s - loss: 0.3993 - val_loss: 0.4950 - 36ms/epoch - 3ms/step\n",
      "Epoch 1567/2500\n",
      "11/11 - 0s - loss: 0.4124 - val_loss: 0.4691 - 32ms/epoch - 3ms/step\n",
      "Epoch 1568/2500\n",
      "11/11 - 0s - loss: 0.4146 - val_loss: 0.4661 - 31ms/epoch - 3ms/step\n",
      "Epoch 1569/2500\n",
      "11/11 - 0s - loss: 0.3960 - val_loss: 0.4933 - 32ms/epoch - 3ms/step\n",
      "Epoch 1570/2500\n",
      "11/11 - 0s - loss: 0.4030 - val_loss: 0.4727 - 33ms/epoch - 3ms/step\n",
      "Epoch 1571/2500\n",
      "11/11 - 0s - loss: 0.4055 - val_loss: 0.4750 - 31ms/epoch - 3ms/step\n",
      "Epoch 1572/2500\n",
      "11/11 - 0s - loss: 0.3999 - val_loss: 0.4713 - 32ms/epoch - 3ms/step\n",
      "Epoch 1573/2500\n",
      "11/11 - 0s - loss: 0.3914 - val_loss: 0.4664 - 31ms/epoch - 3ms/step\n",
      "Epoch 1574/2500\n",
      "11/11 - 0s - loss: 0.3972 - val_loss: 0.4690 - 32ms/epoch - 3ms/step\n",
      "Epoch 1575/2500\n",
      "11/11 - 0s - loss: 0.3889 - val_loss: 0.4744 - 34ms/epoch - 3ms/step\n",
      "Epoch 1576/2500\n",
      "11/11 - 0s - loss: 0.3913 - val_loss: 0.4674 - 33ms/epoch - 3ms/step\n",
      "Epoch 1577/2500\n",
      "11/11 - 0s - loss: 0.3977 - val_loss: 0.4714 - 33ms/epoch - 3ms/step\n",
      "Epoch 1578/2500\n",
      "11/11 - 0s - loss: 0.4037 - val_loss: 0.4772 - 33ms/epoch - 3ms/step\n",
      "Epoch 1579/2500\n",
      "11/11 - 0s - loss: 0.4050 - val_loss: 0.4685 - 31ms/epoch - 3ms/step\n",
      "Epoch 1580/2500\n",
      "11/11 - 0s - loss: 0.3959 - val_loss: 0.4781 - 32ms/epoch - 3ms/step\n",
      "Epoch 1581/2500\n",
      "11/11 - 0s - loss: 0.4027 - val_loss: 0.4842 - 32ms/epoch - 3ms/step\n",
      "Epoch 1582/2500\n",
      "11/11 - 0s - loss: 0.3890 - val_loss: 0.4878 - 47ms/epoch - 4ms/step\n",
      "Epoch 1583/2500\n",
      "11/11 - 0s - loss: 0.4060 - val_loss: 0.4834 - 31ms/epoch - 3ms/step\n",
      "Epoch 1584/2500\n",
      "11/11 - 0s - loss: 0.4058 - val_loss: 0.4684 - 31ms/epoch - 3ms/step\n",
      "Epoch 1585/2500\n",
      "11/11 - 0s - loss: 0.3935 - val_loss: 0.4742 - 32ms/epoch - 3ms/step\n",
      "Epoch 1586/2500\n",
      "11/11 - 0s - loss: 0.3892 - val_loss: 0.4716 - 32ms/epoch - 3ms/step\n",
      "Epoch 1587/2500\n",
      "11/11 - 0s - loss: 0.3895 - val_loss: 0.4669 - 34ms/epoch - 3ms/step\n",
      "Epoch 1588/2500\n",
      "11/11 - 0s - loss: 0.3917 - val_loss: 0.4787 - 35ms/epoch - 3ms/step\n",
      "Epoch 1589/2500\n",
      "11/11 - 0s - loss: 0.3968 - val_loss: 0.4711 - 31ms/epoch - 3ms/step\n",
      "Epoch 1590/2500\n",
      "11/11 - 0s - loss: 0.3969 - val_loss: 0.4708 - 32ms/epoch - 3ms/step\n",
      "Epoch 1591/2500\n",
      "11/11 - 0s - loss: 0.4054 - val_loss: 0.4706 - 31ms/epoch - 3ms/step\n",
      "Epoch 1592/2500\n",
      "11/11 - 0s - loss: 0.4104 - val_loss: 0.4641 - 32ms/epoch - 3ms/step\n",
      "Epoch 1593/2500\n",
      "11/11 - 0s - loss: 0.3754 - val_loss: 0.4865 - 31ms/epoch - 3ms/step\n",
      "Epoch 1594/2500\n",
      "11/11 - 0s - loss: 0.3934 - val_loss: 0.4862 - 32ms/epoch - 3ms/step\n",
      "Epoch 1595/2500\n",
      "11/11 - 0s - loss: 0.3756 - val_loss: 0.4697 - 31ms/epoch - 3ms/step\n",
      "Epoch 1596/2500\n",
      "11/11 - 0s - loss: 0.4053 - val_loss: 0.4710 - 32ms/epoch - 3ms/step\n",
      "Epoch 1597/2500\n",
      "11/11 - 0s - loss: 0.3856 - val_loss: 0.4699 - 34ms/epoch - 3ms/step\n",
      "Epoch 1598/2500\n",
      "11/11 - 0s - loss: 0.3747 - val_loss: 0.4744 - 33ms/epoch - 3ms/step\n",
      "Epoch 1599/2500\n",
      "11/11 - 0s - loss: 0.3973 - val_loss: 0.4747 - 44ms/epoch - 4ms/step\n",
      "Epoch 1600/2500\n",
      "11/11 - 0s - loss: 0.3961 - val_loss: 0.4754 - 34ms/epoch - 3ms/step\n",
      "Epoch 1601/2500\n",
      "11/11 - 0s - loss: 0.3907 - val_loss: 0.4787 - 33ms/epoch - 3ms/step\n",
      "Epoch 1602/2500\n",
      "11/11 - 0s - loss: 0.3982 - val_loss: 0.4830 - 35ms/epoch - 3ms/step\n",
      "Epoch 1603/2500\n",
      "11/11 - 0s - loss: 0.4085 - val_loss: 0.4784 - 33ms/epoch - 3ms/step\n",
      "Epoch 1604/2500\n",
      "11/11 - 0s - loss: 0.3935 - val_loss: 0.4810 - 32ms/epoch - 3ms/step\n",
      "Epoch 1605/2500\n",
      "11/11 - 0s - loss: 0.3913 - val_loss: 0.4818 - 34ms/epoch - 3ms/step\n",
      "Epoch 1606/2500\n",
      "11/11 - 0s - loss: 0.3885 - val_loss: 0.4868 - 37ms/epoch - 3ms/step\n",
      "Epoch 1607/2500\n",
      "11/11 - 0s - loss: 0.3927 - val_loss: 0.4909 - 35ms/epoch - 3ms/step\n",
      "Epoch 1608/2500\n",
      "11/11 - 0s - loss: 0.4089 - val_loss: 0.4795 - 35ms/epoch - 3ms/step\n",
      "Epoch 1609/2500\n",
      "11/11 - 0s - loss: 0.4019 - val_loss: 0.4691 - 32ms/epoch - 3ms/step\n",
      "Epoch 1610/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.4717 - 33ms/epoch - 3ms/step\n",
      "Epoch 1611/2500\n",
      "11/11 - 0s - loss: 0.4076 - val_loss: 0.4694 - 34ms/epoch - 3ms/step\n",
      "Epoch 1612/2500\n",
      "11/11 - 0s - loss: 0.3956 - val_loss: 0.4918 - 37ms/epoch - 3ms/step\n",
      "Epoch 1613/2500\n",
      "11/11 - 0s - loss: 0.3989 - val_loss: 0.4843 - 34ms/epoch - 3ms/step\n",
      "Epoch 1614/2500\n",
      "11/11 - 0s - loss: 0.3906 - val_loss: 0.4792 - 33ms/epoch - 3ms/step\n",
      "Epoch 1615/2500\n",
      "11/11 - 0s - loss: 0.3810 - val_loss: 0.4807 - 32ms/epoch - 3ms/step\n",
      "Epoch 1616/2500\n",
      "11/11 - 0s - loss: 0.3900 - val_loss: 0.4875 - 34ms/epoch - 3ms/step\n",
      "Epoch 1617/2500\n",
      "11/11 - 0s - loss: 0.3809 - val_loss: 0.4956 - 55ms/epoch - 5ms/step\n",
      "Epoch 1618/2500\n",
      "11/11 - 0s - loss: 0.3840 - val_loss: 0.4926 - 35ms/epoch - 3ms/step\n",
      "Epoch 1619/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4764 - 34ms/epoch - 3ms/step\n",
      "Epoch 1620/2500\n",
      "11/11 - 0s - loss: 0.3937 - val_loss: 0.4783 - 37ms/epoch - 3ms/step\n",
      "Epoch 1621/2500\n",
      "11/11 - 0s - loss: 0.3828 - val_loss: 0.4821 - 31ms/epoch - 3ms/step\n",
      "Epoch 1622/2500\n",
      "11/11 - 0s - loss: 0.4005 - val_loss: 0.4748 - 34ms/epoch - 3ms/step\n",
      "Epoch 1623/2500\n",
      "11/11 - 0s - loss: 0.3885 - val_loss: 0.4800 - 32ms/epoch - 3ms/step\n",
      "Epoch 1624/2500\n",
      "11/11 - 0s - loss: 0.3745 - val_loss: 0.4791 - 31ms/epoch - 3ms/step\n",
      "Epoch 1625/2500\n",
      "11/11 - 0s - loss: 0.4130 - val_loss: 0.4869 - 31ms/epoch - 3ms/step\n",
      "Epoch 1626/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4916 - 31ms/epoch - 3ms/step\n",
      "Epoch 1627/2500\n",
      "11/11 - 0s - loss: 0.3961 - val_loss: 0.4855 - 31ms/epoch - 3ms/step\n",
      "Epoch 1628/2500\n",
      "11/11 - 0s - loss: 0.3965 - val_loss: 0.4711 - 30ms/epoch - 3ms/step\n",
      "Epoch 1629/2500\n",
      "11/11 - 0s - loss: 0.3946 - val_loss: 0.4784 - 31ms/epoch - 3ms/step\n",
      "Epoch 1630/2500\n",
      "11/11 - 0s - loss: 0.3939 - val_loss: 0.4858 - 31ms/epoch - 3ms/step\n",
      "Epoch 1631/2500\n",
      "11/11 - 0s - loss: 0.3913 - val_loss: 0.4803 - 31ms/epoch - 3ms/step\n",
      "Epoch 1632/2500\n",
      "11/11 - 0s - loss: 0.3983 - val_loss: 0.4783 - 31ms/epoch - 3ms/step\n",
      "Epoch 1633/2500\n",
      "11/11 - 0s - loss: 0.3847 - val_loss: 0.4807 - 31ms/epoch - 3ms/step\n",
      "Epoch 1634/2500\n",
      "11/11 - 0s - loss: 0.4061 - val_loss: 0.4974 - 31ms/epoch - 3ms/step\n",
      "Epoch 1635/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4863 - 31ms/epoch - 3ms/step\n",
      "Epoch 1636/2500\n",
      "11/11 - 0s - loss: 0.3898 - val_loss: 0.4698 - 31ms/epoch - 3ms/step\n",
      "Epoch 1637/2500\n",
      "11/11 - 0s - loss: 0.3935 - val_loss: 0.4798 - 31ms/epoch - 3ms/step\n",
      "Epoch 1638/2500\n",
      "11/11 - 0s - loss: 0.3868 - val_loss: 0.4790 - 31ms/epoch - 3ms/step\n",
      "Epoch 1639/2500\n",
      "11/11 - 0s - loss: 0.3839 - val_loss: 0.4683 - 43ms/epoch - 4ms/step\n",
      "Epoch 1640/2500\n",
      "11/11 - 0s - loss: 0.3924 - val_loss: 0.4727 - 35ms/epoch - 3ms/step\n",
      "Epoch 1641/2500\n",
      "11/11 - 0s - loss: 0.4043 - val_loss: 0.4834 - 33ms/epoch - 3ms/step\n",
      "Epoch 1642/2500\n",
      "11/11 - 0s - loss: 0.3950 - val_loss: 0.4832 - 31ms/epoch - 3ms/step\n",
      "Epoch 1643/2500\n",
      "11/11 - 0s - loss: 0.3769 - val_loss: 0.4851 - 46ms/epoch - 4ms/step\n",
      "Epoch 1644/2500\n",
      "11/11 - 0s - loss: 0.3889 - val_loss: 0.4810 - 32ms/epoch - 3ms/step\n",
      "Epoch 1645/2500\n",
      "11/11 - 0s - loss: 0.4015 - val_loss: 0.4750 - 31ms/epoch - 3ms/step\n",
      "Epoch 1646/2500\n",
      "11/11 - 0s - loss: 0.3954 - val_loss: 0.4778 - 32ms/epoch - 3ms/step\n",
      "Epoch 1647/2500\n",
      "11/11 - 0s - loss: 0.3833 - val_loss: 0.4760 - 32ms/epoch - 3ms/step\n",
      "Epoch 1648/2500\n",
      "11/11 - 0s - loss: 0.3899 - val_loss: 0.4824 - 35ms/epoch - 3ms/step\n",
      "Epoch 1649/2500\n",
      "11/11 - 0s - loss: 0.3811 - val_loss: 0.4817 - 34ms/epoch - 3ms/step\n",
      "Epoch 1650/2500\n",
      "11/11 - 0s - loss: 0.3874 - val_loss: 0.4889 - 32ms/epoch - 3ms/step\n",
      "Epoch 1651/2500\n",
      "11/11 - 0s - loss: 0.3959 - val_loss: 0.4915 - 33ms/epoch - 3ms/step\n",
      "Epoch 1652/2500\n",
      "11/11 - 0s - loss: 0.3858 - val_loss: 0.4688 - 36ms/epoch - 3ms/step\n",
      "Epoch 1653/2500\n",
      "11/11 - 0s - loss: 0.3725 - val_loss: 0.4740 - 34ms/epoch - 3ms/step\n",
      "Epoch 1654/2500\n",
      "11/11 - 0s - loss: 0.3756 - val_loss: 0.4668 - 33ms/epoch - 3ms/step\n",
      "Epoch 1655/2500\n",
      "11/11 - 0s - loss: 0.3863 - val_loss: 0.4772 - 36ms/epoch - 3ms/step\n",
      "Epoch 1656/2500\n",
      "11/11 - 0s - loss: 0.3887 - val_loss: 0.4827 - 54ms/epoch - 5ms/step\n",
      "Epoch 1657/2500\n",
      "11/11 - 0s - loss: 0.3890 - val_loss: 0.4733 - 37ms/epoch - 3ms/step\n",
      "Epoch 1658/2500\n",
      "11/11 - 0s - loss: 0.3874 - val_loss: 0.4764 - 38ms/epoch - 3ms/step\n",
      "Epoch 1659/2500\n",
      "11/11 - 0s - loss: 0.3917 - val_loss: 0.4731 - 40ms/epoch - 4ms/step\n",
      "Epoch 1660/2500\n",
      "11/11 - 0s - loss: 0.3926 - val_loss: 0.4727 - 42ms/epoch - 4ms/step\n",
      "Epoch 1661/2500\n",
      "11/11 - 0s - loss: 0.3788 - val_loss: 0.4790 - 36ms/epoch - 3ms/step\n",
      "Epoch 1662/2500\n",
      "11/11 - 0s - loss: 0.3895 - val_loss: 0.4924 - 31ms/epoch - 3ms/step\n",
      "Epoch 1663/2500\n",
      "11/11 - 0s - loss: 0.3977 - val_loss: 0.4796 - 32ms/epoch - 3ms/step\n",
      "Epoch 1664/2500\n",
      "11/11 - 0s - loss: 0.3907 - val_loss: 0.4757 - 36ms/epoch - 3ms/step\n",
      "Epoch 1665/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4892 - 31ms/epoch - 3ms/step\n",
      "Epoch 1666/2500\n",
      "11/11 - 0s - loss: 0.3790 - val_loss: 0.4736 - 31ms/epoch - 3ms/step\n",
      "Epoch 1667/2500\n",
      "11/11 - 0s - loss: 0.3975 - val_loss: 0.4699 - 32ms/epoch - 3ms/step\n",
      "Epoch 1668/2500\n",
      "11/11 - 0s - loss: 0.3943 - val_loss: 0.4848 - 31ms/epoch - 3ms/step\n",
      "Epoch 1669/2500\n",
      "11/11 - 0s - loss: 0.3994 - val_loss: 0.4749 - 31ms/epoch - 3ms/step\n",
      "Epoch 1670/2500\n",
      "11/11 - 0s - loss: 0.3932 - val_loss: 0.4687 - 32ms/epoch - 3ms/step\n",
      "Epoch 1671/2500\n",
      "11/11 - 0s - loss: 0.3826 - val_loss: 0.4768 - 31ms/epoch - 3ms/step\n",
      "Epoch 1672/2500\n",
      "11/11 - 0s - loss: 0.4025 - val_loss: 0.4788 - 44ms/epoch - 4ms/step\n",
      "Epoch 1673/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4730 - 31ms/epoch - 3ms/step\n",
      "Epoch 1674/2500\n",
      "11/11 - 0s - loss: 0.3782 - val_loss: 0.4723 - 36ms/epoch - 3ms/step\n",
      "Epoch 1675/2500\n",
      "11/11 - 0s - loss: 0.3921 - val_loss: 0.4896 - 33ms/epoch - 3ms/step\n",
      "Epoch 1676/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.4847 - 34ms/epoch - 3ms/step\n",
      "Epoch 1677/2500\n",
      "11/11 - 0s - loss: 0.3893 - val_loss: 0.4690 - 33ms/epoch - 3ms/step\n",
      "Epoch 1678/2500\n",
      "11/11 - 0s - loss: 0.4053 - val_loss: 0.4751 - 35ms/epoch - 3ms/step\n",
      "Epoch 1679/2500\n",
      "11/11 - 0s - loss: 0.3908 - val_loss: 0.4788 - 36ms/epoch - 3ms/step\n",
      "Epoch 1680/2500\n",
      "11/11 - 0s - loss: 0.3927 - val_loss: 0.4952 - 35ms/epoch - 3ms/step\n",
      "Epoch 1681/2500\n",
      "11/11 - 0s - loss: 0.3897 - val_loss: 0.4796 - 35ms/epoch - 3ms/step\n",
      "Epoch 1682/2500\n",
      "11/11 - 0s - loss: 0.3860 - val_loss: 0.4840 - 37ms/epoch - 3ms/step\n",
      "Epoch 1683/2500\n",
      "11/11 - 0s - loss: 0.3900 - val_loss: 0.4924 - 45ms/epoch - 4ms/step\n",
      "Epoch 1684/2500\n",
      "11/11 - 0s - loss: 0.3922 - val_loss: 0.4891 - 37ms/epoch - 3ms/step\n",
      "Epoch 1685/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.4872 - 35ms/epoch - 3ms/step\n",
      "Epoch 1686/2500\n",
      "11/11 - 0s - loss: 0.4005 - val_loss: 0.4842 - 34ms/epoch - 3ms/step\n",
      "Epoch 1687/2500\n",
      "11/11 - 0s - loss: 0.3785 - val_loss: 0.4748 - 35ms/epoch - 3ms/step\n",
      "Epoch 1688/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.4739 - 34ms/epoch - 3ms/step\n",
      "Epoch 1689/2500\n",
      "11/11 - 0s - loss: 0.3892 - val_loss: 0.4927 - 34ms/epoch - 3ms/step\n",
      "Epoch 1690/2500\n",
      "11/11 - 0s - loss: 0.3990 - val_loss: 0.5009 - 34ms/epoch - 3ms/step\n",
      "Epoch 1691/2500\n",
      "11/11 - 0s - loss: 0.3757 - val_loss: 0.4790 - 33ms/epoch - 3ms/step\n",
      "Epoch 1692/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4815 - 33ms/epoch - 3ms/step\n",
      "Epoch 1693/2500\n",
      "11/11 - 0s - loss: 0.3966 - val_loss: 0.4765 - 33ms/epoch - 3ms/step\n",
      "Epoch 1694/2500\n",
      "11/11 - 0s - loss: 0.3929 - val_loss: 0.4808 - 32ms/epoch - 3ms/step\n",
      "Epoch 1695/2500\n",
      "11/11 - 0s - loss: 0.3870 - val_loss: 0.4808 - 32ms/epoch - 3ms/step\n",
      "Epoch 1696/2500\n",
      "11/11 - 0s - loss: 0.3893 - val_loss: 0.4707 - 33ms/epoch - 3ms/step\n",
      "Epoch 1697/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4703 - 32ms/epoch - 3ms/step\n",
      "Epoch 1698/2500\n",
      "11/11 - 0s - loss: 0.3806 - val_loss: 0.4950 - 31ms/epoch - 3ms/step\n",
      "Epoch 1699/2500\n",
      "11/11 - 0s - loss: 0.3793 - val_loss: 0.4785 - 31ms/epoch - 3ms/step\n",
      "Epoch 1700/2500\n",
      "11/11 - 0s - loss: 0.3934 - val_loss: 0.4723 - 31ms/epoch - 3ms/step\n",
      "Epoch 1701/2500\n",
      "11/11 - 0s - loss: 0.3874 - val_loss: 0.4720 - 35ms/epoch - 3ms/step\n",
      "Epoch 1702/2500\n",
      "11/11 - 0s - loss: 0.3780 - val_loss: 0.4906 - 47ms/epoch - 4ms/step\n",
      "Epoch 1703/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4929 - 41ms/epoch - 4ms/step\n",
      "Epoch 1704/2500\n",
      "11/11 - 0s - loss: 0.3814 - val_loss: 0.4820 - 41ms/epoch - 4ms/step\n",
      "Epoch 1705/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4759 - 37ms/epoch - 3ms/step\n",
      "Epoch 1706/2500\n",
      "11/11 - 0s - loss: 0.3845 - val_loss: 0.4894 - 51ms/epoch - 5ms/step\n",
      "Epoch 1707/2500\n",
      "11/11 - 0s - loss: 0.3948 - val_loss: 0.4782 - 43ms/epoch - 4ms/step\n",
      "Epoch 1708/2500\n",
      "11/11 - 0s - loss: 0.3868 - val_loss: 0.4705 - 33ms/epoch - 3ms/step\n",
      "Epoch 1709/2500\n",
      "11/11 - 0s - loss: 0.3838 - val_loss: 0.4784 - 34ms/epoch - 3ms/step\n",
      "Epoch 1710/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4858 - 39ms/epoch - 4ms/step\n",
      "Epoch 1711/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4831 - 33ms/epoch - 3ms/step\n",
      "Epoch 1712/2500\n",
      "11/11 - 0s - loss: 0.4100 - val_loss: 0.4667 - 33ms/epoch - 3ms/step\n",
      "Epoch 1713/2500\n",
      "11/11 - 0s - loss: 0.3782 - val_loss: 0.4704 - 31ms/epoch - 3ms/step\n",
      "Epoch 1714/2500\n",
      "11/11 - 0s - loss: 0.3823 - val_loss: 0.4822 - 31ms/epoch - 3ms/step\n",
      "Epoch 1715/2500\n",
      "11/11 - 0s - loss: 0.4061 - val_loss: 0.4818 - 31ms/epoch - 3ms/step\n",
      "Epoch 1716/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.4905 - 31ms/epoch - 3ms/step\n",
      "Epoch 1717/2500\n",
      "11/11 - 0s - loss: 0.3924 - val_loss: 0.4784 - 31ms/epoch - 3ms/step\n",
      "Epoch 1718/2500\n",
      "11/11 - 0s - loss: 0.3938 - val_loss: 0.4769 - 31ms/epoch - 3ms/step\n",
      "Epoch 1719/2500\n",
      "11/11 - 0s - loss: 0.3850 - val_loss: 0.4730 - 32ms/epoch - 3ms/step\n",
      "Epoch 1720/2500\n",
      "11/11 - 0s - loss: 0.3815 - val_loss: 0.4769 - 31ms/epoch - 3ms/step\n",
      "Epoch 1721/2500\n",
      "11/11 - 0s - loss: 0.3699 - val_loss: 0.4779 - 32ms/epoch - 3ms/step\n",
      "Epoch 1722/2500\n",
      "11/11 - 0s - loss: 0.3832 - val_loss: 0.4695 - 33ms/epoch - 3ms/step\n",
      "Epoch 1723/2500\n",
      "11/11 - 0s - loss: 0.3906 - val_loss: 0.4678 - 31ms/epoch - 3ms/step\n",
      "Epoch 1724/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4696 - 31ms/epoch - 3ms/step\n",
      "Epoch 1725/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4803 - 34ms/epoch - 3ms/step\n",
      "Epoch 1726/2500\n",
      "11/11 - 0s - loss: 0.3885 - val_loss: 0.4806 - 46ms/epoch - 4ms/step\n",
      "Epoch 1727/2500\n",
      "11/11 - 0s - loss: 0.3910 - val_loss: 0.4796 - 36ms/epoch - 3ms/step\n",
      "Epoch 1728/2500\n",
      "11/11 - 0s - loss: 0.3880 - val_loss: 0.4790 - 31ms/epoch - 3ms/step\n",
      "Epoch 1729/2500\n",
      "11/11 - 0s - loss: 0.3959 - val_loss: 0.4683 - 31ms/epoch - 3ms/step\n",
      "Epoch 1730/2500\n",
      "11/11 - 0s - loss: 0.3925 - val_loss: 0.4787 - 32ms/epoch - 3ms/step\n",
      "Epoch 1731/2500\n",
      "11/11 - 0s - loss: 0.3828 - val_loss: 0.4667 - 32ms/epoch - 3ms/step\n",
      "Epoch 1732/2500\n",
      "11/11 - 0s - loss: 0.3899 - val_loss: 0.4705 - 33ms/epoch - 3ms/step\n",
      "Epoch 1733/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4757 - 33ms/epoch - 3ms/step\n",
      "Epoch 1734/2500\n",
      "11/11 - 0s - loss: 0.3877 - val_loss: 0.4824 - 32ms/epoch - 3ms/step\n",
      "Epoch 1735/2500\n",
      "11/11 - 0s - loss: 0.4001 - val_loss: 0.4948 - 32ms/epoch - 3ms/step\n",
      "Epoch 1736/2500\n",
      "11/11 - 0s - loss: 0.3986 - val_loss: 0.4846 - 34ms/epoch - 3ms/step\n",
      "Epoch 1737/2500\n",
      "11/11 - 0s - loss: 0.3792 - val_loss: 0.4649 - 33ms/epoch - 3ms/step\n",
      "Epoch 1738/2500\n",
      "11/11 - 0s - loss: 0.3840 - val_loss: 0.4746 - 32ms/epoch - 3ms/step\n",
      "Epoch 1739/2500\n",
      "11/11 - 0s - loss: 0.3648 - val_loss: 0.4922 - 38ms/epoch - 3ms/step\n",
      "Epoch 1740/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4702 - 32ms/epoch - 3ms/step\n",
      "Epoch 1741/2500\n",
      "11/11 - 0s - loss: 0.3938 - val_loss: 0.4781 - 35ms/epoch - 3ms/step\n",
      "Epoch 1742/2500\n",
      "11/11 - 0s - loss: 0.3875 - val_loss: 0.4721 - 35ms/epoch - 3ms/step\n",
      "Epoch 1743/2500\n",
      "11/11 - 0s - loss: 0.3766 - val_loss: 0.4737 - 54ms/epoch - 5ms/step\n",
      "Epoch 1744/2500\n",
      "11/11 - 0s - loss: 0.3906 - val_loss: 0.4881 - 35ms/epoch - 3ms/step\n",
      "Epoch 1745/2500\n",
      "11/11 - 0s - loss: 0.3847 - val_loss: 0.4820 - 35ms/epoch - 3ms/step\n",
      "Epoch 1746/2500\n",
      "11/11 - 0s - loss: 0.3816 - val_loss: 0.4793 - 35ms/epoch - 3ms/step\n",
      "Epoch 1747/2500\n",
      "11/11 - 0s - loss: 0.3802 - val_loss: 0.4864 - 34ms/epoch - 3ms/step\n",
      "Epoch 1748/2500\n",
      "11/11 - 0s - loss: 0.3908 - val_loss: 0.4832 - 32ms/epoch - 3ms/step\n",
      "Epoch 1749/2500\n",
      "11/11 - 0s - loss: 0.3877 - val_loss: 0.4941 - 31ms/epoch - 3ms/step\n",
      "Epoch 1750/2500\n",
      "11/11 - 0s - loss: 0.3819 - val_loss: 0.4780 - 31ms/epoch - 3ms/step\n",
      "Epoch 1751/2500\n",
      "11/11 - 0s - loss: 0.3908 - val_loss: 0.4909 - 34ms/epoch - 3ms/step\n",
      "Epoch 1752/2500\n",
      "11/11 - 0s - loss: 0.3868 - val_loss: 0.4805 - 34ms/epoch - 3ms/step\n",
      "Epoch 1753/2500\n",
      "11/11 - 0s - loss: 0.3872 - val_loss: 0.4664 - 34ms/epoch - 3ms/step\n",
      "Epoch 1754/2500\n",
      "11/11 - 0s - loss: 0.3830 - val_loss: 0.4752 - 34ms/epoch - 3ms/step\n",
      "Epoch 1755/2500\n",
      "11/11 - 0s - loss: 0.3845 - val_loss: 0.4816 - 33ms/epoch - 3ms/step\n",
      "Epoch 1756/2500\n",
      "11/11 - 0s - loss: 0.4026 - val_loss: 0.4743 - 33ms/epoch - 3ms/step\n",
      "Epoch 1757/2500\n",
      "11/11 - 0s - loss: 0.3816 - val_loss: 0.4909 - 31ms/epoch - 3ms/step\n",
      "Epoch 1758/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4843 - 31ms/epoch - 3ms/step\n",
      "Epoch 1759/2500\n",
      "11/11 - 0s - loss: 0.3949 - val_loss: 0.4828 - 48ms/epoch - 4ms/step\n",
      "Epoch 1760/2500\n",
      "11/11 - 0s - loss: 0.3813 - val_loss: 0.4846 - 34ms/epoch - 3ms/step\n",
      "Epoch 1761/2500\n",
      "11/11 - 0s - loss: 0.3882 - val_loss: 0.4832 - 38ms/epoch - 3ms/step\n",
      "Epoch 1762/2500\n",
      "11/11 - 0s - loss: 0.3980 - val_loss: 0.4884 - 35ms/epoch - 3ms/step\n",
      "Epoch 1763/2500\n",
      "11/11 - 0s - loss: 0.3888 - val_loss: 0.4877 - 31ms/epoch - 3ms/step\n",
      "Epoch 1764/2500\n",
      "11/11 - 0s - loss: 0.3905 - val_loss: 0.4802 - 34ms/epoch - 3ms/step\n",
      "Epoch 1765/2500\n",
      "11/11 - 0s - loss: 0.3940 - val_loss: 0.4802 - 34ms/epoch - 3ms/step\n",
      "Epoch 1766/2500\n",
      "11/11 - 0s - loss: 0.3829 - val_loss: 0.4912 - 32ms/epoch - 3ms/step\n",
      "Epoch 1767/2500\n",
      "11/11 - 0s - loss: 0.4020 - val_loss: 0.5062 - 36ms/epoch - 3ms/step\n",
      "Epoch 1768/2500\n",
      "11/11 - 0s - loss: 0.3936 - val_loss: 0.4932 - 35ms/epoch - 3ms/step\n",
      "Epoch 1769/2500\n",
      "11/11 - 0s - loss: 0.4023 - val_loss: 0.4908 - 35ms/epoch - 3ms/step\n",
      "Epoch 1770/2500\n",
      "11/11 - 0s - loss: 0.3821 - val_loss: 0.4861 - 34ms/epoch - 3ms/step\n",
      "Epoch 1771/2500\n",
      "11/11 - 0s - loss: 0.3803 - val_loss: 0.4802 - 36ms/epoch - 3ms/step\n",
      "Epoch 1772/2500\n",
      "11/11 - 0s - loss: 0.4026 - val_loss: 0.4699 - 36ms/epoch - 3ms/step\n",
      "Epoch 1773/2500\n",
      "11/11 - 0s - loss: 0.4056 - val_loss: 0.4791 - 48ms/epoch - 4ms/step\n",
      "Epoch 1774/2500\n",
      "11/11 - 0s - loss: 0.3887 - val_loss: 0.4814 - 37ms/epoch - 3ms/step\n",
      "Epoch 1775/2500\n",
      "11/11 - 0s - loss: 0.4031 - val_loss: 0.4720 - 31ms/epoch - 3ms/step\n",
      "Epoch 1776/2500\n",
      "11/11 - 0s - loss: 0.3876 - val_loss: 0.4800 - 31ms/epoch - 3ms/step\n",
      "Epoch 1777/2500\n",
      "11/11 - 0s - loss: 0.3947 - val_loss: 0.4770 - 32ms/epoch - 3ms/step\n",
      "Epoch 1778/2500\n",
      "11/11 - 0s - loss: 0.3876 - val_loss: 0.4780 - 32ms/epoch - 3ms/step\n",
      "Epoch 1779/2500\n",
      "11/11 - 0s - loss: 0.3834 - val_loss: 0.4659 - 33ms/epoch - 3ms/step\n",
      "Epoch 1780/2500\n",
      "11/11 - 0s - loss: 0.3921 - val_loss: 0.4756 - 34ms/epoch - 3ms/step\n",
      "Epoch 1781/2500\n",
      "11/11 - 0s - loss: 0.3825 - val_loss: 0.4875 - 33ms/epoch - 3ms/step\n",
      "Epoch 1782/2500\n",
      "11/11 - 0s - loss: 0.3831 - val_loss: 0.4795 - 33ms/epoch - 3ms/step\n",
      "Epoch 1783/2500\n",
      "11/11 - 0s - loss: 0.3740 - val_loss: 0.4919 - 32ms/epoch - 3ms/step\n",
      "Epoch 1784/2500\n",
      "11/11 - 0s - loss: 0.3940 - val_loss: 0.4770 - 36ms/epoch - 3ms/step\n",
      "Epoch 1785/2500\n",
      "11/11 - 0s - loss: 0.3861 - val_loss: 0.4703 - 33ms/epoch - 3ms/step\n",
      "Epoch 1786/2500\n",
      "11/11 - 0s - loss: 0.3750 - val_loss: 0.4789 - 32ms/epoch - 3ms/step\n",
      "Epoch 1787/2500\n",
      "11/11 - 0s - loss: 0.3957 - val_loss: 0.4772 - 31ms/epoch - 3ms/step\n",
      "Epoch 1788/2500\n",
      "11/11 - 0s - loss: 0.3972 - val_loss: 0.4889 - 31ms/epoch - 3ms/step\n",
      "Epoch 1789/2500\n",
      "11/11 - 0s - loss: 0.3881 - val_loss: 0.4898 - 32ms/epoch - 3ms/step\n",
      "Epoch 1790/2500\n",
      "11/11 - 0s - loss: 0.3958 - val_loss: 0.4795 - 34ms/epoch - 3ms/step\n",
      "Epoch 1791/2500\n",
      "11/11 - 0s - loss: 0.3837 - val_loss: 0.4860 - 34ms/epoch - 3ms/step\n",
      "Epoch 1792/2500\n",
      "11/11 - 0s - loss: 0.3816 - val_loss: 0.4972 - 49ms/epoch - 4ms/step\n",
      "Epoch 1793/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4786 - 45ms/epoch - 4ms/step\n",
      "Epoch 1794/2500\n",
      "11/11 - 0s - loss: 0.3939 - val_loss: 0.4734 - 37ms/epoch - 3ms/step\n",
      "Epoch 1795/2500\n",
      "11/11 - 0s - loss: 0.3818 - val_loss: 0.4972 - 34ms/epoch - 3ms/step\n",
      "Epoch 1796/2500\n",
      "11/11 - 0s - loss: 0.3906 - val_loss: 0.4983 - 34ms/epoch - 3ms/step\n",
      "Epoch 1797/2500\n",
      "11/11 - 0s - loss: 0.3831 - val_loss: 0.4775 - 34ms/epoch - 3ms/step\n",
      "Epoch 1798/2500\n",
      "11/11 - 0s - loss: 0.3839 - val_loss: 0.4841 - 34ms/epoch - 3ms/step\n",
      "Epoch 1799/2500\n",
      "11/11 - 0s - loss: 0.3707 - val_loss: 0.4805 - 33ms/epoch - 3ms/step\n",
      "Epoch 1800/2500\n",
      "11/11 - 0s - loss: 0.3877 - val_loss: 0.4795 - 32ms/epoch - 3ms/step\n",
      "Epoch 1801/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4725 - 31ms/epoch - 3ms/step\n",
      "Epoch 1802/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4726 - 31ms/epoch - 3ms/step\n",
      "Epoch 1803/2500\n",
      "11/11 - 0s - loss: 0.3782 - val_loss: 0.4703 - 30ms/epoch - 3ms/step\n",
      "Epoch 1804/2500\n",
      "11/11 - 0s - loss: 0.3896 - val_loss: 0.4698 - 32ms/epoch - 3ms/step\n",
      "Epoch 1805/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4812 - 37ms/epoch - 3ms/step\n",
      "Epoch 1806/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4816 - 35ms/epoch - 3ms/step\n",
      "Epoch 1807/2500\n",
      "11/11 - 0s - loss: 0.3742 - val_loss: 0.4758 - 43ms/epoch - 4ms/step\n",
      "Epoch 1808/2500\n",
      "11/11 - 0s - loss: 0.3857 - val_loss: 0.4770 - 35ms/epoch - 3ms/step\n",
      "Epoch 1809/2500\n",
      "11/11 - 0s - loss: 0.3860 - val_loss: 0.4791 - 35ms/epoch - 3ms/step\n",
      "Epoch 1810/2500\n",
      "11/11 - 0s - loss: 0.3803 - val_loss: 0.4699 - 33ms/epoch - 3ms/step\n",
      "Epoch 1811/2500\n",
      "11/11 - 0s - loss: 0.3793 - val_loss: 0.4792 - 31ms/epoch - 3ms/step\n",
      "Epoch 1812/2500\n",
      "11/11 - 0s - loss: 0.3891 - val_loss: 0.4758 - 33ms/epoch - 3ms/step\n",
      "Epoch 1813/2500\n",
      "11/11 - 0s - loss: 0.3671 - val_loss: 0.4823 - 33ms/epoch - 3ms/step\n",
      "Epoch 1814/2500\n",
      "11/11 - 0s - loss: 0.3806 - val_loss: 0.4887 - 35ms/epoch - 3ms/step\n",
      "Epoch 1815/2500\n",
      "11/11 - 0s - loss: 0.3915 - val_loss: 0.4883 - 32ms/epoch - 3ms/step\n",
      "Epoch 1816/2500\n",
      "11/11 - 0s - loss: 0.3861 - val_loss: 0.4858 - 31ms/epoch - 3ms/step\n",
      "Epoch 1817/2500\n",
      "11/11 - 0s - loss: 0.3785 - val_loss: 0.4810 - 32ms/epoch - 3ms/step\n",
      "Epoch 1818/2500\n",
      "11/11 - 0s - loss: 0.3729 - val_loss: 0.4724 - 31ms/epoch - 3ms/step\n",
      "Epoch 1819/2500\n",
      "11/11 - 0s - loss: 0.3855 - val_loss: 0.4686 - 34ms/epoch - 3ms/step\n",
      "Epoch 1820/2500\n",
      "11/11 - 0s - loss: 0.3852 - val_loss: 0.4700 - 37ms/epoch - 3ms/step\n",
      "Epoch 1821/2500\n",
      "11/11 - 0s - loss: 0.3738 - val_loss: 0.4829 - 41ms/epoch - 4ms/step\n",
      "Epoch 1822/2500\n",
      "11/11 - 0s - loss: 0.3815 - val_loss: 0.4809 - 39ms/epoch - 4ms/step\n",
      "Epoch 1823/2500\n",
      "11/11 - 0s - loss: 0.3643 - val_loss: 0.4702 - 32ms/epoch - 3ms/step\n",
      "Epoch 1824/2500\n",
      "11/11 - 0s - loss: 0.3819 - val_loss: 0.4798 - 33ms/epoch - 3ms/step\n",
      "Epoch 1825/2500\n",
      "11/11 - 0s - loss: 0.3708 - val_loss: 0.4744 - 36ms/epoch - 3ms/step\n",
      "Epoch 1826/2500\n",
      "11/11 - 0s - loss: 0.3858 - val_loss: 0.4721 - 32ms/epoch - 3ms/step\n",
      "Epoch 1827/2500\n",
      "11/11 - 0s - loss: 0.3918 - val_loss: 0.4756 - 33ms/epoch - 3ms/step\n",
      "Epoch 1828/2500\n",
      "11/11 - 0s - loss: 0.3819 - val_loss: 0.4719 - 33ms/epoch - 3ms/step\n",
      "Epoch 1829/2500\n",
      "11/11 - 0s - loss: 0.3964 - val_loss: 0.4847 - 33ms/epoch - 3ms/step\n",
      "Epoch 1830/2500\n",
      "11/11 - 0s - loss: 0.3881 - val_loss: 0.4890 - 32ms/epoch - 3ms/step\n",
      "Epoch 1831/2500\n",
      "11/11 - 0s - loss: 0.3856 - val_loss: 0.4690 - 33ms/epoch - 3ms/step\n",
      "Epoch 1832/2500\n",
      "11/11 - 0s - loss: 0.3986 - val_loss: 0.4672 - 33ms/epoch - 3ms/step\n",
      "Epoch 1833/2500\n",
      "11/11 - 0s - loss: 0.3818 - val_loss: 0.5009 - 33ms/epoch - 3ms/step\n",
      "Epoch 1834/2500\n",
      "11/11 - 0s - loss: 0.3955 - val_loss: 0.4753 - 32ms/epoch - 3ms/step\n",
      "Epoch 1835/2500\n",
      "11/11 - 0s - loss: 0.3899 - val_loss: 0.4791 - 48ms/epoch - 4ms/step\n",
      "Epoch 1836/2500\n",
      "11/11 - 0s - loss: 0.3891 - val_loss: 0.4789 - 36ms/epoch - 3ms/step\n",
      "Epoch 1837/2500\n",
      "11/11 - 0s - loss: 0.3880 - val_loss: 0.4744 - 34ms/epoch - 3ms/step\n",
      "Epoch 1838/2500\n",
      "11/11 - 0s - loss: 0.3877 - val_loss: 0.4898 - 35ms/epoch - 3ms/step\n",
      "Epoch 1839/2500\n",
      "11/11 - 0s - loss: 0.3824 - val_loss: 0.4786 - 34ms/epoch - 3ms/step\n",
      "Epoch 1840/2500\n",
      "11/11 - 0s - loss: 0.3879 - val_loss: 0.4876 - 33ms/epoch - 3ms/step\n",
      "Epoch 1841/2500\n",
      "11/11 - 0s - loss: 0.3836 - val_loss: 0.4902 - 37ms/epoch - 3ms/step\n",
      "Epoch 1842/2500\n",
      "11/11 - 0s - loss: 0.3839 - val_loss: 0.4777 - 33ms/epoch - 3ms/step\n",
      "Epoch 1843/2500\n",
      "11/11 - 0s - loss: 0.3893 - val_loss: 0.4846 - 36ms/epoch - 3ms/step\n",
      "Epoch 1844/2500\n",
      "11/11 - 0s - loss: 0.3797 - val_loss: 0.4871 - 33ms/epoch - 3ms/step\n",
      "Epoch 1845/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4887 - 34ms/epoch - 3ms/step\n",
      "Epoch 1846/2500\n",
      "11/11 - 0s - loss: 0.3921 - val_loss: 0.4697 - 33ms/epoch - 3ms/step\n",
      "Epoch 1847/2500\n",
      "11/11 - 0s - loss: 0.4026 - val_loss: 0.4690 - 34ms/epoch - 3ms/step\n",
      "Epoch 1848/2500\n",
      "11/11 - 0s - loss: 0.3912 - val_loss: 0.4839 - 36ms/epoch - 3ms/step\n",
      "Epoch 1849/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4817 - 32ms/epoch - 3ms/step\n",
      "Epoch 1850/2500\n",
      "11/11 - 0s - loss: 0.3753 - val_loss: 0.4758 - 33ms/epoch - 3ms/step\n",
      "Epoch 1851/2500\n",
      "11/11 - 0s - loss: 0.3840 - val_loss: 0.4728 - 33ms/epoch - 3ms/step\n",
      "Epoch 1852/2500\n",
      "11/11 - 0s - loss: 0.3799 - val_loss: 0.4825 - 39ms/epoch - 4ms/step\n",
      "Epoch 1853/2500\n",
      "11/11 - 0s - loss: 0.4012 - val_loss: 0.4826 - 40ms/epoch - 4ms/step\n",
      "Epoch 1854/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.4676 - 34ms/epoch - 3ms/step\n",
      "Epoch 1855/2500\n",
      "11/11 - 0s - loss: 0.3868 - val_loss: 0.4753 - 31ms/epoch - 3ms/step\n",
      "Epoch 1856/2500\n",
      "11/11 - 0s - loss: 0.3965 - val_loss: 0.4802 - 31ms/epoch - 3ms/step\n",
      "Epoch 1857/2500\n",
      "11/11 - 0s - loss: 0.3885 - val_loss: 0.4802 - 33ms/epoch - 3ms/step\n",
      "Epoch 1858/2500\n",
      "11/11 - 0s - loss: 0.3839 - val_loss: 0.4781 - 33ms/epoch - 3ms/step\n",
      "Epoch 1859/2500\n",
      "11/11 - 0s - loss: 0.3878 - val_loss: 0.4703 - 33ms/epoch - 3ms/step\n",
      "Epoch 1860/2500\n",
      "11/11 - 0s - loss: 0.3843 - val_loss: 0.4641 - 33ms/epoch - 3ms/step\n",
      "Epoch 1861/2500\n",
      "11/11 - 0s - loss: 0.3819 - val_loss: 0.4706 - 34ms/epoch - 3ms/step\n",
      "Epoch 1862/2500\n",
      "11/11 - 0s - loss: 0.3866 - val_loss: 0.4698 - 33ms/epoch - 3ms/step\n",
      "Epoch 1863/2500\n",
      "11/11 - 0s - loss: 0.3810 - val_loss: 0.4653 - 32ms/epoch - 3ms/step\n",
      "Epoch 1864/2500\n",
      "11/11 - 0s - loss: 0.3787 - val_loss: 0.4748 - 36ms/epoch - 3ms/step\n",
      "Epoch 1865/2500\n",
      "11/11 - 0s - loss: 0.3906 - val_loss: 0.4825 - 34ms/epoch - 3ms/step\n",
      "Epoch 1866/2500\n",
      "11/11 - 0s - loss: 0.3824 - val_loss: 0.4738 - 32ms/epoch - 3ms/step\n",
      "Epoch 1867/2500\n",
      "11/11 - 0s - loss: 0.3605 - val_loss: 0.4767 - 32ms/epoch - 3ms/step\n",
      "Epoch 1868/2500\n",
      "11/11 - 0s - loss: 0.3780 - val_loss: 0.4870 - 34ms/epoch - 3ms/step\n",
      "Epoch 1869/2500\n",
      "11/11 - 0s - loss: 0.3875 - val_loss: 0.4828 - 44ms/epoch - 4ms/step\n",
      "Epoch 1870/2500\n",
      "11/11 - 0s - loss: 0.3792 - val_loss: 0.4722 - 40ms/epoch - 4ms/step\n",
      "Epoch 1871/2500\n",
      "11/11 - 0s - loss: 0.3878 - val_loss: 0.4748 - 33ms/epoch - 3ms/step\n",
      "Epoch 1872/2500\n",
      "11/11 - 0s - loss: 0.3797 - val_loss: 0.4842 - 34ms/epoch - 3ms/step\n",
      "Epoch 1873/2500\n",
      "11/11 - 0s - loss: 0.3669 - val_loss: 0.4856 - 33ms/epoch - 3ms/step\n",
      "Epoch 1874/2500\n",
      "11/11 - 0s - loss: 0.3787 - val_loss: 0.4883 - 32ms/epoch - 3ms/step\n",
      "Epoch 1875/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4872 - 35ms/epoch - 3ms/step\n",
      "Epoch 1876/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4744 - 33ms/epoch - 3ms/step\n",
      "Epoch 1877/2500\n",
      "11/11 - 0s - loss: 0.3838 - val_loss: 0.4805 - 32ms/epoch - 3ms/step\n",
      "Epoch 1878/2500\n",
      "11/11 - 0s - loss: 0.3822 - val_loss: 0.4951 - 32ms/epoch - 3ms/step\n",
      "Epoch 1879/2500\n",
      "11/11 - 0s - loss: 0.4076 - val_loss: 0.4851 - 32ms/epoch - 3ms/step\n",
      "Epoch 1880/2500\n",
      "11/11 - 0s - loss: 0.3898 - val_loss: 0.4914 - 33ms/epoch - 3ms/step\n",
      "Epoch 1881/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4823 - 32ms/epoch - 3ms/step\n",
      "Epoch 1882/2500\n",
      "11/11 - 0s - loss: 0.3803 - val_loss: 0.4820 - 35ms/epoch - 3ms/step\n",
      "Epoch 1883/2500\n",
      "11/11 - 0s - loss: 0.3936 - val_loss: 0.4884 - 34ms/epoch - 3ms/step\n",
      "Epoch 1884/2500\n",
      "11/11 - 0s - loss: 0.3921 - val_loss: 0.4843 - 38ms/epoch - 3ms/step\n",
      "Epoch 1885/2500\n",
      "11/11 - 0s - loss: 0.3767 - val_loss: 0.4787 - 33ms/epoch - 3ms/step\n",
      "Epoch 1886/2500\n",
      "11/11 - 0s - loss: 0.3869 - val_loss: 0.4887 - 41ms/epoch - 4ms/step\n",
      "Epoch 1887/2500\n",
      "11/11 - 0s - loss: 0.3602 - val_loss: 0.4733 - 32ms/epoch - 3ms/step\n",
      "Epoch 1888/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4811 - 33ms/epoch - 3ms/step\n",
      "Epoch 1889/2500\n",
      "11/11 - 0s - loss: 0.3830 - val_loss: 0.4820 - 36ms/epoch - 3ms/step\n",
      "Epoch 1890/2500\n",
      "11/11 - 0s - loss: 0.3829 - val_loss: 0.4795 - 34ms/epoch - 3ms/step\n",
      "Epoch 1891/2500\n",
      "11/11 - 0s - loss: 0.3897 - val_loss: 0.4836 - 32ms/epoch - 3ms/step\n",
      "Epoch 1892/2500\n",
      "11/11 - 0s - loss: 0.3719 - val_loss: 0.4797 - 33ms/epoch - 3ms/step\n",
      "Epoch 1893/2500\n",
      "11/11 - 0s - loss: 0.3829 - val_loss: 0.4831 - 33ms/epoch - 3ms/step\n",
      "Epoch 1894/2500\n",
      "11/11 - 0s - loss: 0.3811 - val_loss: 0.4911 - 31ms/epoch - 3ms/step\n",
      "Epoch 1895/2500\n",
      "11/11 - 0s - loss: 0.3781 - val_loss: 0.4875 - 35ms/epoch - 3ms/step\n",
      "Epoch 1896/2500\n",
      "11/11 - 0s - loss: 0.3858 - val_loss: 0.4766 - 32ms/epoch - 3ms/step\n",
      "Epoch 1897/2500\n",
      "11/11 - 0s - loss: 0.3776 - val_loss: 0.4897 - 32ms/epoch - 3ms/step\n",
      "Epoch 1898/2500\n",
      "11/11 - 0s - loss: 0.3901 - val_loss: 0.4778 - 32ms/epoch - 3ms/step\n",
      "Epoch 1899/2500\n",
      "11/11 - 0s - loss: 0.3808 - val_loss: 0.4833 - 32ms/epoch - 3ms/step\n",
      "Epoch 1900/2500\n",
      "11/11 - 0s - loss: 0.3896 - val_loss: 0.4771 - 43ms/epoch - 4ms/step\n",
      "Epoch 1901/2500\n",
      "11/11 - 0s - loss: 0.3735 - val_loss: 0.4811 - 38ms/epoch - 3ms/step\n",
      "Epoch 1902/2500\n",
      "11/11 - 0s - loss: 0.3721 - val_loss: 0.4928 - 31ms/epoch - 3ms/step\n",
      "Epoch 1903/2500\n",
      "11/11 - 0s - loss: 0.3923 - val_loss: 0.4776 - 33ms/epoch - 3ms/step\n",
      "Epoch 1904/2500\n",
      "11/11 - 0s - loss: 0.3750 - val_loss: 0.4754 - 33ms/epoch - 3ms/step\n",
      "Epoch 1905/2500\n",
      "11/11 - 0s - loss: 0.3855 - val_loss: 0.4819 - 39ms/epoch - 4ms/step\n",
      "Epoch 1906/2500\n",
      "11/11 - 0s - loss: 0.3853 - val_loss: 0.4750 - 34ms/epoch - 3ms/step\n",
      "Epoch 1907/2500\n",
      "11/11 - 0s - loss: 0.3890 - val_loss: 0.4814 - 35ms/epoch - 3ms/step\n",
      "Epoch 1908/2500\n",
      "11/11 - 0s - loss: 0.3893 - val_loss: 0.4918 - 37ms/epoch - 3ms/step\n",
      "Epoch 1909/2500\n",
      "11/11 - 0s - loss: 0.3752 - val_loss: 0.4825 - 35ms/epoch - 3ms/step\n",
      "Epoch 1910/2500\n",
      "11/11 - 0s - loss: 0.3937 - val_loss: 0.4741 - 35ms/epoch - 3ms/step\n",
      "Epoch 1911/2500\n",
      "11/11 - 0s - loss: 0.3808 - val_loss: 0.4737 - 35ms/epoch - 3ms/step\n",
      "Epoch 1912/2500\n",
      "11/11 - 0s - loss: 0.3709 - val_loss: 0.4762 - 34ms/epoch - 3ms/step\n",
      "Epoch 1913/2500\n",
      "11/11 - 0s - loss: 0.3997 - val_loss: 0.4794 - 35ms/epoch - 3ms/step\n",
      "Epoch 1914/2500\n",
      "11/11 - 0s - loss: 0.3948 - val_loss: 0.4813 - 38ms/epoch - 3ms/step\n",
      "Epoch 1915/2500\n",
      "11/11 - 0s - loss: 0.3763 - val_loss: 0.4884 - 34ms/epoch - 3ms/step\n",
      "Epoch 1916/2500\n",
      "11/11 - 0s - loss: 0.3675 - val_loss: 0.4858 - 34ms/epoch - 3ms/step\n",
      "Epoch 1917/2500\n",
      "11/11 - 0s - loss: 0.3715 - val_loss: 0.4899 - 52ms/epoch - 5ms/step\n",
      "Epoch 1918/2500\n",
      "11/11 - 0s - loss: 0.3754 - val_loss: 0.4890 - 35ms/epoch - 3ms/step\n",
      "Epoch 1919/2500\n",
      "11/11 - 0s - loss: 0.3737 - val_loss: 0.4995 - 35ms/epoch - 3ms/step\n",
      "Epoch 1920/2500\n",
      "11/11 - 0s - loss: 0.3936 - val_loss: 0.4907 - 40ms/epoch - 4ms/step\n",
      "Epoch 1921/2500\n",
      "11/11 - 0s - loss: 0.3880 - val_loss: 0.4913 - 37ms/epoch - 3ms/step\n",
      "Epoch 1922/2500\n",
      "11/11 - 0s - loss: 0.3902 - val_loss: 0.4812 - 35ms/epoch - 3ms/step\n",
      "Epoch 1923/2500\n",
      "11/11 - 0s - loss: 0.3835 - val_loss: 0.4863 - 36ms/epoch - 3ms/step\n",
      "Epoch 1924/2500\n",
      "11/11 - 0s - loss: 0.3828 - val_loss: 0.4875 - 35ms/epoch - 3ms/step\n",
      "Epoch 1925/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4795 - 38ms/epoch - 3ms/step\n",
      "Epoch 1926/2500\n",
      "11/11 - 0s - loss: 0.3774 - val_loss: 0.4900 - 36ms/epoch - 3ms/step\n",
      "Epoch 1927/2500\n",
      "11/11 - 0s - loss: 0.3939 - val_loss: 0.4824 - 33ms/epoch - 3ms/step\n",
      "Epoch 1928/2500\n",
      "11/11 - 0s - loss: 0.3834 - val_loss: 0.4811 - 31ms/epoch - 3ms/step\n",
      "Epoch 1929/2500\n",
      "11/11 - 0s - loss: 0.3810 - val_loss: 0.4877 - 34ms/epoch - 3ms/step\n",
      "Epoch 1930/2500\n",
      "11/11 - 0s - loss: 0.3803 - val_loss: 0.4800 - 34ms/epoch - 3ms/step\n",
      "Epoch 1931/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4670 - 33ms/epoch - 3ms/step\n",
      "Epoch 1932/2500\n",
      "11/11 - 0s - loss: 0.3844 - val_loss: 0.4777 - 43ms/epoch - 4ms/step\n",
      "Epoch 1933/2500\n",
      "11/11 - 0s - loss: 0.3626 - val_loss: 0.4767 - 38ms/epoch - 3ms/step\n",
      "Epoch 1934/2500\n",
      "11/11 - 0s - loss: 0.3754 - val_loss: 0.4743 - 33ms/epoch - 3ms/step\n",
      "Epoch 1935/2500\n",
      "11/11 - 0s - loss: 0.3726 - val_loss: 0.4915 - 33ms/epoch - 3ms/step\n",
      "Epoch 1936/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4896 - 33ms/epoch - 3ms/step\n",
      "Epoch 1937/2500\n",
      "11/11 - 0s - loss: 0.3836 - val_loss: 0.4744 - 32ms/epoch - 3ms/step\n",
      "Epoch 1938/2500\n",
      "11/11 - 0s - loss: 0.3878 - val_loss: 0.4902 - 32ms/epoch - 3ms/step\n",
      "Epoch 1939/2500\n",
      "11/11 - 0s - loss: 0.3909 - val_loss: 0.5090 - 31ms/epoch - 3ms/step\n",
      "Epoch 1940/2500\n",
      "11/11 - 0s - loss: 0.3873 - val_loss: 0.4862 - 34ms/epoch - 3ms/step\n",
      "Epoch 1941/2500\n",
      "11/11 - 0s - loss: 0.3825 - val_loss: 0.4830 - 33ms/epoch - 3ms/step\n",
      "Epoch 1942/2500\n",
      "11/11 - 0s - loss: 0.4008 - val_loss: 0.4869 - 32ms/epoch - 3ms/step\n",
      "Epoch 1943/2500\n",
      "11/11 - 0s - loss: 0.3810 - val_loss: 0.4855 - 32ms/epoch - 3ms/step\n",
      "Epoch 1944/2500\n",
      "11/11 - 0s - loss: 0.3868 - val_loss: 0.4876 - 32ms/epoch - 3ms/step\n",
      "Epoch 1945/2500\n",
      "11/11 - 0s - loss: 0.3885 - val_loss: 0.4870 - 36ms/epoch - 3ms/step\n",
      "Epoch 1946/2500\n",
      "11/11 - 0s - loss: 0.3902 - val_loss: 0.4890 - 32ms/epoch - 3ms/step\n",
      "Epoch 1947/2500\n",
      "11/11 - 0s - loss: 0.3854 - val_loss: 0.4886 - 33ms/epoch - 3ms/step\n",
      "Epoch 1948/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.4907 - 33ms/epoch - 3ms/step\n",
      "Epoch 1949/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4888 - 42ms/epoch - 4ms/step\n",
      "Epoch 1950/2500\n",
      "11/11 - 0s - loss: 0.3834 - val_loss: 0.4915 - 37ms/epoch - 3ms/step\n",
      "Epoch 1951/2500\n",
      "11/11 - 0s - loss: 0.3784 - val_loss: 0.4835 - 34ms/epoch - 3ms/step\n",
      "Epoch 1952/2500\n",
      "11/11 - 0s - loss: 0.3877 - val_loss: 0.4858 - 35ms/epoch - 3ms/step\n",
      "Epoch 1953/2500\n",
      "11/11 - 0s - loss: 0.4012 - val_loss: 0.4837 - 34ms/epoch - 3ms/step\n",
      "Epoch 1954/2500\n",
      "11/11 - 0s - loss: 0.3846 - val_loss: 0.4835 - 36ms/epoch - 3ms/step\n",
      "Epoch 1955/2500\n",
      "11/11 - 0s - loss: 0.3732 - val_loss: 0.4848 - 34ms/epoch - 3ms/step\n",
      "Epoch 1956/2500\n",
      "11/11 - 0s - loss: 0.3863 - val_loss: 0.4829 - 32ms/epoch - 3ms/step\n",
      "Epoch 1957/2500\n",
      "11/11 - 0s - loss: 0.3883 - val_loss: 0.5025 - 32ms/epoch - 3ms/step\n",
      "Epoch 1958/2500\n",
      "11/11 - 0s - loss: 0.3917 - val_loss: 0.4780 - 34ms/epoch - 3ms/step\n",
      "Epoch 1959/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4829 - 34ms/epoch - 3ms/step\n",
      "Epoch 1960/2500\n",
      "11/11 - 0s - loss: 0.3761 - val_loss: 0.4858 - 38ms/epoch - 3ms/step\n",
      "Epoch 1961/2500\n",
      "11/11 - 0s - loss: 0.3886 - val_loss: 0.4823 - 36ms/epoch - 3ms/step\n",
      "Epoch 1962/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4960 - 35ms/epoch - 3ms/step\n",
      "Epoch 1963/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4747 - 33ms/epoch - 3ms/step\n",
      "Epoch 1964/2500\n",
      "11/11 - 0s - loss: 0.3864 - val_loss: 0.4747 - 34ms/epoch - 3ms/step\n",
      "Epoch 1965/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4806 - 63ms/epoch - 6ms/step\n",
      "Epoch 1966/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.4882 - 32ms/epoch - 3ms/step\n",
      "Epoch 1967/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4908 - 35ms/epoch - 3ms/step\n",
      "Epoch 1968/2500\n",
      "11/11 - 0s - loss: 0.3666 - val_loss: 0.4763 - 33ms/epoch - 3ms/step\n",
      "Epoch 1969/2500\n",
      "11/11 - 0s - loss: 0.3899 - val_loss: 0.4836 - 32ms/epoch - 3ms/step\n",
      "Epoch 1970/2500\n",
      "11/11 - 0s - loss: 0.3781 - val_loss: 0.5022 - 32ms/epoch - 3ms/step\n",
      "Epoch 1971/2500\n",
      "11/11 - 0s - loss: 0.3911 - val_loss: 0.4930 - 33ms/epoch - 3ms/step\n",
      "Epoch 1972/2500\n",
      "11/11 - 0s - loss: 0.3825 - val_loss: 0.4865 - 32ms/epoch - 3ms/step\n",
      "Epoch 1973/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4990 - 35ms/epoch - 3ms/step\n",
      "Epoch 1974/2500\n",
      "11/11 - 0s - loss: 0.3819 - val_loss: 0.4782 - 32ms/epoch - 3ms/step\n",
      "Epoch 1975/2500\n",
      "11/11 - 0s - loss: 0.3888 - val_loss: 0.4876 - 32ms/epoch - 3ms/step\n",
      "Epoch 1976/2500\n",
      "11/11 - 0s - loss: 0.3848 - val_loss: 0.4789 - 33ms/epoch - 3ms/step\n",
      "Epoch 1977/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4809 - 33ms/epoch - 3ms/step\n",
      "Epoch 1978/2500\n",
      "11/11 - 0s - loss: 0.3777 - val_loss: 0.4948 - 31ms/epoch - 3ms/step\n",
      "Epoch 1979/2500\n",
      "11/11 - 0s - loss: 0.3942 - val_loss: 0.4821 - 32ms/epoch - 3ms/step\n",
      "Epoch 1980/2500\n",
      "11/11 - 0s - loss: 0.3814 - val_loss: 0.4886 - 36ms/epoch - 3ms/step\n",
      "Epoch 1981/2500\n",
      "11/11 - 0s - loss: 0.3672 - val_loss: 0.4997 - 33ms/epoch - 3ms/step\n",
      "Epoch 1982/2500\n",
      "11/11 - 0s - loss: 0.3789 - val_loss: 0.4884 - 46ms/epoch - 4ms/step\n",
      "Epoch 1983/2500\n",
      "11/11 - 0s - loss: 0.3784 - val_loss: 0.4950 - 32ms/epoch - 3ms/step\n",
      "Epoch 1984/2500\n",
      "11/11 - 0s - loss: 0.3866 - val_loss: 0.4823 - 33ms/epoch - 3ms/step\n",
      "Epoch 1985/2500\n",
      "11/11 - 0s - loss: 0.3852 - val_loss: 0.4768 - 40ms/epoch - 4ms/step\n",
      "Epoch 1986/2500\n",
      "11/11 - 0s - loss: 0.3828 - val_loss: 0.4829 - 37ms/epoch - 3ms/step\n",
      "Epoch 1987/2500\n",
      "11/11 - 0s - loss: 0.3809 - val_loss: 0.4794 - 35ms/epoch - 3ms/step\n",
      "Epoch 1988/2500\n",
      "11/11 - 0s - loss: 0.3773 - val_loss: 0.4920 - 37ms/epoch - 3ms/step\n",
      "Epoch 1989/2500\n",
      "11/11 - 0s - loss: 0.3722 - val_loss: 0.4799 - 40ms/epoch - 4ms/step\n",
      "Epoch 1990/2500\n",
      "11/11 - 0s - loss: 0.3944 - val_loss: 0.4829 - 35ms/epoch - 3ms/step\n",
      "Epoch 1991/2500\n",
      "11/11 - 0s - loss: 0.3758 - val_loss: 0.4994 - 34ms/epoch - 3ms/step\n",
      "Epoch 1992/2500\n",
      "11/11 - 0s - loss: 0.3838 - val_loss: 0.4767 - 32ms/epoch - 3ms/step\n",
      "Epoch 1993/2500\n",
      "11/11 - 0s - loss: 0.3889 - val_loss: 0.4711 - 33ms/epoch - 3ms/step\n",
      "Epoch 1994/2500\n",
      "11/11 - 0s - loss: 0.3769 - val_loss: 0.4868 - 33ms/epoch - 3ms/step\n",
      "Epoch 1995/2500\n",
      "11/11 - 0s - loss: 0.3936 - val_loss: 0.4970 - 32ms/epoch - 3ms/step\n",
      "Epoch 1996/2500\n",
      "11/11 - 0s - loss: 0.3728 - val_loss: 0.4937 - 32ms/epoch - 3ms/step\n",
      "Epoch 1997/2500\n",
      "11/11 - 0s - loss: 0.3745 - val_loss: 0.4726 - 33ms/epoch - 3ms/step\n",
      "Epoch 1998/2500\n",
      "11/11 - 0s - loss: 0.3868 - val_loss: 0.4740 - 32ms/epoch - 3ms/step\n",
      "Epoch 1999/2500\n",
      "11/11 - 0s - loss: 0.3721 - val_loss: 0.4932 - 31ms/epoch - 3ms/step\n",
      "Epoch 2000/2500\n",
      "11/11 - 0s - loss: 0.3952 - val_loss: 0.5127 - 33ms/epoch - 3ms/step\n",
      "Epoch 2001/2500\n",
      "11/11 - 0s - loss: 0.3812 - val_loss: 0.4869 - 32ms/epoch - 3ms/step\n",
      "Epoch 2002/2500\n",
      "11/11 - 0s - loss: 0.3879 - val_loss: 0.4810 - 49ms/epoch - 4ms/step\n",
      "Epoch 2003/2500\n",
      "11/11 - 0s - loss: 0.3594 - val_loss: 0.4972 - 35ms/epoch - 3ms/step\n",
      "Epoch 2004/2500\n",
      "11/11 - 0s - loss: 0.3908 - val_loss: 0.4768 - 34ms/epoch - 3ms/step\n",
      "Epoch 2005/2500\n",
      "11/11 - 0s - loss: 0.3861 - val_loss: 0.4794 - 39ms/epoch - 4ms/step\n",
      "Epoch 2006/2500\n",
      "11/11 - 0s - loss: 0.3948 - val_loss: 0.4885 - 35ms/epoch - 3ms/step\n",
      "Epoch 2007/2500\n",
      "11/11 - 0s - loss: 0.3866 - val_loss: 0.4867 - 38ms/epoch - 3ms/step\n",
      "Epoch 2008/2500\n",
      "11/11 - 0s - loss: 0.3850 - val_loss: 0.4822 - 37ms/epoch - 3ms/step\n",
      "Epoch 2009/2500\n",
      "11/11 - 0s - loss: 0.3823 - val_loss: 0.4797 - 35ms/epoch - 3ms/step\n",
      "Epoch 2010/2500\n",
      "11/11 - 0s - loss: 0.3899 - val_loss: 0.4783 - 34ms/epoch - 3ms/step\n",
      "Epoch 2011/2500\n",
      "11/11 - 0s - loss: 0.3913 - val_loss: 0.4794 - 34ms/epoch - 3ms/step\n",
      "Epoch 2012/2500\n",
      "11/11 - 0s - loss: 0.3912 - val_loss: 0.4731 - 35ms/epoch - 3ms/step\n",
      "Epoch 2013/2500\n",
      "11/11 - 0s - loss: 0.3891 - val_loss: 0.4720 - 38ms/epoch - 3ms/step\n",
      "Epoch 2014/2500\n",
      "11/11 - 0s - loss: 0.3985 - val_loss: 0.4766 - 35ms/epoch - 3ms/step\n",
      "Epoch 2015/2500\n",
      "11/11 - 0s - loss: 0.3874 - val_loss: 0.4950 - 35ms/epoch - 3ms/step\n",
      "Epoch 2016/2500\n",
      "11/11 - 0s - loss: 0.3681 - val_loss: 0.4758 - 36ms/epoch - 3ms/step\n",
      "Epoch 2017/2500\n",
      "11/11 - 0s - loss: 0.3672 - val_loss: 0.4754 - 36ms/epoch - 3ms/step\n",
      "Epoch 2018/2500\n",
      "11/11 - 0s - loss: 0.3927 - val_loss: 0.4683 - 33ms/epoch - 3ms/step\n",
      "Epoch 2019/2500\n",
      "11/11 - 0s - loss: 0.3975 - val_loss: 0.4809 - 45ms/epoch - 4ms/step\n",
      "Epoch 2020/2500\n",
      "11/11 - 0s - loss: 0.3790 - val_loss: 0.4948 - 36ms/epoch - 3ms/step\n",
      "Epoch 2021/2500\n",
      "11/11 - 0s - loss: 0.3922 - val_loss: 0.4897 - 34ms/epoch - 3ms/step\n",
      "Epoch 2022/2500\n",
      "11/11 - 0s - loss: 0.3648 - val_loss: 0.4782 - 32ms/epoch - 3ms/step\n",
      "Epoch 2023/2500\n",
      "11/11 - 0s - loss: 0.3951 - val_loss: 0.4764 - 34ms/epoch - 3ms/step\n",
      "Epoch 2024/2500\n",
      "11/11 - 0s - loss: 0.3835 - val_loss: 0.4755 - 34ms/epoch - 3ms/step\n",
      "Epoch 2025/2500\n",
      "11/11 - 0s - loss: 0.3635 - val_loss: 0.4845 - 43ms/epoch - 4ms/step\n",
      "Epoch 2026/2500\n",
      "11/11 - 0s - loss: 0.3719 - val_loss: 0.4821 - 39ms/epoch - 4ms/step\n",
      "Epoch 2027/2500\n",
      "11/11 - 0s - loss: 0.3753 - val_loss: 0.4852 - 36ms/epoch - 3ms/step\n",
      "Epoch 2028/2500\n",
      "11/11 - 0s - loss: 0.3809 - val_loss: 0.4773 - 34ms/epoch - 3ms/step\n",
      "Epoch 2029/2500\n",
      "11/11 - 0s - loss: 0.3816 - val_loss: 0.4743 - 32ms/epoch - 3ms/step\n",
      "Epoch 2030/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4783 - 31ms/epoch - 3ms/step\n",
      "Epoch 2031/2500\n",
      "11/11 - 0s - loss: 0.3882 - val_loss: 0.4808 - 32ms/epoch - 3ms/step\n",
      "Epoch 2032/2500\n",
      "11/11 - 0s - loss: 0.3808 - val_loss: 0.4713 - 34ms/epoch - 3ms/step\n",
      "Epoch 2033/2500\n",
      "11/11 - 0s - loss: 0.3797 - val_loss: 0.4820 - 32ms/epoch - 3ms/step\n",
      "Epoch 2034/2500\n",
      "11/11 - 0s - loss: 0.3694 - val_loss: 0.5002 - 31ms/epoch - 3ms/step\n",
      "Epoch 2035/2500\n",
      "11/11 - 0s - loss: 0.3922 - val_loss: 0.4893 - 45ms/epoch - 4ms/step\n",
      "Epoch 2036/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4737 - 33ms/epoch - 3ms/step\n",
      "Epoch 2037/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4769 - 33ms/epoch - 3ms/step\n",
      "Epoch 2038/2500\n",
      "11/11 - 0s - loss: 0.3863 - val_loss: 0.4912 - 32ms/epoch - 3ms/step\n",
      "Epoch 2039/2500\n",
      "11/11 - 0s - loss: 0.3820 - val_loss: 0.4801 - 35ms/epoch - 3ms/step\n",
      "Epoch 2040/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4826 - 32ms/epoch - 3ms/step\n",
      "Epoch 2041/2500\n",
      "11/11 - 0s - loss: 0.3760 - val_loss: 0.4829 - 32ms/epoch - 3ms/step\n",
      "Epoch 2042/2500\n",
      "11/11 - 0s - loss: 0.3744 - val_loss: 0.4809 - 33ms/epoch - 3ms/step\n",
      "Epoch 2043/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4795 - 34ms/epoch - 3ms/step\n",
      "Epoch 2044/2500\n",
      "11/11 - 0s - loss: 0.3892 - val_loss: 0.4804 - 32ms/epoch - 3ms/step\n",
      "Epoch 2045/2500\n",
      "11/11 - 0s - loss: 0.3808 - val_loss: 0.4878 - 38ms/epoch - 3ms/step\n",
      "Epoch 2046/2500\n",
      "11/11 - 0s - loss: 0.3867 - val_loss: 0.4919 - 35ms/epoch - 3ms/step\n",
      "Epoch 2047/2500\n",
      "11/11 - 0s - loss: 0.3792 - val_loss: 0.4947 - 33ms/epoch - 3ms/step\n",
      "Epoch 2048/2500\n",
      "11/11 - 0s - loss: 0.3644 - val_loss: 0.4893 - 32ms/epoch - 3ms/step\n",
      "Epoch 2049/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4822 - 31ms/epoch - 3ms/step\n",
      "Epoch 2050/2500\n",
      "11/11 - 0s - loss: 0.3761 - val_loss: 0.4795 - 32ms/epoch - 3ms/step\n",
      "Epoch 2051/2500\n",
      "11/11 - 0s - loss: 0.3674 - val_loss: 0.4798 - 32ms/epoch - 3ms/step\n",
      "Epoch 2052/2500\n",
      "11/11 - 0s - loss: 0.3854 - val_loss: 0.4879 - 47ms/epoch - 4ms/step\n",
      "Epoch 2053/2500\n",
      "11/11 - 0s - loss: 0.3796 - val_loss: 0.4815 - 34ms/epoch - 3ms/step\n",
      "Epoch 2054/2500\n",
      "11/11 - 0s - loss: 0.3879 - val_loss: 0.4884 - 34ms/epoch - 3ms/step\n",
      "Epoch 2055/2500\n",
      "11/11 - 0s - loss: 0.3762 - val_loss: 0.4910 - 33ms/epoch - 3ms/step\n",
      "Epoch 2056/2500\n",
      "11/11 - 0s - loss: 0.3558 - val_loss: 0.4799 - 33ms/epoch - 3ms/step\n",
      "Epoch 2057/2500\n",
      "11/11 - 0s - loss: 0.3787 - val_loss: 0.4794 - 32ms/epoch - 3ms/step\n",
      "Epoch 2058/2500\n",
      "11/11 - 0s - loss: 0.3668 - val_loss: 0.4841 - 33ms/epoch - 3ms/step\n",
      "Epoch 2059/2500\n",
      "11/11 - 0s - loss: 0.3749 - val_loss: 0.4789 - 34ms/epoch - 3ms/step\n",
      "Epoch 2060/2500\n",
      "11/11 - 0s - loss: 0.3809 - val_loss: 0.4885 - 32ms/epoch - 3ms/step\n",
      "Epoch 2061/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.4989 - 33ms/epoch - 3ms/step\n",
      "Epoch 2062/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4880 - 32ms/epoch - 3ms/step\n",
      "Epoch 2063/2500\n",
      "11/11 - 0s - loss: 0.3660 - val_loss: 0.4886 - 32ms/epoch - 3ms/step\n",
      "Epoch 2064/2500\n",
      "11/11 - 0s - loss: 0.3797 - val_loss: 0.4882 - 35ms/epoch - 3ms/step\n",
      "Epoch 2065/2500\n",
      "11/11 - 0s - loss: 0.3826 - val_loss: 0.4927 - 34ms/epoch - 3ms/step\n",
      "Epoch 2066/2500\n",
      "11/11 - 0s - loss: 0.3716 - val_loss: 0.4939 - 35ms/epoch - 3ms/step\n",
      "Epoch 2067/2500\n",
      "11/11 - 0s - loss: 0.3836 - val_loss: 0.4812 - 34ms/epoch - 3ms/step\n",
      "Epoch 2068/2500\n",
      "11/11 - 0s - loss: 0.3742 - val_loss: 0.4824 - 32ms/epoch - 3ms/step\n",
      "Epoch 2069/2500\n",
      "11/11 - 0s - loss: 0.3720 - val_loss: 0.4840 - 32ms/epoch - 3ms/step\n",
      "Epoch 2070/2500\n",
      "11/11 - 0s - loss: 0.3863 - val_loss: 0.4861 - 32ms/epoch - 3ms/step\n",
      "Epoch 2071/2500\n",
      "11/11 - 0s - loss: 0.3792 - val_loss: 0.4880 - 46ms/epoch - 4ms/step\n",
      "Epoch 2072/2500\n",
      "11/11 - 0s - loss: 0.3920 - val_loss: 0.4918 - 37ms/epoch - 3ms/step\n",
      "Epoch 2073/2500\n",
      "11/11 - 0s - loss: 0.3771 - val_loss: 0.4794 - 33ms/epoch - 3ms/step\n",
      "Epoch 2074/2500\n",
      "11/11 - 0s - loss: 0.3720 - val_loss: 0.4943 - 35ms/epoch - 3ms/step\n",
      "Epoch 2075/2500\n",
      "11/11 - 0s - loss: 0.3777 - val_loss: 0.4817 - 37ms/epoch - 3ms/step\n",
      "Epoch 2076/2500\n",
      "11/11 - 0s - loss: 0.3832 - val_loss: 0.4820 - 33ms/epoch - 3ms/step\n",
      "Epoch 2077/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4997 - 32ms/epoch - 3ms/step\n",
      "Epoch 2078/2500\n",
      "11/11 - 0s - loss: 0.3723 - val_loss: 0.4875 - 34ms/epoch - 3ms/step\n",
      "Epoch 2079/2500\n",
      "11/11 - 0s - loss: 0.3798 - val_loss: 0.4818 - 32ms/epoch - 3ms/step\n",
      "Epoch 2080/2500\n",
      "11/11 - 0s - loss: 0.3847 - val_loss: 0.4889 - 32ms/epoch - 3ms/step\n",
      "Epoch 2081/2500\n",
      "11/11 - 0s - loss: 0.3707 - val_loss: 0.4908 - 32ms/epoch - 3ms/step\n",
      "Epoch 2082/2500\n",
      "11/11 - 0s - loss: 0.3694 - val_loss: 0.4805 - 31ms/epoch - 3ms/step\n",
      "Epoch 2083/2500\n",
      "11/11 - 0s - loss: 0.3929 - val_loss: 0.4794 - 38ms/epoch - 3ms/step\n",
      "Epoch 2084/2500\n",
      "11/11 - 0s - loss: 0.3810 - val_loss: 0.4997 - 32ms/epoch - 3ms/step\n",
      "Epoch 2085/2500\n",
      "11/11 - 0s - loss: 0.3909 - val_loss: 0.4815 - 34ms/epoch - 3ms/step\n",
      "Epoch 2086/2500\n",
      "11/11 - 0s - loss: 0.3949 - val_loss: 0.4824 - 32ms/epoch - 3ms/step\n",
      "Epoch 2087/2500\n",
      "11/11 - 0s - loss: 0.3842 - val_loss: 0.5082 - 32ms/epoch - 3ms/step\n",
      "Epoch 2088/2500\n",
      "11/11 - 0s - loss: 0.3788 - val_loss: 0.4864 - 46ms/epoch - 4ms/step\n",
      "Epoch 2089/2500\n",
      "11/11 - 0s - loss: 0.3781 - val_loss: 0.4863 - 32ms/epoch - 3ms/step\n",
      "Epoch 2090/2500\n",
      "11/11 - 0s - loss: 0.3873 - val_loss: 0.4864 - 32ms/epoch - 3ms/step\n",
      "Epoch 2091/2500\n",
      "11/11 - 0s - loss: 0.3641 - val_loss: 0.4905 - 34ms/epoch - 3ms/step\n",
      "Epoch 2092/2500\n",
      "11/11 - 0s - loss: 0.3773 - val_loss: 0.4885 - 33ms/epoch - 3ms/step\n",
      "Epoch 2093/2500\n",
      "11/11 - 0s - loss: 0.3987 - val_loss: 0.4810 - 32ms/epoch - 3ms/step\n",
      "Epoch 2094/2500\n",
      "11/11 - 0s - loss: 0.3659 - val_loss: 0.4909 - 31ms/epoch - 3ms/step\n",
      "Epoch 2095/2500\n",
      "11/11 - 0s - loss: 0.3839 - val_loss: 0.4954 - 31ms/epoch - 3ms/step\n",
      "Epoch 2096/2500\n",
      "11/11 - 0s - loss: 0.3727 - val_loss: 0.4889 - 31ms/epoch - 3ms/step\n",
      "Epoch 2097/2500\n",
      "11/11 - 0s - loss: 0.3773 - val_loss: 0.4882 - 34ms/epoch - 3ms/step\n",
      "Epoch 2098/2500\n",
      "11/11 - 0s - loss: 0.3742 - val_loss: 0.4887 - 34ms/epoch - 3ms/step\n",
      "Epoch 2099/2500\n",
      "11/11 - 0s - loss: 0.3873 - val_loss: 0.4890 - 32ms/epoch - 3ms/step\n",
      "Epoch 2100/2500\n",
      "11/11 - 0s - loss: 0.3735 - val_loss: 0.4982 - 32ms/epoch - 3ms/step\n",
      "Epoch 2101/2500\n",
      "11/11 - 0s - loss: 0.3724 - val_loss: 0.4800 - 32ms/epoch - 3ms/step\n",
      "Epoch 2102/2500\n",
      "11/11 - 0s - loss: 0.3766 - val_loss: 0.4838 - 34ms/epoch - 3ms/step\n",
      "Epoch 2103/2500\n",
      "11/11 - 0s - loss: 0.3833 - val_loss: 0.4823 - 34ms/epoch - 3ms/step\n",
      "Epoch 2104/2500\n",
      "11/11 - 0s - loss: 0.3926 - val_loss: 0.4868 - 54ms/epoch - 5ms/step\n",
      "Epoch 2105/2500\n",
      "11/11 - 0s - loss: 0.3867 - val_loss: 0.4973 - 34ms/epoch - 3ms/step\n",
      "Epoch 2106/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4896 - 33ms/epoch - 3ms/step\n",
      "Epoch 2107/2500\n",
      "11/11 - 0s - loss: 0.3806 - val_loss: 0.4927 - 32ms/epoch - 3ms/step\n",
      "Epoch 2108/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4943 - 32ms/epoch - 3ms/step\n",
      "Epoch 2109/2500\n",
      "11/11 - 0s - loss: 0.3847 - val_loss: 0.4894 - 32ms/epoch - 3ms/step\n",
      "Epoch 2110/2500\n",
      "11/11 - 0s - loss: 0.3831 - val_loss: 0.4795 - 32ms/epoch - 3ms/step\n",
      "Epoch 2111/2500\n",
      "11/11 - 0s - loss: 0.3902 - val_loss: 0.4908 - 35ms/epoch - 3ms/step\n",
      "Epoch 2112/2500\n",
      "11/11 - 0s - loss: 0.3845 - val_loss: 0.4881 - 32ms/epoch - 3ms/step\n",
      "Epoch 2113/2500\n",
      "11/11 - 0s - loss: 0.3760 - val_loss: 0.4877 - 32ms/epoch - 3ms/step\n",
      "Epoch 2114/2500\n",
      "11/11 - 0s - loss: 0.3884 - val_loss: 0.4916 - 31ms/epoch - 3ms/step\n",
      "Epoch 2115/2500\n",
      "11/11 - 0s - loss: 0.3831 - val_loss: 0.4911 - 32ms/epoch - 3ms/step\n",
      "Epoch 2116/2500\n",
      "11/11 - 0s - loss: 0.3650 - val_loss: 0.4802 - 32ms/epoch - 3ms/step\n",
      "Epoch 2117/2500\n",
      "11/11 - 0s - loss: 0.3826 - val_loss: 0.4866 - 34ms/epoch - 3ms/step\n",
      "Epoch 2118/2500\n",
      "11/11 - 0s - loss: 0.3813 - val_loss: 0.4905 - 32ms/epoch - 3ms/step\n",
      "Epoch 2119/2500\n",
      "11/11 - 0s - loss: 0.3833 - val_loss: 0.4900 - 31ms/epoch - 3ms/step\n",
      "Epoch 2120/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4863 - 35ms/epoch - 3ms/step\n",
      "Epoch 2121/2500\n",
      "11/11 - 0s - loss: 0.3701 - val_loss: 0.4879 - 44ms/epoch - 4ms/step\n",
      "Epoch 2122/2500\n",
      "11/11 - 0s - loss: 0.3788 - val_loss: 0.4859 - 36ms/epoch - 3ms/step\n",
      "Epoch 2123/2500\n",
      "11/11 - 0s - loss: 0.3931 - val_loss: 0.4865 - 32ms/epoch - 3ms/step\n",
      "Epoch 2124/2500\n",
      "11/11 - 0s - loss: 0.3707 - val_loss: 0.4810 - 35ms/epoch - 3ms/step\n",
      "Epoch 2125/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4839 - 31ms/epoch - 3ms/step\n",
      "Epoch 2126/2500\n",
      "11/11 - 0s - loss: 0.3820 - val_loss: 0.4925 - 32ms/epoch - 3ms/step\n",
      "Epoch 2127/2500\n",
      "11/11 - 0s - loss: 0.3872 - val_loss: 0.4787 - 31ms/epoch - 3ms/step\n",
      "Epoch 2128/2500\n",
      "11/11 - 0s - loss: 0.3969 - val_loss: 0.4880 - 31ms/epoch - 3ms/step\n",
      "Epoch 2129/2500\n",
      "11/11 - 0s - loss: 0.3741 - val_loss: 0.4948 - 31ms/epoch - 3ms/step\n",
      "Epoch 2130/2500\n",
      "11/11 - 0s - loss: 0.3830 - val_loss: 0.4963 - 35ms/epoch - 3ms/step\n",
      "Epoch 2131/2500\n",
      "11/11 - 0s - loss: 0.3669 - val_loss: 0.4895 - 34ms/epoch - 3ms/step\n",
      "Epoch 2132/2500\n",
      "11/11 - 0s - loss: 0.3723 - val_loss: 0.4864 - 32ms/epoch - 3ms/step\n",
      "Epoch 2133/2500\n",
      "11/11 - 0s - loss: 0.3707 - val_loss: 0.4949 - 34ms/epoch - 3ms/step\n",
      "Epoch 2134/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4820 - 32ms/epoch - 3ms/step\n",
      "Epoch 2135/2500\n",
      "11/11 - 0s - loss: 0.3826 - val_loss: 0.4997 - 31ms/epoch - 3ms/step\n",
      "Epoch 2136/2500\n",
      "11/11 - 0s - loss: 0.3901 - val_loss: 0.4845 - 36ms/epoch - 3ms/step\n",
      "Epoch 2137/2500\n",
      "11/11 - 0s - loss: 0.3639 - val_loss: 0.4785 - 37ms/epoch - 3ms/step\n",
      "Epoch 2138/2500\n",
      "11/11 - 0s - loss: 0.3786 - val_loss: 0.4796 - 37ms/epoch - 3ms/step\n",
      "Epoch 2139/2500\n",
      "11/11 - 0s - loss: 0.3921 - val_loss: 0.4748 - 41ms/epoch - 4ms/step\n",
      "Epoch 2140/2500\n",
      "11/11 - 0s - loss: 0.3724 - val_loss: 0.4729 - 40ms/epoch - 4ms/step\n",
      "Epoch 2141/2500\n",
      "11/11 - 0s - loss: 0.3771 - val_loss: 0.4782 - 36ms/epoch - 3ms/step\n",
      "Epoch 2142/2500\n",
      "11/11 - 0s - loss: 0.3807 - val_loss: 0.4854 - 38ms/epoch - 3ms/step\n",
      "Epoch 2143/2500\n",
      "11/11 - 0s - loss: 0.3821 - val_loss: 0.4778 - 35ms/epoch - 3ms/step\n",
      "Epoch 2144/2500\n",
      "11/11 - 0s - loss: 0.3800 - val_loss: 0.4828 - 35ms/epoch - 3ms/step\n",
      "Epoch 2145/2500\n",
      "11/11 - 0s - loss: 0.3818 - val_loss: 0.4925 - 33ms/epoch - 3ms/step\n",
      "Epoch 2146/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4861 - 33ms/epoch - 3ms/step\n",
      "Epoch 2147/2500\n",
      "11/11 - 0s - loss: 0.3810 - val_loss: 0.4814 - 33ms/epoch - 3ms/step\n",
      "Epoch 2148/2500\n",
      "11/11 - 0s - loss: 0.3916 - val_loss: 0.4799 - 33ms/epoch - 3ms/step\n",
      "Epoch 2149/2500\n",
      "11/11 - 0s - loss: 0.3818 - val_loss: 0.4831 - 35ms/epoch - 3ms/step\n",
      "Epoch 2150/2500\n",
      "11/11 - 0s - loss: 0.3766 - val_loss: 0.4786 - 32ms/epoch - 3ms/step\n",
      "Epoch 2151/2500\n",
      "11/11 - 0s - loss: 0.3731 - val_loss: 0.4851 - 32ms/epoch - 3ms/step\n",
      "Epoch 2152/2500\n",
      "11/11 - 0s - loss: 0.3961 - val_loss: 0.4856 - 33ms/epoch - 3ms/step\n",
      "Epoch 2153/2500\n",
      "11/11 - 0s - loss: 0.3586 - val_loss: 0.4727 - 32ms/epoch - 3ms/step\n",
      "Epoch 2154/2500\n",
      "11/11 - 0s - loss: 0.3742 - val_loss: 0.4886 - 32ms/epoch - 3ms/step\n",
      "Epoch 2155/2500\n",
      "11/11 - 0s - loss: 0.3796 - val_loss: 0.4941 - 47ms/epoch - 4ms/step\n",
      "Epoch 2156/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4798 - 32ms/epoch - 3ms/step\n",
      "Epoch 2157/2500\n",
      "11/11 - 0s - loss: 0.3775 - val_loss: 0.4717 - 31ms/epoch - 3ms/step\n",
      "Epoch 2158/2500\n",
      "11/11 - 0s - loss: 0.3685 - val_loss: 0.4804 - 32ms/epoch - 3ms/step\n",
      "Epoch 2159/2500\n",
      "11/11 - 0s - loss: 0.3733 - val_loss: 0.4784 - 35ms/epoch - 3ms/step\n",
      "Epoch 2160/2500\n",
      "11/11 - 0s - loss: 0.3812 - val_loss: 0.4772 - 35ms/epoch - 3ms/step\n",
      "Epoch 2161/2500\n",
      "11/11 - 0s - loss: 0.3736 - val_loss: 0.4823 - 34ms/epoch - 3ms/step\n",
      "Epoch 2162/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4973 - 32ms/epoch - 3ms/step\n",
      "Epoch 2163/2500\n",
      "11/11 - 0s - loss: 0.3848 - val_loss: 0.4962 - 33ms/epoch - 3ms/step\n",
      "Epoch 2164/2500\n",
      "11/11 - 0s - loss: 0.3693 - val_loss: 0.4688 - 33ms/epoch - 3ms/step\n",
      "Epoch 2165/2500\n",
      "11/11 - 0s - loss: 0.3778 - val_loss: 0.4772 - 33ms/epoch - 3ms/step\n",
      "Epoch 2166/2500\n",
      "11/11 - 0s - loss: 0.3725 - val_loss: 0.4877 - 31ms/epoch - 3ms/step\n",
      "Epoch 2167/2500\n",
      "11/11 - 0s - loss: 0.3831 - val_loss: 0.4849 - 33ms/epoch - 3ms/step\n",
      "Epoch 2168/2500\n",
      "11/11 - 0s - loss: 0.3772 - val_loss: 0.4783 - 32ms/epoch - 3ms/step\n",
      "Epoch 2169/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4845 - 33ms/epoch - 3ms/step\n",
      "Epoch 2170/2500\n",
      "11/11 - 0s - loss: 0.3870 - val_loss: 0.4836 - 32ms/epoch - 3ms/step\n",
      "Epoch 2171/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4742 - 31ms/epoch - 3ms/step\n",
      "Epoch 2172/2500\n",
      "11/11 - 0s - loss: 0.3738 - val_loss: 0.4754 - 44ms/epoch - 4ms/step\n",
      "Epoch 2173/2500\n",
      "11/11 - 0s - loss: 0.3767 - val_loss: 0.4904 - 32ms/epoch - 3ms/step\n",
      "Epoch 2174/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4744 - 33ms/epoch - 3ms/step\n",
      "Epoch 2175/2500\n",
      "11/11 - 0s - loss: 0.3821 - val_loss: 0.4894 - 31ms/epoch - 3ms/step\n",
      "Epoch 2176/2500\n",
      "11/11 - 0s - loss: 0.3718 - val_loss: 0.4785 - 32ms/epoch - 3ms/step\n",
      "Epoch 2177/2500\n",
      "11/11 - 0s - loss: 0.3627 - val_loss: 0.4944 - 32ms/epoch - 3ms/step\n",
      "Epoch 2178/2500\n",
      "11/11 - 0s - loss: 0.3763 - val_loss: 0.4831 - 33ms/epoch - 3ms/step\n",
      "Epoch 2179/2500\n",
      "11/11 - 0s - loss: 0.3777 - val_loss: 0.4832 - 35ms/epoch - 3ms/step\n",
      "Epoch 2180/2500\n",
      "11/11 - 0s - loss: 0.3689 - val_loss: 0.5054 - 32ms/epoch - 3ms/step\n",
      "Epoch 2181/2500\n",
      "11/11 - 0s - loss: 0.3898 - val_loss: 0.4908 - 35ms/epoch - 3ms/step\n",
      "Epoch 2182/2500\n",
      "11/11 - 0s - loss: 0.3735 - val_loss: 0.4841 - 32ms/epoch - 3ms/step\n",
      "Epoch 2183/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4809 - 31ms/epoch - 3ms/step\n",
      "Epoch 2184/2500\n",
      "11/11 - 0s - loss: 0.3785 - val_loss: 0.4966 - 31ms/epoch - 3ms/step\n",
      "Epoch 2185/2500\n",
      "11/11 - 0s - loss: 0.3799 - val_loss: 0.4725 - 32ms/epoch - 3ms/step\n",
      "Epoch 2186/2500\n",
      "11/11 - 0s - loss: 0.3661 - val_loss: 0.5114 - 31ms/epoch - 3ms/step\n",
      "Epoch 2187/2500\n",
      "11/11 - 0s - loss: 0.3744 - val_loss: 0.4768 - 35ms/epoch - 3ms/step\n",
      "Epoch 2188/2500\n",
      "11/11 - 0s - loss: 0.3865 - val_loss: 0.4734 - 31ms/epoch - 3ms/step\n",
      "Epoch 2189/2500\n",
      "11/11 - 0s - loss: 0.3773 - val_loss: 0.4813 - 33ms/epoch - 3ms/step\n",
      "Epoch 2190/2500\n",
      "11/11 - 0s - loss: 0.3741 - val_loss: 0.4869 - 44ms/epoch - 4ms/step\n",
      "Epoch 2191/2500\n",
      "11/11 - 0s - loss: 0.3671 - val_loss: 0.4768 - 32ms/epoch - 3ms/step\n",
      "Epoch 2192/2500\n",
      "11/11 - 0s - loss: 0.3620 - val_loss: 0.4854 - 34ms/epoch - 3ms/step\n",
      "Epoch 2193/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4922 - 35ms/epoch - 3ms/step\n",
      "Epoch 2194/2500\n",
      "11/11 - 0s - loss: 0.3721 - val_loss: 0.4863 - 31ms/epoch - 3ms/step\n",
      "Epoch 2195/2500\n",
      "11/11 - 0s - loss: 0.3787 - val_loss: 0.4804 - 33ms/epoch - 3ms/step\n",
      "Epoch 2196/2500\n",
      "11/11 - 0s - loss: 0.3780 - val_loss: 0.4832 - 30ms/epoch - 3ms/step\n",
      "Epoch 2197/2500\n",
      "11/11 - 0s - loss: 0.3602 - val_loss: 0.4853 - 33ms/epoch - 3ms/step\n",
      "Epoch 2198/2500\n",
      "11/11 - 0s - loss: 0.3805 - val_loss: 0.4991 - 32ms/epoch - 3ms/step\n",
      "Epoch 2199/2500\n",
      "11/11 - 0s - loss: 0.3671 - val_loss: 0.4721 - 35ms/epoch - 3ms/step\n",
      "Epoch 2200/2500\n",
      "11/11 - 0s - loss: 0.3784 - val_loss: 0.4799 - 32ms/epoch - 3ms/step\n",
      "Epoch 2201/2500\n",
      "11/11 - 0s - loss: 0.3773 - val_loss: 0.4840 - 32ms/epoch - 3ms/step\n",
      "Epoch 2202/2500\n",
      "11/11 - 0s - loss: 0.3680 - val_loss: 0.4865 - 32ms/epoch - 3ms/step\n",
      "Epoch 2203/2500\n",
      "11/11 - 0s - loss: 0.3689 - val_loss: 0.4863 - 32ms/epoch - 3ms/step\n",
      "Epoch 2204/2500\n",
      "11/11 - 0s - loss: 0.3888 - val_loss: 0.4816 - 31ms/epoch - 3ms/step\n",
      "Epoch 2205/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4880 - 34ms/epoch - 3ms/step\n",
      "Epoch 2206/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4845 - 32ms/epoch - 3ms/step\n",
      "Epoch 2207/2500\n",
      "11/11 - 0s - loss: 0.3767 - val_loss: 0.4807 - 31ms/epoch - 3ms/step\n",
      "Epoch 2208/2500\n",
      "11/11 - 0s - loss: 0.3766 - val_loss: 0.4836 - 44ms/epoch - 4ms/step\n",
      "Epoch 2209/2500\n",
      "11/11 - 0s - loss: 0.3761 - val_loss: 0.4878 - 32ms/epoch - 3ms/step\n",
      "Epoch 2210/2500\n",
      "11/11 - 0s - loss: 0.3799 - val_loss: 0.4776 - 32ms/epoch - 3ms/step\n",
      "Epoch 2211/2500\n",
      "11/11 - 0s - loss: 0.3656 - val_loss: 0.5008 - 35ms/epoch - 3ms/step\n",
      "Epoch 2212/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4905 - 32ms/epoch - 3ms/step\n",
      "Epoch 2213/2500\n",
      "11/11 - 0s - loss: 0.3714 - val_loss: 0.4965 - 31ms/epoch - 3ms/step\n",
      "Epoch 2214/2500\n",
      "11/11 - 0s - loss: 0.3779 - val_loss: 0.5041 - 32ms/epoch - 3ms/step\n",
      "Epoch 2215/2500\n",
      "11/11 - 0s - loss: 0.3769 - val_loss: 0.4861 - 35ms/epoch - 3ms/step\n",
      "Epoch 2216/2500\n",
      "11/11 - 0s - loss: 0.3724 - val_loss: 0.4793 - 32ms/epoch - 3ms/step\n",
      "Epoch 2217/2500\n",
      "11/11 - 0s - loss: 0.3679 - val_loss: 0.4979 - 33ms/epoch - 3ms/step\n",
      "Epoch 2218/2500\n",
      "11/11 - 0s - loss: 0.3924 - val_loss: 0.4787 - 33ms/epoch - 3ms/step\n",
      "Epoch 2219/2500\n",
      "11/11 - 0s - loss: 0.3643 - val_loss: 0.4952 - 31ms/epoch - 3ms/step\n",
      "Epoch 2220/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4814 - 31ms/epoch - 3ms/step\n",
      "Epoch 2221/2500\n",
      "11/11 - 0s - loss: 0.3754 - val_loss: 0.4808 - 32ms/epoch - 3ms/step\n",
      "Epoch 2222/2500\n",
      "11/11 - 0s - loss: 0.3749 - val_loss: 0.4851 - 32ms/epoch - 3ms/step\n",
      "Epoch 2223/2500\n",
      "11/11 - 0s - loss: 0.3767 - val_loss: 0.4761 - 33ms/epoch - 3ms/step\n",
      "Epoch 2224/2500\n",
      "11/11 - 0s - loss: 0.3944 - val_loss: 0.4810 - 37ms/epoch - 3ms/step\n",
      "Epoch 2225/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4993 - 39ms/epoch - 4ms/step\n",
      "Epoch 2226/2500\n",
      "11/11 - 0s - loss: 0.3804 - val_loss: 0.4859 - 31ms/epoch - 3ms/step\n",
      "Epoch 2227/2500\n",
      "11/11 - 0s - loss: 0.3859 - val_loss: 0.4869 - 33ms/epoch - 3ms/step\n",
      "Epoch 2228/2500\n",
      "11/11 - 0s - loss: 0.3666 - val_loss: 0.4880 - 32ms/epoch - 3ms/step\n",
      "Epoch 2229/2500\n",
      "11/11 - 0s - loss: 0.3789 - val_loss: 0.4879 - 34ms/epoch - 3ms/step\n",
      "Epoch 2230/2500\n",
      "11/11 - 0s - loss: 0.3744 - val_loss: 0.4894 - 33ms/epoch - 3ms/step\n",
      "Epoch 2231/2500\n",
      "11/11 - 0s - loss: 0.3703 - val_loss: 0.4829 - 31ms/epoch - 3ms/step\n",
      "Epoch 2232/2500\n",
      "11/11 - 0s - loss: 0.3649 - val_loss: 0.4910 - 31ms/epoch - 3ms/step\n",
      "Epoch 2233/2500\n",
      "11/11 - 0s - loss: 0.3800 - val_loss: 0.4886 - 33ms/epoch - 3ms/step\n",
      "Epoch 2234/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4909 - 33ms/epoch - 3ms/step\n",
      "Epoch 2235/2500\n",
      "11/11 - 0s - loss: 0.3774 - val_loss: 0.4881 - 35ms/epoch - 3ms/step\n",
      "Epoch 2236/2500\n",
      "11/11 - 0s - loss: 0.3786 - val_loss: 0.4734 - 32ms/epoch - 3ms/step\n",
      "Epoch 2237/2500\n",
      "11/11 - 0s - loss: 0.3726 - val_loss: 0.5020 - 32ms/epoch - 3ms/step\n",
      "Epoch 2238/2500\n",
      "11/11 - 0s - loss: 0.3911 - val_loss: 0.4896 - 32ms/epoch - 3ms/step\n",
      "Epoch 2239/2500\n",
      "11/11 - 0s - loss: 0.3849 - val_loss: 0.4813 - 31ms/epoch - 3ms/step\n",
      "Epoch 2240/2500\n",
      "11/11 - 0s - loss: 0.3754 - val_loss: 0.4893 - 32ms/epoch - 3ms/step\n",
      "Epoch 2241/2500\n",
      "11/11 - 0s - loss: 0.3756 - val_loss: 0.4886 - 35ms/epoch - 3ms/step\n",
      "Epoch 2242/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4879 - 31ms/epoch - 3ms/step\n",
      "Epoch 2243/2500\n",
      "11/11 - 0s - loss: 0.3601 - val_loss: 0.4874 - 31ms/epoch - 3ms/step\n",
      "Epoch 2244/2500\n",
      "11/11 - 0s - loss: 0.3693 - val_loss: 0.4820 - 32ms/epoch - 3ms/step\n",
      "Epoch 2245/2500\n",
      "11/11 - 0s - loss: 0.3921 - val_loss: 0.4892 - 31ms/epoch - 3ms/step\n",
      "Epoch 2246/2500\n",
      "11/11 - 0s - loss: 0.3699 - val_loss: 0.4871 - 32ms/epoch - 3ms/step\n",
      "Epoch 2247/2500\n",
      "11/11 - 0s - loss: 0.3829 - val_loss: 0.4753 - 39ms/epoch - 4ms/step\n",
      "Epoch 2248/2500\n",
      "11/11 - 0s - loss: 0.3866 - val_loss: 0.4929 - 43ms/epoch - 4ms/step\n",
      "Epoch 2249/2500\n",
      "11/11 - 0s - loss: 0.3852 - val_loss: 0.4817 - 31ms/epoch - 3ms/step\n",
      "Epoch 2250/2500\n",
      "11/11 - 0s - loss: 0.3639 - val_loss: 0.4794 - 32ms/epoch - 3ms/step\n",
      "Epoch 2251/2500\n",
      "11/11 - 0s - loss: 0.3794 - val_loss: 0.4843 - 36ms/epoch - 3ms/step\n",
      "Epoch 2252/2500\n",
      "11/11 - 0s - loss: 0.3753 - val_loss: 0.5044 - 36ms/epoch - 3ms/step\n",
      "Epoch 2253/2500\n",
      "11/11 - 0s - loss: 0.3813 - val_loss: 0.4824 - 32ms/epoch - 3ms/step\n",
      "Epoch 2254/2500\n",
      "11/11 - 0s - loss: 0.3787 - val_loss: 0.4883 - 31ms/epoch - 3ms/step\n",
      "Epoch 2255/2500\n",
      "11/11 - 0s - loss: 0.3729 - val_loss: 0.4865 - 32ms/epoch - 3ms/step\n",
      "Epoch 2256/2500\n",
      "11/11 - 0s - loss: 0.3758 - val_loss: 0.4932 - 32ms/epoch - 3ms/step\n",
      "Epoch 2257/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4821 - 32ms/epoch - 3ms/step\n",
      "Epoch 2258/2500\n",
      "11/11 - 0s - loss: 0.3718 - val_loss: 0.4915 - 33ms/epoch - 3ms/step\n",
      "Epoch 2259/2500\n",
      "11/11 - 0s - loss: 0.3669 - val_loss: 0.4876 - 32ms/epoch - 3ms/step\n",
      "Epoch 2260/2500\n",
      "11/11 - 0s - loss: 0.3727 - val_loss: 0.4786 - 32ms/epoch - 3ms/step\n",
      "Epoch 2261/2500\n",
      "11/11 - 0s - loss: 0.3755 - val_loss: 0.4861 - 31ms/epoch - 3ms/step\n",
      "Epoch 2262/2500\n",
      "11/11 - 0s - loss: 0.3770 - val_loss: 0.4874 - 31ms/epoch - 3ms/step\n",
      "Epoch 2263/2500\n",
      "11/11 - 0s - loss: 0.3767 - val_loss: 0.4824 - 33ms/epoch - 3ms/step\n",
      "Epoch 2264/2500\n",
      "11/11 - 0s - loss: 0.3706 - val_loss: 0.4807 - 32ms/epoch - 3ms/step\n",
      "Epoch 2265/2500\n",
      "11/11 - 0s - loss: 0.3577 - val_loss: 0.4874 - 31ms/epoch - 3ms/step\n",
      "Epoch 2266/2500\n",
      "11/11 - 0s - loss: 0.3916 - val_loss: 0.4880 - 32ms/epoch - 3ms/step\n",
      "Epoch 2267/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4856 - 33ms/epoch - 3ms/step\n",
      "Epoch 2268/2500\n",
      "11/11 - 0s - loss: 0.3694 - val_loss: 0.4930 - 32ms/epoch - 3ms/step\n",
      "Epoch 2269/2500\n",
      "11/11 - 0s - loss: 0.3727 - val_loss: 0.4827 - 56ms/epoch - 5ms/step\n",
      "Epoch 2270/2500\n",
      "11/11 - 0s - loss: 0.3606 - val_loss: 0.4952 - 33ms/epoch - 3ms/step\n",
      "Epoch 2271/2500\n",
      "11/11 - 0s - loss: 0.3771 - val_loss: 0.4788 - 33ms/epoch - 3ms/step\n",
      "Epoch 2272/2500\n",
      "11/11 - 0s - loss: 0.3787 - val_loss: 0.4904 - 31ms/epoch - 3ms/step\n",
      "Epoch 2273/2500\n",
      "11/11 - 0s - loss: 0.3737 - val_loss: 0.4923 - 31ms/epoch - 3ms/step\n",
      "Epoch 2274/2500\n",
      "11/11 - 0s - loss: 0.3769 - val_loss: 0.4885 - 31ms/epoch - 3ms/step\n",
      "Epoch 2275/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4920 - 33ms/epoch - 3ms/step\n",
      "Epoch 2276/2500\n",
      "11/11 - 0s - loss: 0.3816 - val_loss: 0.4843 - 32ms/epoch - 3ms/step\n",
      "Epoch 2277/2500\n",
      "11/11 - 0s - loss: 0.3739 - val_loss: 0.4899 - 31ms/epoch - 3ms/step\n",
      "Epoch 2278/2500\n",
      "11/11 - 0s - loss: 0.3658 - val_loss: 0.4853 - 31ms/epoch - 3ms/step\n",
      "Epoch 2279/2500\n",
      "11/11 - 0s - loss: 0.3684 - val_loss: 0.4902 - 31ms/epoch - 3ms/step\n",
      "Epoch 2280/2500\n",
      "11/11 - 0s - loss: 0.3680 - val_loss: 0.4884 - 32ms/epoch - 3ms/step\n",
      "Epoch 2281/2500\n",
      "11/11 - 0s - loss: 0.3733 - val_loss: 0.4762 - 35ms/epoch - 3ms/step\n",
      "Epoch 2282/2500\n",
      "11/11 - 0s - loss: 0.3727 - val_loss: 0.4773 - 32ms/epoch - 3ms/step\n",
      "Epoch 2283/2500\n",
      "11/11 - 0s - loss: 0.3829 - val_loss: 0.4865 - 40ms/epoch - 4ms/step\n",
      "Epoch 2284/2500\n",
      "11/11 - 0s - loss: 0.3815 - val_loss: 0.4893 - 35ms/epoch - 3ms/step\n",
      "Epoch 2285/2500\n",
      "11/11 - 0s - loss: 0.3621 - val_loss: 0.4823 - 34ms/epoch - 3ms/step\n",
      "Epoch 2286/2500\n",
      "11/11 - 0s - loss: 0.3888 - val_loss: 0.5095 - 37ms/epoch - 3ms/step\n",
      "Epoch 2287/2500\n",
      "11/11 - 0s - loss: 0.3724 - val_loss: 0.4924 - 34ms/epoch - 3ms/step\n",
      "Epoch 2288/2500\n",
      "11/11 - 0s - loss: 0.3775 - val_loss: 0.4875 - 34ms/epoch - 3ms/step\n",
      "Epoch 2289/2500\n",
      "11/11 - 0s - loss: 0.3776 - val_loss: 0.4886 - 31ms/epoch - 3ms/step\n",
      "Epoch 2290/2500\n",
      "11/11 - 0s - loss: 0.3879 - val_loss: 0.5021 - 32ms/epoch - 3ms/step\n",
      "Epoch 2291/2500\n",
      "11/11 - 0s - loss: 0.3795 - val_loss: 0.4925 - 49ms/epoch - 4ms/step\n",
      "Epoch 2292/2500\n",
      "11/11 - 0s - loss: 0.3746 - val_loss: 0.4854 - 31ms/epoch - 3ms/step\n",
      "Epoch 2293/2500\n",
      "11/11 - 0s - loss: 0.3857 - val_loss: 0.4910 - 32ms/epoch - 3ms/step\n",
      "Epoch 2294/2500\n",
      "11/11 - 0s - loss: 0.3674 - val_loss: 0.4800 - 32ms/epoch - 3ms/step\n",
      "Epoch 2295/2500\n",
      "11/11 - 0s - loss: 0.3659 - val_loss: 0.4854 - 32ms/epoch - 3ms/step\n",
      "Epoch 2296/2500\n",
      "11/11 - 0s - loss: 0.3891 - val_loss: 0.4705 - 31ms/epoch - 3ms/step\n",
      "Epoch 2297/2500\n",
      "11/11 - 0s - loss: 0.3811 - val_loss: 0.4897 - 34ms/epoch - 3ms/step\n",
      "Epoch 2298/2500\n",
      "11/11 - 0s - loss: 0.3624 - val_loss: 0.4865 - 32ms/epoch - 3ms/step\n",
      "Epoch 2299/2500\n",
      "11/11 - 0s - loss: 0.3717 - val_loss: 0.4998 - 31ms/epoch - 3ms/step\n",
      "Epoch 2300/2500\n",
      "11/11 - 0s - loss: 0.3636 - val_loss: 0.4802 - 32ms/epoch - 3ms/step\n",
      "Epoch 2301/2500\n",
      "11/11 - 0s - loss: 0.3740 - val_loss: 0.4853 - 32ms/epoch - 3ms/step\n",
      "Epoch 2302/2500\n",
      "11/11 - 0s - loss: 0.3646 - val_loss: 0.4829 - 35ms/epoch - 3ms/step\n",
      "Epoch 2303/2500\n",
      "11/11 - 0s - loss: 0.3672 - val_loss: 0.4902 - 31ms/epoch - 3ms/step\n",
      "Epoch 2304/2500\n",
      "11/11 - 0s - loss: 0.3631 - val_loss: 0.4937 - 33ms/epoch - 3ms/step\n",
      "Epoch 2305/2500\n",
      "11/11 - 0s - loss: 0.3912 - val_loss: 0.4910 - 49ms/epoch - 4ms/step\n",
      "Epoch 2306/2500\n",
      "11/11 - 0s - loss: 0.3693 - val_loss: 0.4839 - 33ms/epoch - 3ms/step\n",
      "Epoch 2307/2500\n",
      "11/11 - 0s - loss: 0.3683 - val_loss: 0.4847 - 32ms/epoch - 3ms/step\n",
      "Epoch 2308/2500\n",
      "11/11 - 0s - loss: 0.3748 - val_loss: 0.4833 - 37ms/epoch - 3ms/step\n",
      "Epoch 2309/2500\n",
      "11/11 - 0s - loss: 0.3668 - val_loss: 0.4807 - 32ms/epoch - 3ms/step\n",
      "Epoch 2310/2500\n",
      "11/11 - 0s - loss: 0.3847 - val_loss: 0.4847 - 32ms/epoch - 3ms/step\n",
      "Epoch 2311/2500\n",
      "11/11 - 0s - loss: 0.3751 - val_loss: 0.4875 - 32ms/epoch - 3ms/step\n",
      "Epoch 2312/2500\n",
      "11/11 - 0s - loss: 0.3668 - val_loss: 0.5018 - 31ms/epoch - 3ms/step\n",
      "Epoch 2313/2500\n",
      "11/11 - 0s - loss: 0.3953 - val_loss: 0.5016 - 33ms/epoch - 3ms/step\n",
      "Epoch 2314/2500\n",
      "11/11 - 0s - loss: 0.3729 - val_loss: 0.4935 - 33ms/epoch - 3ms/step\n",
      "Epoch 2315/2500\n",
      "11/11 - 0s - loss: 0.3791 - val_loss: 0.4842 - 31ms/epoch - 3ms/step\n",
      "Epoch 2316/2500\n",
      "11/11 - 0s - loss: 0.3742 - val_loss: 0.4949 - 32ms/epoch - 3ms/step\n",
      "Epoch 2317/2500\n",
      "11/11 - 0s - loss: 0.3684 - val_loss: 0.4833 - 31ms/epoch - 3ms/step\n",
      "Epoch 2318/2500\n",
      "11/11 - 0s - loss: 0.3887 - val_loss: 0.4695 - 31ms/epoch - 3ms/step\n",
      "Epoch 2319/2500\n",
      "11/11 - 0s - loss: 0.3743 - val_loss: 0.4926 - 40ms/epoch - 4ms/step\n",
      "Epoch 2320/2500\n",
      "11/11 - 0s - loss: 0.3760 - val_loss: 0.4758 - 39ms/epoch - 4ms/step\n",
      "Epoch 2321/2500\n",
      "11/11 - 0s - loss: 0.3734 - val_loss: 0.4940 - 32ms/epoch - 3ms/step\n",
      "Epoch 2322/2500\n",
      "11/11 - 0s - loss: 0.3690 - val_loss: 0.5027 - 33ms/epoch - 3ms/step\n",
      "Epoch 2323/2500\n",
      "11/11 - 0s - loss: 0.3792 - val_loss: 0.4826 - 37ms/epoch - 3ms/step\n",
      "Epoch 2324/2500\n",
      "11/11 - 0s - loss: 0.3882 - val_loss: 0.4754 - 32ms/epoch - 3ms/step\n",
      "Epoch 2325/2500\n",
      "11/11 - 0s - loss: 0.3730 - val_loss: 0.4970 - 34ms/epoch - 3ms/step\n",
      "Epoch 2326/2500\n",
      "11/11 - 0s - loss: 0.3683 - val_loss: 0.4928 - 33ms/epoch - 3ms/step\n",
      "Epoch 2327/2500\n",
      "11/11 - 0s - loss: 0.3771 - val_loss: 0.4725 - 32ms/epoch - 3ms/step\n",
      "Epoch 2328/2500\n",
      "11/11 - 0s - loss: 0.3779 - val_loss: 0.4766 - 32ms/epoch - 3ms/step\n",
      "Epoch 2329/2500\n",
      "11/11 - 0s - loss: 0.3772 - val_loss: 0.4861 - 32ms/epoch - 3ms/step\n",
      "Epoch 2330/2500\n",
      "11/11 - 0s - loss: 0.3767 - val_loss: 0.4871 - 31ms/epoch - 3ms/step\n",
      "Epoch 2331/2500\n",
      "11/11 - 0s - loss: 0.3639 - val_loss: 0.4948 - 33ms/epoch - 3ms/step\n",
      "Epoch 2332/2500\n",
      "11/11 - 0s - loss: 0.3806 - val_loss: 0.4787 - 31ms/epoch - 3ms/step\n",
      "Epoch 2333/2500\n",
      "11/11 - 0s - loss: 0.3769 - val_loss: 0.4810 - 32ms/epoch - 3ms/step\n",
      "Epoch 2334/2500\n",
      "11/11 - 0s - loss: 0.3656 - val_loss: 0.4745 - 31ms/epoch - 3ms/step\n",
      "Epoch 2335/2500\n",
      "11/11 - 0s - loss: 0.3712 - val_loss: 0.4889 - 32ms/epoch - 3ms/step\n",
      "Epoch 2336/2500\n",
      "11/11 - 0s - loss: 0.3729 - val_loss: 0.4826 - 44ms/epoch - 4ms/step\n",
      "Epoch 2337/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4985 - 33ms/epoch - 3ms/step\n",
      "Epoch 2338/2500\n",
      "11/11 - 0s - loss: 0.3859 - val_loss: 0.4743 - 32ms/epoch - 3ms/step\n",
      "Epoch 2339/2500\n",
      "11/11 - 0s - loss: 0.3744 - val_loss: 0.4745 - 33ms/epoch - 3ms/step\n",
      "Epoch 2340/2500\n",
      "11/11 - 0s - loss: 0.3729 - val_loss: 0.4836 - 32ms/epoch - 3ms/step\n",
      "Epoch 2341/2500\n",
      "11/11 - 0s - loss: 0.3854 - val_loss: 0.4917 - 35ms/epoch - 3ms/step\n",
      "Epoch 2342/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4759 - 32ms/epoch - 3ms/step\n",
      "Epoch 2343/2500\n",
      "11/11 - 0s - loss: 0.3719 - val_loss: 0.4872 - 32ms/epoch - 3ms/step\n",
      "Epoch 2344/2500\n",
      "11/11 - 0s - loss: 0.3830 - val_loss: 0.4843 - 35ms/epoch - 3ms/step\n",
      "Epoch 2345/2500\n",
      "11/11 - 0s - loss: 0.3712 - val_loss: 0.4773 - 34ms/epoch - 3ms/step\n",
      "Epoch 2346/2500\n",
      "11/11 - 0s - loss: 0.3709 - val_loss: 0.4909 - 32ms/epoch - 3ms/step\n",
      "Epoch 2347/2500\n",
      "11/11 - 0s - loss: 0.3653 - val_loss: 0.4790 - 31ms/epoch - 3ms/step\n",
      "Epoch 2348/2500\n",
      "11/11 - 0s - loss: 0.3716 - val_loss: 0.4853 - 31ms/epoch - 3ms/step\n",
      "Epoch 2349/2500\n",
      "11/11 - 0s - loss: 0.3803 - val_loss: 0.4870 - 34ms/epoch - 3ms/step\n",
      "Epoch 2350/2500\n",
      "11/11 - 0s - loss: 0.3694 - val_loss: 0.4752 - 40ms/epoch - 4ms/step\n",
      "Epoch 2351/2500\n",
      "11/11 - 0s - loss: 0.3736 - val_loss: 0.4747 - 34ms/epoch - 3ms/step\n",
      "Epoch 2352/2500\n",
      "11/11 - 0s - loss: 0.3851 - val_loss: 0.4811 - 32ms/epoch - 3ms/step\n",
      "Epoch 2353/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4875 - 33ms/epoch - 3ms/step\n",
      "Epoch 2354/2500\n",
      "11/11 - 0s - loss: 0.3736 - val_loss: 0.4807 - 33ms/epoch - 3ms/step\n",
      "Epoch 2355/2500\n",
      "11/11 - 0s - loss: 0.3788 - val_loss: 0.4737 - 32ms/epoch - 3ms/step\n",
      "Epoch 2356/2500\n",
      "11/11 - 0s - loss: 0.3667 - val_loss: 0.4783 - 32ms/epoch - 3ms/step\n",
      "Epoch 2357/2500\n",
      "11/11 - 0s - loss: 0.3641 - val_loss: 0.4875 - 31ms/epoch - 3ms/step\n",
      "Epoch 2358/2500\n",
      "11/11 - 0s - loss: 0.3652 - val_loss: 0.4826 - 33ms/epoch - 3ms/step\n",
      "Epoch 2359/2500\n",
      "11/11 - 0s - loss: 0.3753 - val_loss: 0.4771 - 36ms/epoch - 3ms/step\n",
      "Epoch 2360/2500\n",
      "11/11 - 0s - loss: 0.3730 - val_loss: 0.4825 - 36ms/epoch - 3ms/step\n",
      "Epoch 2361/2500\n",
      "11/11 - 0s - loss: 0.3894 - val_loss: 0.4906 - 33ms/epoch - 3ms/step\n",
      "Epoch 2362/2500\n",
      "11/11 - 0s - loss: 0.3601 - val_loss: 0.4847 - 31ms/epoch - 3ms/step\n",
      "Epoch 2363/2500\n",
      "11/11 - 0s - loss: 0.3866 - val_loss: 0.4789 - 33ms/epoch - 3ms/step\n",
      "Epoch 2364/2500\n",
      "11/11 - 0s - loss: 0.3709 - val_loss: 0.4758 - 35ms/epoch - 3ms/step\n",
      "Epoch 2365/2500\n",
      "11/11 - 0s - loss: 0.3765 - val_loss: 0.4775 - 33ms/epoch - 3ms/step\n",
      "Epoch 2366/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.5060 - 31ms/epoch - 3ms/step\n",
      "Epoch 2367/2500\n",
      "11/11 - 0s - loss: 0.3764 - val_loss: 0.4779 - 32ms/epoch - 3ms/step\n",
      "Epoch 2368/2500\n",
      "11/11 - 0s - loss: 0.3815 - val_loss: 0.4791 - 31ms/epoch - 3ms/step\n",
      "Epoch 2369/2500\n",
      "11/11 - 0s - loss: 0.3760 - val_loss: 0.4896 - 30ms/epoch - 3ms/step\n",
      "Epoch 2370/2500\n",
      "11/11 - 0s - loss: 0.3728 - val_loss: 0.4970 - 33ms/epoch - 3ms/step\n",
      "Epoch 2371/2500\n",
      "11/11 - 0s - loss: 0.3706 - val_loss: 0.4826 - 44ms/epoch - 4ms/step\n",
      "Epoch 2372/2500\n",
      "11/11 - 0s - loss: 0.3628 - val_loss: 0.4841 - 34ms/epoch - 3ms/step\n",
      "Epoch 2373/2500\n",
      "11/11 - 0s - loss: 0.3669 - val_loss: 0.4927 - 32ms/epoch - 3ms/step\n",
      "Epoch 2374/2500\n",
      "11/11 - 0s - loss: 0.3737 - val_loss: 0.4830 - 31ms/epoch - 3ms/step\n",
      "Epoch 2375/2500\n",
      "11/11 - 0s - loss: 0.3814 - val_loss: 0.4772 - 32ms/epoch - 3ms/step\n",
      "Epoch 2376/2500\n",
      "11/11 - 0s - loss: 0.3860 - val_loss: 0.5117 - 38ms/epoch - 3ms/step\n",
      "Epoch 2377/2500\n",
      "11/11 - 0s - loss: 0.3609 - val_loss: 0.4833 - 32ms/epoch - 3ms/step\n",
      "Epoch 2378/2500\n",
      "11/11 - 0s - loss: 0.3620 - val_loss: 0.4803 - 33ms/epoch - 3ms/step\n",
      "Epoch 2379/2500\n",
      "11/11 - 0s - loss: 0.3587 - val_loss: 0.4937 - 31ms/epoch - 3ms/step\n",
      "Epoch 2380/2500\n",
      "11/11 - 0s - loss: 0.3818 - val_loss: 0.4834 - 31ms/epoch - 3ms/step\n",
      "Epoch 2381/2500\n",
      "11/11 - 0s - loss: 0.3635 - val_loss: 0.4764 - 32ms/epoch - 3ms/step\n",
      "Epoch 2382/2500\n",
      "11/11 - 0s - loss: 0.3874 - val_loss: 0.4827 - 34ms/epoch - 3ms/step\n",
      "Epoch 2383/2500\n",
      "11/11 - 0s - loss: 0.3943 - val_loss: 0.4847 - 31ms/epoch - 3ms/step\n",
      "Epoch 2384/2500\n",
      "11/11 - 0s - loss: 0.3817 - val_loss: 0.4697 - 32ms/epoch - 3ms/step\n",
      "Epoch 2385/2500\n",
      "11/11 - 0s - loss: 0.3711 - val_loss: 0.4837 - 32ms/epoch - 3ms/step\n",
      "Epoch 2386/2500\n",
      "11/11 - 0s - loss: 0.3711 - val_loss: 0.4855 - 31ms/epoch - 3ms/step\n",
      "Epoch 2387/2500\n",
      "11/11 - 0s - loss: 0.3860 - val_loss: 0.4794 - 32ms/epoch - 3ms/step\n",
      "Epoch 2388/2500\n",
      "11/11 - 0s - loss: 0.3743 - val_loss: 0.4888 - 34ms/epoch - 3ms/step\n",
      "Epoch 2389/2500\n",
      "11/11 - 0s - loss: 0.3823 - val_loss: 0.4855 - 32ms/epoch - 3ms/step\n",
      "Epoch 2390/2500\n",
      "11/11 - 0s - loss: 0.3714 - val_loss: 0.4741 - 32ms/epoch - 3ms/step\n",
      "Epoch 2391/2500\n",
      "11/11 - 0s - loss: 0.3591 - val_loss: 0.4771 - 32ms/epoch - 3ms/step\n",
      "Epoch 2392/2500\n",
      "11/11 - 0s - loss: 0.3704 - val_loss: 0.4892 - 32ms/epoch - 3ms/step\n",
      "Epoch 2393/2500\n",
      "11/11 - 0s - loss: 0.3799 - val_loss: 0.4903 - 52ms/epoch - 5ms/step\n",
      "Epoch 2394/2500\n",
      "11/11 - 0s - loss: 0.3741 - val_loss: 0.4849 - 36ms/epoch - 3ms/step\n",
      "Epoch 2395/2500\n",
      "11/11 - 0s - loss: 0.3865 - val_loss: 0.4836 - 32ms/epoch - 3ms/step\n",
      "Epoch 2396/2500\n",
      "11/11 - 0s - loss: 0.3709 - val_loss: 0.4866 - 33ms/epoch - 3ms/step\n",
      "Epoch 2397/2500\n",
      "11/11 - 0s - loss: 0.3780 - val_loss: 0.4886 - 31ms/epoch - 3ms/step\n",
      "Epoch 2398/2500\n",
      "11/11 - 0s - loss: 0.3629 - val_loss: 0.4805 - 32ms/epoch - 3ms/step\n",
      "Epoch 2399/2500\n",
      "11/11 - 0s - loss: 0.3672 - val_loss: 0.4810 - 31ms/epoch - 3ms/step\n",
      "Epoch 2400/2500\n",
      "11/11 - 0s - loss: 0.3789 - val_loss: 0.4781 - 34ms/epoch - 3ms/step\n",
      "Epoch 2401/2500\n",
      "11/11 - 0s - loss: 0.3743 - val_loss: 0.4757 - 31ms/epoch - 3ms/step\n",
      "Epoch 2402/2500\n",
      "11/11 - 0s - loss: 0.3704 - val_loss: 0.4812 - 32ms/epoch - 3ms/step\n",
      "Epoch 2403/2500\n",
      "11/11 - 0s - loss: 0.3805 - val_loss: 0.4799 - 31ms/epoch - 3ms/step\n",
      "Epoch 2404/2500\n",
      "11/11 - 0s - loss: 0.3741 - val_loss: 0.4774 - 31ms/epoch - 3ms/step\n",
      "Epoch 2405/2500\n",
      "11/11 - 0s - loss: 0.3636 - val_loss: 0.4871 - 31ms/epoch - 3ms/step\n",
      "Epoch 2406/2500\n",
      "11/11 - 0s - loss: 0.3638 - val_loss: 0.4931 - 34ms/epoch - 3ms/step\n",
      "Epoch 2407/2500\n",
      "11/11 - 0s - loss: 0.3690 - val_loss: 0.4827 - 31ms/epoch - 3ms/step\n",
      "Epoch 2408/2500\n",
      "11/11 - 0s - loss: 0.3788 - val_loss: 0.4921 - 31ms/epoch - 3ms/step\n",
      "Epoch 2409/2500\n",
      "11/11 - 0s - loss: 0.3686 - val_loss: 0.4864 - 31ms/epoch - 3ms/step\n",
      "Epoch 2410/2500\n",
      "11/11 - 0s - loss: 0.3645 - val_loss: 0.4773 - 32ms/epoch - 3ms/step\n",
      "Epoch 2411/2500\n",
      "11/11 - 0s - loss: 0.3721 - val_loss: 0.4863 - 36ms/epoch - 3ms/step\n",
      "Epoch 2412/2500\n",
      "11/11 - 0s - loss: 0.3736 - val_loss: 0.4903 - 52ms/epoch - 5ms/step\n",
      "Epoch 2413/2500\n",
      "11/11 - 0s - loss: 0.3621 - val_loss: 0.4868 - 32ms/epoch - 3ms/step\n",
      "Epoch 2414/2500\n",
      "11/11 - 0s - loss: 0.3730 - val_loss: 0.4808 - 33ms/epoch - 3ms/step\n",
      "Epoch 2415/2500\n",
      "11/11 - 0s - loss: 0.3696 - val_loss: 0.4772 - 32ms/epoch - 3ms/step\n",
      "Epoch 2416/2500\n",
      "11/11 - 0s - loss: 0.3768 - val_loss: 0.4868 - 31ms/epoch - 3ms/step\n",
      "Epoch 2417/2500\n",
      "11/11 - 0s - loss: 0.3801 - val_loss: 0.4832 - 32ms/epoch - 3ms/step\n",
      "Epoch 2418/2500\n",
      "11/11 - 0s - loss: 0.3781 - val_loss: 0.4814 - 33ms/epoch - 3ms/step\n",
      "Epoch 2419/2500\n",
      "11/11 - 0s - loss: 0.3739 - val_loss: 0.4856 - 34ms/epoch - 3ms/step\n",
      "Epoch 2420/2500\n",
      "11/11 - 0s - loss: 0.3698 - val_loss: 0.4917 - 31ms/epoch - 3ms/step\n",
      "Epoch 2421/2500\n",
      "11/11 - 0s - loss: 0.3803 - val_loss: 0.4855 - 32ms/epoch - 3ms/step\n",
      "Epoch 2422/2500\n",
      "11/11 - 0s - loss: 0.3820 - val_loss: 0.4940 - 32ms/epoch - 3ms/step\n",
      "Epoch 2423/2500\n",
      "11/11 - 0s - loss: 0.3678 - val_loss: 0.5004 - 34ms/epoch - 3ms/step\n",
      "Epoch 2424/2500\n",
      "11/11 - 0s - loss: 0.3692 - val_loss: 0.5015 - 31ms/epoch - 3ms/step\n",
      "Epoch 2425/2500\n",
      "11/11 - 0s - loss: 0.3845 - val_loss: 0.4936 - 31ms/epoch - 3ms/step\n",
      "Epoch 2426/2500\n",
      "11/11 - 0s - loss: 0.3712 - val_loss: 0.4885 - 31ms/epoch - 3ms/step\n",
      "Epoch 2427/2500\n",
      "11/11 - 0s - loss: 0.3748 - val_loss: 0.5039 - 31ms/epoch - 3ms/step\n",
      "Epoch 2428/2500\n",
      "11/11 - 0s - loss: 0.3678 - val_loss: 0.5000 - 37ms/epoch - 3ms/step\n",
      "Epoch 2429/2500\n",
      "11/11 - 0s - loss: 0.3775 - val_loss: 0.4940 - 49ms/epoch - 4ms/step\n",
      "Epoch 2430/2500\n",
      "11/11 - 0s - loss: 0.3555 - val_loss: 0.4813 - 32ms/epoch - 3ms/step\n",
      "Epoch 2431/2500\n",
      "11/11 - 0s - loss: 0.3726 - val_loss: 0.4911 - 32ms/epoch - 3ms/step\n",
      "Epoch 2432/2500\n",
      "11/11 - 0s - loss: 0.3652 - val_loss: 0.4859 - 32ms/epoch - 3ms/step\n",
      "Epoch 2433/2500\n",
      "11/11 - 0s - loss: 0.3740 - val_loss: 0.4889 - 33ms/epoch - 3ms/step\n",
      "Epoch 2434/2500\n",
      "11/11 - 0s - loss: 0.3727 - val_loss: 0.4865 - 35ms/epoch - 3ms/step\n",
      "Epoch 2435/2500\n",
      "11/11 - 0s - loss: 0.3639 - val_loss: 0.4864 - 32ms/epoch - 3ms/step\n",
      "Epoch 2436/2500\n",
      "11/11 - 0s - loss: 0.3802 - val_loss: 0.4824 - 32ms/epoch - 3ms/step\n",
      "Epoch 2437/2500\n",
      "11/11 - 0s - loss: 0.3705 - val_loss: 0.4917 - 31ms/epoch - 3ms/step\n",
      "Epoch 2438/2500\n",
      "11/11 - 0s - loss: 0.3613 - val_loss: 0.4787 - 31ms/epoch - 3ms/step\n",
      "Epoch 2439/2500\n",
      "11/11 - 0s - loss: 0.3732 - val_loss: 0.4858 - 32ms/epoch - 3ms/step\n",
      "Epoch 2440/2500\n",
      "11/11 - 0s - loss: 0.3704 - val_loss: 0.4957 - 35ms/epoch - 3ms/step\n",
      "Epoch 2441/2500\n",
      "11/11 - 0s - loss: 0.3602 - val_loss: 0.4997 - 31ms/epoch - 3ms/step\n",
      "Epoch 2442/2500\n",
      "11/11 - 0s - loss: 0.3661 - val_loss: 0.4910 - 32ms/epoch - 3ms/step\n",
      "Epoch 2443/2500\n",
      "11/11 - 0s - loss: 0.3823 - val_loss: 0.4910 - 31ms/epoch - 3ms/step\n",
      "Epoch 2444/2500\n",
      "11/11 - 0s - loss: 0.3718 - val_loss: 0.4809 - 31ms/epoch - 3ms/step\n",
      "Epoch 2445/2500\n",
      "11/11 - 0s - loss: 0.3713 - val_loss: 0.4889 - 36ms/epoch - 3ms/step\n",
      "Epoch 2446/2500\n",
      "11/11 - 0s - loss: 0.3680 - val_loss: 0.5021 - 33ms/epoch - 3ms/step\n",
      "Epoch 2447/2500\n",
      "11/11 - 0s - loss: 0.3724 - val_loss: 0.4844 - 31ms/epoch - 3ms/step\n",
      "Epoch 2448/2500\n",
      "11/11 - 0s - loss: 0.3682 - val_loss: 0.4840 - 33ms/epoch - 3ms/step\n",
      "Epoch 2449/2500\n",
      "11/11 - 0s - loss: 0.3752 - val_loss: 0.4964 - 33ms/epoch - 3ms/step\n",
      "Epoch 2450/2500\n",
      "11/11 - 0s - loss: 0.3675 - val_loss: 0.4893 - 43ms/epoch - 4ms/step\n",
      "Epoch 2451/2500\n",
      "11/11 - 0s - loss: 0.3753 - val_loss: 0.4932 - 35ms/epoch - 3ms/step\n",
      "Epoch 2452/2500\n",
      "11/11 - 0s - loss: 0.3669 - val_loss: 0.4920 - 33ms/epoch - 3ms/step\n",
      "Epoch 2453/2500\n",
      "11/11 - 0s - loss: 0.3634 - val_loss: 0.4863 - 32ms/epoch - 3ms/step\n",
      "Epoch 2454/2500\n",
      "11/11 - 0s - loss: 0.3613 - val_loss: 0.4930 - 33ms/epoch - 3ms/step\n",
      "Epoch 2455/2500\n",
      "11/11 - 0s - loss: 0.3662 - val_loss: 0.5014 - 32ms/epoch - 3ms/step\n",
      "Epoch 2456/2500\n",
      "11/11 - 0s - loss: 0.3859 - val_loss: 0.4812 - 31ms/epoch - 3ms/step\n",
      "Epoch 2457/2500\n",
      "11/11 - 0s - loss: 0.3776 - val_loss: 0.4841 - 34ms/epoch - 3ms/step\n",
      "Epoch 2458/2500\n",
      "11/11 - 0s - loss: 0.3701 - val_loss: 0.4939 - 32ms/epoch - 3ms/step\n",
      "Epoch 2459/2500\n",
      "11/11 - 0s - loss: 0.3665 - val_loss: 0.4977 - 32ms/epoch - 3ms/step\n",
      "Epoch 2460/2500\n",
      "11/11 - 0s - loss: 0.3549 - val_loss: 0.4805 - 32ms/epoch - 3ms/step\n",
      "Epoch 2461/2500\n",
      "11/11 - 0s - loss: 0.3607 - val_loss: 0.4868 - 32ms/epoch - 3ms/step\n",
      "Epoch 2462/2500\n",
      "11/11 - 0s - loss: 0.3619 - val_loss: 0.4970 - 34ms/epoch - 3ms/step\n",
      "Epoch 2463/2500\n",
      "11/11 - 0s - loss: 0.3700 - val_loss: 0.4769 - 39ms/epoch - 4ms/step\n",
      "Epoch 2464/2500\n",
      "11/11 - 0s - loss: 0.3619 - val_loss: 0.4852 - 32ms/epoch - 3ms/step\n",
      "Epoch 2465/2500\n",
      "11/11 - 0s - loss: 0.3775 - val_loss: 0.4903 - 33ms/epoch - 3ms/step\n",
      "Epoch 2466/2500\n",
      "11/11 - 0s - loss: 0.3784 - val_loss: 0.4754 - 32ms/epoch - 3ms/step\n",
      "Epoch 2467/2500\n",
      "11/11 - 0s - loss: 0.3673 - val_loss: 0.4830 - 32ms/epoch - 3ms/step\n",
      "Epoch 2468/2500\n",
      "11/11 - 0s - loss: 0.3717 - val_loss: 0.4805 - 32ms/epoch - 3ms/step\n",
      "Epoch 2469/2500\n",
      "11/11 - 0s - loss: 0.3872 - val_loss: 0.4926 - 34ms/epoch - 3ms/step\n",
      "Epoch 2470/2500\n",
      "11/11 - 0s - loss: 0.3735 - val_loss: 0.4772 - 31ms/epoch - 3ms/step\n",
      "Epoch 2471/2500\n",
      "11/11 - 0s - loss: 0.3658 - val_loss: 0.4751 - 46ms/epoch - 4ms/step\n",
      "Epoch 2472/2500\n",
      "11/11 - 0s - loss: 0.3732 - val_loss: 0.4926 - 34ms/epoch - 3ms/step\n",
      "Epoch 2473/2500\n",
      "11/11 - 0s - loss: 0.3577 - val_loss: 0.4788 - 32ms/epoch - 3ms/step\n",
      "Epoch 2474/2500\n",
      "11/11 - 0s - loss: 0.3729 - val_loss: 0.4791 - 33ms/epoch - 3ms/step\n",
      "Epoch 2475/2500\n",
      "11/11 - 0s - loss: 0.3594 - val_loss: 0.4902 - 31ms/epoch - 3ms/step\n",
      "Epoch 2476/2500\n",
      "11/11 - 0s - loss: 0.3784 - val_loss: 0.4938 - 30ms/epoch - 3ms/step\n",
      "Epoch 2477/2500\n",
      "11/11 - 0s - loss: 0.3683 - val_loss: 0.4849 - 32ms/epoch - 3ms/step\n",
      "Epoch 2478/2500\n",
      "11/11 - 0s - loss: 0.3546 - val_loss: 0.4905 - 32ms/epoch - 3ms/step\n",
      "Epoch 2479/2500\n",
      "11/11 - 0s - loss: 0.3827 - val_loss: 0.4870 - 33ms/epoch - 3ms/step\n",
      "Epoch 2480/2500\n",
      "11/11 - 0s - loss: 0.3641 - val_loss: 0.4847 - 42ms/epoch - 4ms/step\n",
      "Epoch 2481/2500\n",
      "11/11 - 0s - loss: 0.3881 - val_loss: 0.4858 - 34ms/epoch - 3ms/step\n",
      "Epoch 2482/2500\n",
      "11/11 - 0s - loss: 0.3772 - val_loss: 0.4802 - 32ms/epoch - 3ms/step\n",
      "Epoch 2483/2500\n",
      "11/11 - 0s - loss: 0.3588 - val_loss: 0.4875 - 35ms/epoch - 3ms/step\n",
      "Epoch 2484/2500\n",
      "11/11 - 0s - loss: 0.3723 - val_loss: 0.4869 - 32ms/epoch - 3ms/step\n",
      "Epoch 2485/2500\n",
      "11/11 - 0s - loss: 0.3682 - val_loss: 0.4882 - 33ms/epoch - 3ms/step\n",
      "Epoch 2486/2500\n",
      "11/11 - 0s - loss: 0.3674 - val_loss: 0.4885 - 48ms/epoch - 4ms/step\n",
      "Epoch 2487/2500\n",
      "11/11 - 0s - loss: 0.3686 - val_loss: 0.4928 - 31ms/epoch - 3ms/step\n",
      "Epoch 2488/2500\n",
      "11/11 - 0s - loss: 0.3692 - val_loss: 0.5096 - 32ms/epoch - 3ms/step\n",
      "Epoch 2489/2500\n",
      "11/11 - 0s - loss: 0.3659 - val_loss: 0.4932 - 31ms/epoch - 3ms/step\n",
      "Epoch 2490/2500\n",
      "11/11 - 0s - loss: 0.3646 - val_loss: 0.4907 - 32ms/epoch - 3ms/step\n",
      "Epoch 2491/2500\n",
      "11/11 - 0s - loss: 0.3811 - val_loss: 0.4899 - 32ms/epoch - 3ms/step\n",
      "Epoch 2492/2500\n",
      "11/11 - 0s - loss: 0.3693 - val_loss: 0.4924 - 34ms/epoch - 3ms/step\n",
      "Epoch 2493/2500\n",
      "11/11 - 0s - loss: 0.3635 - val_loss: 0.4998 - 31ms/epoch - 3ms/step\n",
      "Epoch 2494/2500\n",
      "11/11 - 0s - loss: 0.3699 - val_loss: 0.4850 - 31ms/epoch - 3ms/step\n",
      "Epoch 2495/2500\n",
      "11/11 - 0s - loss: 0.3699 - val_loss: 0.4802 - 32ms/epoch - 3ms/step\n",
      "Epoch 2496/2500\n",
      "11/11 - 0s - loss: 0.3623 - val_loss: 0.4925 - 34ms/epoch - 3ms/step\n",
      "Epoch 2497/2500\n",
      "11/11 - 0s - loss: 0.3837 - val_loss: 0.4822 - 35ms/epoch - 3ms/step\n",
      "Epoch 2498/2500\n",
      "11/11 - 0s - loss: 0.3815 - val_loss: 0.5108 - 35ms/epoch - 3ms/step\n",
      "Epoch 2499/2500\n",
      "11/11 - 0s - loss: 0.3744 - val_loss: 0.5006 - 32ms/epoch - 3ms/step\n",
      "Epoch 2500/2500\n",
      "11/11 - 0s - loss: 0.3733 - val_loss: 0.4957 - 32ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAESCAYAAAAVLtXjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4dklEQVR4nO3deUBU5f7H8ffMADPs7rggKi6BW5pmWq5o7gu5xFUvttjNtl9ZN03NSktzazdNzbzdrFtZaZlZ5m5WbpULiBsJijsowgCzn98fA4PEItAMy8z39Q/MOXPOeZ4Z+MwzzznneVSKoigIIYTwKOrKLoAQQoiKJ+EvhBAeSMJfCCE8kIS/EEJ4IAl/IYTwQBL+QgjhgST8RZWSkpJCx44dK+XYb7/9Nl9//bXT9mc0GnnrrbeIjo5mxIgRDBs2jBUrViBXV4uqwKuyCyBEVfHUU085bV+KovDYY4/RrFkzPv/8c7RaLdeuXWPSpElkZ2czefJkpx1LiPKQ8BfVhslk4rXXXmP//v1YrVZat27NzJkzCQgIYPv27SxfvhyTycTVq1eJjo5m8uTJ7N27l7lz5+Ln50d2djZTpkxhyZIlNG7cmJMnT2IymXjxxRfp2rUr06ZNo2XLlkycOJF27drx8MMP8/PPP3P58mUmTJjA/fffj9VqZeHChWzbto3AwEDat29PYmIiq1evLlDW/fv38+eff7JixQo0Gg0ANWvWZOHChZw7dw6A2NhYxo8fz8CBAws9btu2LX379uXYsWOMHj2aAwcOsHz5cgASExO5//772bFjB0lJScydO5f09HSsViuxsbGMHj2arKwspk+fTnJyMmq1mjZt2vDyyy+jVsuXfWEn4S+qjbwgXbt2LSqVijfeeIPXXnuNl156iVWrVjF//nyaNm3KpUuX6NOnDxMmTADg5MmTbNmyhUaNGrF3714OHz7MSy+9RGRkJKtWreLdd9+la9euBY5lMpmoWbMmn332GXFxcYwdO5axY8eybt064uPj2bBhAyqVikcffbTIssbFxdG+fXtH8Odp2rQpTZs2vWldzWYzffr04e2330av17N8+XKuXLlC3bp1Wbt2LSNHjkRRFJ588kkWLlxImzZtyMzMJCYmhhYtWpCUlERWVhbffPMNVquVl156ibNnz9KkSZPyvfjC7Uj4i2pjx44dZGZm8ssvvwD2gKxduzYqlYply5axY8cONmzYQGJiIoqikJOTA0CDBg1o1KiRYz8NGzYkMjISgNatW7Nu3boij9e3b18A2rRpg8lkIjs7m507dzJixAi0Wi0AMTExhVr9AGq1+m/37Xfu3BmAgIAABgwYwPr167n//vtZv349//vf/0hKSuLMmTPMmDHDsY3BYODo0aP06NGDN998k9jYWO68807uu+8+CX5RgIS/qDZsNhszZsygV69eAGRlZWE0GsnOzuaee+6hX79+dO7cmVGjRrFlyxZH+Pr5+RXYj06nc/yuUqmKDem8gFepVIC9H9/Lq+C/THHdKLfeeiv//e9/sVqtBVr/hw8fZvXq1SxatMixzzxms7nAPm4s95gxY3jhhRdo3rw5LVq0oHHjxhw/fpygoCC++eYbx/NSU1MJDAxEq9WyefNm9u7dy549e3jggQeYOXOmo4tJCOkAFNVG9+7d+eSTTzCZTNhsNl544QXeeOMNkpOT0ev1TJ48maioKPbt2+d4jrP16tWL9evXYzKZsFgsxX5r6NixI+Hh4cybNw+j0QjYg3nOnDmEhoYCUKtWLeLi4gA4c+YMx48fL/a4HTp0AGDJkiWMGTMGgGbNmqHVah3hf+HCBYYOHUpcXBz/+9//mD59Ot27d2fKlCl0796dkydPOuU1EO5BWv6iysnOzi50uednn33GY489xoIFC7jnnnuwWq1ERkYybdo0/Pz86N27N4MGDSIoKIiwsDBatGhBcnIyPj4+Ti3byJEjOX36NNHR0fj5+REaGoqvr2+Rz33nnXd48803GTlyJBqNBpvNRnR0NBMnTgTg0UcfZdq0aezcuZPw8HBHN09xxowZw9KlS+nXrx8APj4+LF26lLlz57Jy5UosFgtPPfUUnTp1IjIykn379jF48GB8fX1p2LCh4xyIEAAqGdJZiNLbvXs3aWlpjBgxAoA5c+ag1WqZMmVKJZdMiLKR8BeiDC5dusS0adNIS0vDarUSERHBrFmzCAwMrOyiCVEmEv5CCOGB5ISvEEJ4IAl/IYTwQNXiap+DBw86rrkuK6PRWO5tqyups2eQOnuGv1Nno9HouEz4r6pF+Gu1WscdmWWVkJBQ7m2rK6mzZ5A6e4a/U+eEhIRi10m3jxBCeCAJfyGE8EAS/kII4YGqRZ+/EMKzmc1mUlJSMJvNJfZju6PS1Fmn0xEaGoq3t3ep9yvhL4So8lJSUggMDCQkJKTQKK3uLicnp9jxo8A+MmxaWhopKSk0a9as1PuVbh8hRJVnMBgcczeIglQqFbVr18ZgMJRpOwl/IUS1IMFfvPK8Nm4d/ml6Iz8nZ1V2MYQQospx6/Bf98c55u64RI7JWtlFEUJUY0ajkS+++KJUz127di1bt251cYn+PrcOf0UBBbDKwKVCiL/hypUrpQ7/kSNHOuZ/rsrc+mqfvG4wq03CXwh38dVvKaw5cNap+7y3c2NGdQotdv2yZcs4deoUERER3HnnnWRnZzN37ly+/vpr4uLiSE9PJyIignnz5rF48WLq1KlDeHg477//Pt7e3qSkpDB48GAeffRRp5b773BJ+FutVmbOnMnp06dRqVTMnj2bVq1aOdZv27aNJUuW4OXlxahRo7j33ntdUQw06vyJt4UQorweeeQRTpw4QY8ePbh+/TozZ85Er9cTFBTEf/7zH2w2G0OGDOHSpUsFtjt//rxjzucePXq4f/hv374dsM+7unfvXt58803ee+89wH7Dwrx58/jyyy/x9fVl7NixREVFUadOHaeXQ53b9JeGvxDuY1Sn0BJb6a6Wdy29Vqvl6tWrPPPMM/j5+ZGdnY3ZbC7w3FatWuHl5YWXlxc6na4yilssl4R/v3796N27N2D/5AsKCnKsS0xMJCwsjODgYAA6derE/v37GTRoULH7MxqN5bqr7/Kl6wAcO36cmr5u3cNVgMFg8Li7IKXO7s1sNpOTk4OiKOTk5FT48U0mExaLBbPZjNVqJScnhx07dpCSksLChQu5evUqmzdvxmAwYDabMZvNGI1GbDabo7zlLXtptyvr3c8uS0QvLy+ee+45Nm/ezDvvvONYrtfrC8x36u/vj16vL3Ff5R3S+feMZCCNFi1aUi+oan3qupIMe+sZPKnOCQkJ+Pr63vRuV1dp1KgRVqsVq9WKt7c3vr6+dO7cmZUrV/LQQw+hUqlo3Lgx169fx9vbG29vb7RaLRqNxlFelUpVrrKXts7e3t6F/h5K+jBwaXN4wYIFPPvss9x777189913+Pn5ERAQQFZW/rX3WVlZLpv8Wrp9hBDOoNVq+eabbwosq1u3Ll999VWh53bq1Mnx+x133OH4/eeff3ZdAcvBJZd6fv311yxfvhwAX19fVCoVarX9UM2bNyc5OZn09HRMJhMHDhygY8eOrigGued7sckJXyGEKMAlLf/+/fszffp0xo8fj8ViYcaMGWzevJns7GxiYmKYNm0aEydORFEURo0aRUhIiCuK4bjlWS71FEKIglwS/n5+frz99tvFro+KiiIqKsoVhy5Ao8q71NPlhxJCiGrFre/wze1pkm4fIYT4C/cOf8cJXwl/IYS4kVuHv0rCXwghiuTW4a+RSz2FEKJIbh3+cqmnEKIixcbGkpiYWOz6qKgojEZjBZaoeG495oGj28dWyQURQjjPwU/hj4+du8+O/4QOY527zyrOrcNfWv5CCGd44oknmDBhAl26dOHIkSMsXLiQWrVqkZmZyeXLlxk3bhzjxo0r9f5SUlKYMWMGVqsVlUrFzJkziYiIYPr06SQnJ2MwGJgwYQLR0dEsXryY33//HYvFQv/+/Xn44YedUie3Dv+8IZ0l/IVwIx3GVngrfcyYMaxbt44uXbqwdu1a7rjjDlq1akX//v25dOkSsbGxZQr/hQsXMmHCBPr160dCQgIzZszgo48+Yv/+/axZswbIHw5i48aNfPzxx9SrV4+1a9c6rU5uHf4yto8Qwhl69OjBokWLSE9P58CBA6xcuZLXX3+dH3/8kYCAACwWS5n2l5iYyO233w5AZGQkFy9eJCAggBkzZvDCCy+g1+sZPnw4AK+++iqvv/46qamp9OjRw2l1cuvwl5m8hBDOoFarGThwILNmzaJfv36sWrWKDh06MG7cOPbs2cPOnTvLtL/mzZtz4MAB+vbtS0JCAnXq1OHy5cvEx8ezZMkSjEYjvXr1YtiwYWzevJk33ngDgMGDBzNkyBAaNWr0t+vk1uGvVslMXkII5xg1ahT9+vVj06ZNpKSkMGfOHDZu3EhgYCAajQaTyVTqfU2dOpUXXniBVatWYbFYmDt3LnXr1uXKlSv84x//QK1W8+CDD+Lj40NwcDD33nsvOp2Ou+66i4YNGzqlPiqlGiRjecct//lUKuNX7mXUbaG8fu+tLihZ1eRJ47znkTq7t7y6VtZ4/pWptHUu6u+hpL8Rt27553X7fPV7ikeFvxCi8hw+fJhFixYVWj5o0KAynRR2NbcO/7xuHyFE9VcNOikAaN++PatXr67QY5bntXHrO3yFEO5Bp9ORlpZWbT4AKpKiKKSlpZV5gni3bvnL9f1CuIfQ0FBSUlI4f/483t7elV2cCmU2m29aZ51OR2hoaJn269bhj2S/EG7B29ubZs2aedRJ7jyuqrNbd/t4ady6ekIIUW5unY5eGjnhK4QQRXHr8PeRlr8QQhTJrdOxdYMgAHq2qlvJJRFCiKrFrcNfrVYRrFPTuKZn3REohBA349bhD6BGJRf9CCHEX7h9+KOqPncGCiFERXH78FcDkv1CCFGQ24e/SiV3+gohxF+5d/gf28gXtqfBVrZZdoQQwt25d/inJ9OM8/hYsyu7JEIIUaU4fWwfs9nMjBkzOHfuHCaTiUcffZS+ffs61n/44Yd88cUX1KpVC4DZs2cTHh7u7GLYedlHufO25rhm/0IIUU05PfzXr19PjRo1HJMdR0dHFwj/uLg4FixYQNu2bZ196MK87df3eymln15NCCE8gdPDf+DAgQwYMACwX2Kp0WgKrI+Pj2fFihVcuXKF3r17M2nSJGcXIV9u+P987KzrjiGEENWQ08Pf398fAL1ez5NPPsnkyZMLrB8yZAjjxo0jICCAJ554gu3bt9OnT58S92k0GklISCh7WS6kEgb42Ezl2r66MhgMHlVfkDp7Cqmz87hkPP8LFy7w+OOPM27cOIYNG+ZYrigK9913H4GBgQD06tWLo0eP3jT8tVpt+caz1l2Bn8BfY/aoMcBlzHPPIHX2DH+nziV9aDj9ap/U1FQefPBBpkyZwujRowus0+v1DB06lKysLBRFYe/eva7t+/eyd/s0DXLvi5qEEKKsnN7yX7ZsGRkZGSxdupSlS5cCMGbMGHJycoiJieHpp59mwoQJ+Pj40K1bN3r16uXsIuTztl/t42UzuO4YQghRDTk9/GfOnMnMmTOLXR8dHU10dLSzD1s0bz8A1FYJfyGEuJF794d45bX8jZVcECGEqFrcO/xzL/X0lvAXQogC3Dv8c1v+NlMOmQZzJRdGCCGqDo8If53KxMyv4yq5MEIIUXW4d/ir1RjxQYeJVL10/QghRB73Dn/AlBv+apWqsosihBBVhtuHv1FlD38hhBD53D78TfigU0n4CyHEjdw+/O19/nKljxBC3Mj9w1+lxRcjKunzF0IIB7cPf4va3u3jo5HwF0KIPG4f/g1qBqDFxB3Nald2UYQQospw+/BXe+ukz18IIf7C7cNf0WjRYcKmKJVdFCGEqDI8I/xVJmyS/UII4eD24W/z0uGLUVr+QghxA7cPf0WtRYcZRcJfCCEc3D788dKiVZlRbLbKLokQQlQZbh/+ikYLgNoqQzwIIUQetw9/W+6Y/mprTiWXRAghqg63D39yW/4amcRdCCEc3D78bRL+QghRiNuHv+KV1+cv4S+EEHncPvxtGnufv8Yq0zgKIUQetw//vKt9NDYJfyGEyOP24S99/kIIUZjbh39ey39n/JlKLokQQlQdbh/+eS1/HSbSs+VGLyGEABeEv9lsZsqUKYwbN47Ro0ezdevWAuu3bdvGqFGjiImJYc2aNc4+fCFK7k1eOpWJ/UnXXH48IYSoDrycvcP169dTo0YNFi1aRHp6OtHR0fTt2xewfzDMmzePL7/8El9fX8aOHUtUVBR16tRxdjEcbmz5W2VcZyGEAFzQ8h84cCBPPfUUAIqioNFoHOsSExMJCwsjODgYHx8fOnXqxP79+51dhAIUR/ibUcs0vkIIAbig5e/v7w+AXq/nySefZPLkyY51er2ewMDAAs/V6/U33afRaCQhIaFc5TGYbNgUFTqVkXPnUkhQu3/Xj8FgKPfrVV1JnT2D1Nl5nB7+ABcuXODxxx9n3LhxDBs2zLE8ICCArKwsx+OsrKwCHwbF0Wq1REZGlqssCQkJGPFGh4nGoY2JjAwp136qk4SEhHK/XtWV1NkzSJ3Lvm1xnN7tk5qayoMPPsiUKVMYPXp0gXXNmzcnOTmZ9PR0TCYTBw4coGPHjs4uQiE5+Nj7/GVCFyGEAFzQ8l+2bBkZGRksXbqUpUuXAjBmzBhycnKIiYlh2rRpTJw4EUVRGDVqFCEhrm+JG/BBhxmLVcJfCCHABeE/c+ZMZs6cWez6qKgooqKinH3YEuUo9kncLTKblxBCAB5wkxeAt9YPX4xyqacQQuTyiPAPqVMTHSYsEv5CCAF4SPjj7Yuvyih9/kIIkctDwt8PX0xYpc9fCCEADwl/lbcvvhil20cIIXJ5TPjrVDK2jxBC5PGQ8Ldf7WOWPn8hhAA8Jfy1vvhiwmKVPn8hhIByhr/JVL0mRVF7++KrMvHfX05XdlGEEKJKKDH8bxyRc9WqVY7fH3roIZcVyBVU3r4AZGRlV3JJhBCiaigx/NPS0hy/79ixw/G7Ut0GSMubzYvq9Y1FCCFcpdTdPjcGvkpVzWZF8bJP6KLFXMkFEUKIqqHE8L8x5Ktd4N8ot+Vf17eafWMRQggXKXFUz1OnTvHvf/8bRVEK/J6YmFhR5XOO3PCv71eNP8CEEMKJSgz/t956y/H7P/7xjyJ/rxZyu33UVkMlF0QIIaqGEsO/S5cuBR7r9XrWrVvHp59+ysaNG11aMKfysl/to7bJCV8hhIBSnvA9deoUs2bNol+/fpw8eZL58+e7ulzOldvyV1mMlVwQIYSoGkps+W/atIlPPvkEs9nMyJEjOX36NC+//HJFlc15cvv81TYJfyGEgJu0/J977jk6dOjA0qVLGTNmDD4+PhVVLufKbfl7WSX8hRACStHyX7duHePHj6dVq1Zcu3atosrlXLktf40i4S+EEHCTln9ISAiPPPIIGzduJCYmhsaNGxMVFcWCBQsqqnzOkdvy91ZM2GRYZyGEKLnlP3369AKPdTodrVq14osvvuC5555zacGcKrflr8XM1WwTdQK0lVwgIYSoXCWGf1xcHAaDgeHDh9OxY0fHEA/jx4+vkMI5jXd++E9a/RtfPXpnJRdICCEqV4ndPt9++y1LlizBaDSyYsUKDh48SFhYGD169Kio8jnHDS3/g2fTK7csQghRBZTY8gdo1aoVzz77LAD79+/n9ddf5+LFi6xZs8blhXMajf0qJa3KjFVm8xJCiJuHP9jv7N28eTMbNmwgJyeH4cOHu7pczqVSYVFrZVRPIYTIVWL4b9y4kY0bN3L+/Hn69+/P7NmzCQ0NraiyOZVV7YNWxvMXQgjgJuH/zDPPEB4eTkREBCdOnODNN990rHv99dddXjhnsqq1Ev5CCJGrxPD/6KOPKqocLmfT+KBVSbePEEJAGUf1LItDhw7x2muvsXr16gLLP/zwQ7744gtq1aoFwOzZswkPDy/3cUrLKn3+QgjhUKoTvmX1/vvvs379enx9fQuti4uLY8GCBbRt29YVhy6WTaOT8BdCiFwuCf+wsDAWL17M1KlTC62Lj49nxYoVXLlyhd69ezNp0qSb7s9oNJKQkFCushgMBhISEqhhVTnCv7z7qi7y6uxJpM6eQersPC4J/wEDBpCSklLkuiFDhjBu3DgCAgJ44okn2L59O3369Clxf1qtlsjIyHKVJSEhgcjISK7vCEKbmQ5Q7n1VF3l19iRSZ88gdS77tsUp1WQuzqIoCvfddx+1atXCx8eHXr16cfTo0Qo5dnBgIDq52kcIIYAKDn+9Xs/QoUPJyspCURT27t1bcX3/XvknfC1WW8UcUwghqiiXdPv81bfffkt2djYxMTE8/fTTTJgwAR8fH7p160avXr0qogjgpXO0/L87coERHRpVzHGFEKIKcln4h4aGOsb/GTZsmGN5dHQ00dHRrjps8bx1juv8s4zWij++EEJUIRXa7VOpbmj5m6XbRwjh4Twq/PP6/E0WCX8hhGfznPD39s1t+St89XvRl6EKIYSn8Jzw99KiVil4Y+XYxczKLo0QQlQqDwp/+1ATcq2/EEJ4UvjnzuMr4S+EEJ4U/nnz+MqwzkII4YHhn9vyP5B0tTJLI4QQlcpzwt+7YJ9/erZ8AxBCeC7PCX8vLYDjWn+NRlWZpRFCiErlQeGf2/JX2Vv+iqJUZmmEEKJSeU74517tMySiBgAPfngAq00+AIQQnslzwj/3hG+3Jv6ORRabDPMghPBMnhP+uSd8fWxGxyLJfiGEp/Kc8M/t8/dS8m/ykpa/EMJTeU7457b8vWwGxyLp8xdCeCqPC3/FlO1YJOEvhPBUnhP+Gh9QqfFXWxyLJPyFEJ7Kc8JfpQIvX/xU+X3+h1KuV2KBhBCi8nhO+IO968ec43j4r48OVGJhhBCi8nhW+ANkXansEgghRKXzrPDPToWE9Swe27GySyKEEJXKs8I/l0adP6ibnPQVQngizwr/iKGgq1Eg/L87cqESCySEEJXDs8I/oB5ovLlxQE+dl2e9BEIIAZ4W/hotWIxcuJ5/xc87205WYoGEEKJyeFb4e2nBYsBoyR/TJ+5cBnqjpYSNhBDC/XhY+OvAaqKOv3eBxTPWHqmkAgkhROVwWfgfOnSI2NjYQsu3bdvGqFGjiImJYc2aNa46fNFyp3Ic1b4OD9zV1LE4KS2rYsshhBCVzMsVO33//fdZv349vr6+BZabzWbmzZvHl19+ia+vL2PHjiUqKoo6deq4ohiF+QQAoDJm8ESfFvzn5ySg4KWfQgjhCVzS8g8LC2Px4sWFlicmJhIWFkZwcDA+Pj506tSJ/fv3u6IIRQtuZP+ZcZ7aAVrH4j/OpHP8YmbFlUMIISqZS1r+AwYMICUlpdByvV5PYGCg47G/vz96vf6m+zMajSQkJJSrLAaDwbGt3+VrNAGSTx4lO6Pgt5JPdx0hpl3Nch2jqrmxzp5C6uwZpM7O45LwL05AQABZWfn961lZWQU+DIqj1WqJjIws1zETEhLytw3Kge3QpH4tiIhk15Qm9Fy0HYAMxa/cx6hqCtTZQ0idPYPUuezbFqdCr/Zp3rw5ycnJpKenYzKZOHDgAB07VuA4O7mTuGO1D+scVtvPsWrtH+cqrhxCCFHJKqTl/+2335KdnU1MTAzTpk1j4sSJKIrCqFGjCAkJqYgi2Gl87D+PbYA20YVWp+mNBc4FCCGEu3JZ+IeGhjou5Rw2bJhjeVRUFFFRUa46bMk0udf3H/kCRq0stLrTnC2cnjcYlUqu/hFCuDcPu8nr5q36bJO1AgoihBCVy7PCP/c6/xs91rt5gccZBnNFlUYIISqNZ4W/NgDC7gTfWo5FUwdGFHhKeraEvxDC/XlW+APUi7RP5l6MdX+c46vfUvg1Ma0CCyWEEBWrQq/zrxK8fcFsKHb1il1/On5Pmj+kIkokhBAVzvNa/l46sORQYEYXIYTwMJ4X/t46UGxgze/bb1TDt4QNhBDC/Xhe+HvlBr0lfzav2G5Ninxqmt6I0WLlPz+flonehRBuxTP7/AHMOaALBqBHyzrM/77wUzvN2eL43d/Hi+iOjVi+M5F/9QxH562piNIKIYRLeF7L/8bwz9WmYTBJ84fQukFQsZtlGMx89GsSr28+wQe7T7u6lEII4VKeF/7a3IA3pBda9em/uha7mcFsddz9azDLXcBCiOrN88K/RmP7z/QzhVYF+3kXWpbntR9PsPHIBQBk5B8hRHXneeFfs6n955oJYCw8e9f/Hrqj2E2P5c72Jad+hRDVneeFf+5JXgBObSm0+o7w2gxsU596gcUPArd42ylXlEwIISqM54X/jQ59XmiRRq1iWWwn2ocGF7FBvus5Zlb+9Ccf70nm1GWZ/1cIUb143qWeYG/9G67DiSKu78x1s8v6b539Y4HHMhSEEKI68cyW/7+P23+2u7fYp9Tyt8/6tXT8bRVRIiGEqFCeGf7evhDUKH9axyLMGt6Gufe0ZVDb+tQpxdSOO45fBiBVb+RSRvEDxwkhRFXgmeEPkHEODn4MWWlFjvIZoPVi/B1NUKlU7Jra+6a7u/8/+7mebabznC3c8epWsowWLFabCwouhBB/n+eGf55F4bD6nhKf4ufjRc0S7gHIc+vL+ecB2ry0ie4Ltv/t4gkhhCtI+AOc+eWmT9nxbB9+m9mvTLu9mGGg6bTvylsqIYRwGc8N/z4zCz62Wkp8erCfN7UDtPRoWafMh1Jy5w6w2RSMFhkaQghR+Tw3/FuPKPj4ldpw8NObbrY8thMAnZvULPWh3tpykulrjxA+YyO3zPwBk8VG7Ad72funTBUphKgcnhv+dVvlD/WQ5+AnN93Mz8eLuNkD+N+/ujKobX3C6/jfdJu3t57k0335Ywn984O9/HQylUc+/q2spRZCCKfw3PAHuH9jwccXDpdqswCtFz5eat77Zye2PdubvTP60qkM3wT2nb4KwLVsM2evZtN02nes/jUJm03hm4PnZOIYIYTLeXb4BzeCEUvyHxuvw8apZd5NSJCOrx69k63/7sXDPcP58emepd62x0L7FUEvfBPP5M8P8tRnB2k+YyN/XtGzYlciF6/LPQNCCOfz7PAH6PhPCGmb/3jf8vyJXuLWFjnyZ3Ga1w1gxuBIWoUEsmtKH1ZP7FKmoqw/dN7xe9TrO3l14zG6ztvKrPXxfPlbSpn2JYQQJZHwBxi3puDjU1thVjB8+QB8/1y5dhlW248eLesyZcAtf7t4H/6SxLNfHCLDYGbN/rMoikK2yYL5hpvIMgxm+ZYghCg1zxzY7a+CG8FL6TC7hv3x5+Pz12Ve/Fu7frxPCxZtOv639pGn/Sz7TWRTv8o/N3F63mA2HL7A/336BwAJLw90yrGEEO7NJS1/m83Giy++SExMDLGxsSQnJxdYP2fOHEaOHElsbCyxsbFkZlaBIZFVKrhrcuHl5w7A7rfyHyfthtRT9vsCTm0pVbdQvUAtGrWKto2C6NK0FgtHt2f/82W7Yaw4zaZvdAQ/wOTP/+CHExk8/snvNJ32XYHLSdcfOs+CH4455bhCVLiLR1Cbs5y/X0WBq2Wcl9tiKv1zrxy3H6MsbDawGCHjQtm2KwOXtPy3bNmCyWTi888/5+DBg8yfP5/33nvPsT4+Pp6VK1dSq1YtVxy+/O6ebb/8c8Pk/GWG67DlJTDp7ecHPvzL0M11I+DxvZB5CY5+DX61od3oAk/Z/VwUCgpaL02Rh/3q0W6Meu9XAP4vqsXfmixmU/wlNt3wOGbFHn6dHkW3edscy0Z2bETLkMBC2569mk1SWhbdwmuTci2HpqW4jLXSmHNAsYFPFS1j9lVY3hPG/BdCO7nkEGpTBlw8At5+9r9bddF/XwXYbGCzgGK1N1wC6hV+zvmD9p8NOxS/n4tH4PtpMPZT0AbC2X1QpyX45f5Pmw2QsB7ajbE3rEpy6ah9mPUTP8B3z8CzJ2H/BxB2B5zYBC3uts+d+vEomgQ3h/a/27c79xv41oSazexzcmdetO/n+6mQ8C1M2mUP3bx6XD0NOVehUe77cT0FAurb7/EBGLkSmtwJ70dBt8ehRV84vQua94Xdb8KQ1+x/b9eS4e320OQu6P8KXIyzTwtbuwU07gL+dUEXBGf2wKoB9n0PeR1ufyi/zvrLsLQbxHwMibn/m9466PygvU4bJsPv/wVA128VEFnya1gOKkUp60fSzc2bN4/27dszZIg9KHv06MFPP/0E2L8VdO/endtuu43U1FRGjx7N6NGjS9odBw8eRKu9+ciaRTEYDOh0ulI/X5cWT7MtE8t0jDM93yJs12TH44SYPXhlXcDLcBVDzQj8Lx/AGNQMi1/Bf7TjqQZ+PZPN/bcV/hA8c1XPy9/Gc466ZSpL0RQ6qBI5ojTDij0gVMD4DjX5+OA17m4RwOg2NZj0jf2kcqBWTabRxpPd6pBptGGy2vjkUDov9Qlh9vZL9GseQB/Vb3S6ozc+mWfxSz3MtZb572G22cao/yXxr861GNmmhhPKX1iL9cPxzrlMQsweoOD7rLIaqb9/PqltHsAcGJb7EiigWFEpVmr8uZ70ZsMABcXLFwBt+kn8Lx1AY0gjrfUD6K4dw6KrjSkwjEa/vkDQ2a0cv2cLNm8/Gu98CrXNTHLvJeiuHafxrskk9fsAc2Bj/C/8Qr1DS9A36EadYx8DkHDvL6hsFurEf4Du2nHOd52F1ScIr5xUWn47nKx6t3G9ySACz+0g8PzPAFxpMxHvrItkhdxO3SPLOH/HS+TUvRUUhRqnv6XBgfkFXo9r4cO50m4SNi8/1FYjrb62d/+l3DkXm8YXtc1EUPIPBKXsKLDd5XaP4H/pgP1vNKAxWv3ZQq91auv70Te4i6Zb/1VonT6kCwGX9tlfYpWaM70Xo7t6jBp/rkebaf/Gf6nDk1xteS+1j/+PeoeXlvWt9nhn2z2JvvW4cm8fGVn0B4dLwv/555+nf//+9OrVC4DevXuzZcsWvLy80Ov1fPTRRzzwwANYrVYmTJjAq6++SkRERLH7S0hIKLYCN1PmbRUF4tfBnqWQsr9cx6T/XPjx+cLL798ITe+CX5dC2ik48AGM/wpa9oOrf9q/NeRNM7n0Trgcz/qgcTx9eRAqFBrVDiI5LZtFo9tz7GImn+07Q33zGRKVhpQ0rXwbVRLfaWew2BLN65Z7qUs6FtRcIwiAhzTfUVeVzmuWGMxo6KY+ykWlFg9ofmCIZg+zzfdxXAklGy3XlQDqqtLZqp1CdqsR+J78FpVi4+zjZ7h0aBOdeg5h6/GrPPTJYbSYOPJ0G3KCmnHheg4R9YPsLcKMc/Yus8sJcP0s9H0x94orFXz9CBz+HB74AepFwMktsPlFyDwPEUOhRhPwrwNbZ9sr13Y03D2bxIRDNA8NgdDOcHgNrL0hqJ46bG+plUeLfkVO9ylERTkx4ntadbyzXNuWlH8ua/nfeuutDB48GICePXuya9cuAKxWKzk5OQQEBACwcOFCWrVqRXR0dLH7q9Dwv9He5bDrNXgmAU5thk//YV/+/EWYW798+yyJfz37B8CVhCJXK9ogVMYM+wNdDftX3VyZ6mB+NLfjE0s/Vvq8xsfWfmQo/tyt+Q0/DLRTJwGwyjKQB71+AGCZZSj7bBGs8nnN6VXJUXzwVdn7RReZ72W45hduUcvlqqIITbpD8u78xw98D/8ZVPrtG3W2n5vrGAvnfrc3rG6Ltc/ZcSne3lVjzrZ3CyX9VPr9NuwI5/PPp6GrYd9XcGPw8rF3Cd14/LL61zb45V171/JtE2DdJLDkXrHX9XG4ZRBcOEhCzX4uyT+XhP+mTZvYvn078+fP5+DBg7z77rusXLkSgMTERCZPnszXX3+NzWYjNjaWV155hZYtW5arAjfzt8L/RtlXYWEziF4GHcba3zD9ZXi3s71F2uMZe5/iV2XrMhKV4NZx9m8XAfUgs5Qn1G4ZAsdLOUJrn+dh+1y4dzU0aA9v31ry8wMbwK1j7Q2MQYsgqAFsnAIRQ6BOK8g4DzsX2MMopC1cOQbpZ0gKH0/TdnfCse9g0/TC+w0Og5pN7GEY1hUGvApnfoW2o+zzWJzaDM16wU+vQ6+p9m84tZrb+9rTz9rPIcR/be8jP/yZvW++74v2/u2W/YHcb8k1wuyP9Zft39DMBvs5MpsFGt4GZ/fAhmfsfd69phZ9bsKYaT9+m2KGVzdlASoSEpOJrGEG3xr2b8naoJufU7iRotifbzHC5aP21zfvvJGigM0KmlKeCs1Ks2/rrbP3+9drDepirqG5nGA/P1iWsuZyVf65JPxtNhuzZs3ixIkTKIrCq6++yq5duwgLC6Nv376sXLmS77//Hm9vb0aMGMHYsWPLXYGbcVr4l9bRb2DNBHt3wT+/sv9BXYq3d1V892/IuWY/kVS/vf2famGz/G3V3tD7Ofs6VPZ/QpvVfuIo+Wf7N5GGHaHl3fZwaTXIPkCdSmVvNZTg8p2zqPfLLMdjvaIjQGVvZay3dqNOzRrcmWGf03iZZShnlBDibE3R40tH1Sm+svWgreo0G7T5o6Husrajp+YIAIst0QSQwwNem7ik1CCIbEfL/0aDjPNooEpjiGYP71uGkqSE0EiVykteH9FTc4R11rv4j2Ugh5Vwpnh9zm5bO/bYIvHBghH7zGveWAgiizTsXWRbfJ6ljuo6v9paM0hj76pbVnsaD7CeHzKbos25wi5bewLu+hfXs83UD9bR+5a6/JZ8jY0HjvPk7QHcvyGDGYMjuL9zPXzO7LJ/AwvtwqWkeHQNIgj2tc/nYLRYMVtsmKwK3+w/SZ/6RpqGNc0/0VmBHH/bimIPW20gXEuCwIb2lqkbqvD/5yqgWoW/s1Wr8FcUOPSZ/Subb42bP19/GVQa8K/tnOPnpJOQdIFIv3QIrA8BIeDjl38s/WWon3tHsyGDnKS9pIXcRd1ALVqNGlQqPt6TzNLtp/hlel/Ss014a9R8c/A8b2w+wWO9m/PyhqOOwzUI1tG5aS2+PXSeugE+ZOozMZB/cl6NjX7q39hla19geXXwVN+WvL31JAALR7dn6pdFj/30wX2d6dy0Fvf/Zx+BOm/MFhsRDQIJrelHx7AafPhzEr8lX+Nqlonojo34d/9W+HprSM8x4+etYdR7v3Dv7Y3x0ai5pX4gXcNro1GrsFht/HQyla7htfH1KdhaNllsJJ487vjbttkU1Or8VqXBbMVothFcikmIqhMJf+dtK+Hvhlxd56jXd5CRY+FAEZPb/BB3gdYNgtmXdJW7W4fw7zUH2ZJw2bG+9y112XH8SqHtBrerT+LlLI5fqgL3fFRB7RoFc+Tc9RKfs2ZSN5btTGTbsfzXOyRIi02BK5lGx7KI+oEcu5jJE31aENkgiCHtGwBw7GIGO45fYWTHRtQL0mGy2Gg183se6dWcaYMiuJ5jJtjXm8sZBowWGx/+koTWS83UgcVfrHE1y8Tp1CxaNwjC10fDxiMXMFlsRHdsBNjnujBZbY7LoNP0Rrxyu06K+uCS/2fnbSvh74ZcXWebTUGlAlUp+y9PXc7kQNI1LmYYmNyvFRevG8g0mFGpVBy/mMmfV/T8X9+WKIrCH2fT2XH8Cu0aBfNb8jWW7UxkxuAI/tEljCCdN2arjcVbT/LB7tNkmax8/1QP/H28iH1/N8npZpfVWdzc4Hb1CQnSEVbLj9nfHi20/sZvUjMGR/DZ/rP8eSX/pq2597Tl+XVxjscP3NWUrQmX6dy0Juv+OFfoPqnH+zSnfrAvt4QEUi9QS8MavizbmcifV/R8fdA+TlZs1yY0rOHLr3+msevEFfpG1MNiUzidmsWOZ3uTmmXkQrqBYF9vmtT241DKdeoFapn8+UEGta1Pg2Ade/68yuR+LUnLMpGclkVURAi/nErFbFMwW2ysO3iOugFaJnZvRkiQDo1ahUatYt/pqzSv64/eaGHJ9lNMGxRJoM4Lb03h8wKKomC2KuSYrWjUKlIzjY77bCT8JfxLzVPrHBDShB3HL6NWq1CrVNzVvA5htf0cz4k/f53a/lpMFhupWUbqB+l4ePUBGgb7Mm9kOxb8cIyY2xsz6r1faVTDl68fv4vTqVm8u/0U0R0acjXLxJzv7FdiRUXU4+TlTM5ezSlV+bqF1+ZXmbxHAIE6LzINJc8cmMfXW8OHI0O5o2Pbmz+5CCVlgYztI9xG41p+xHZrWuz6Ng2DHb/nfShs+L8ejmULR9uvykman38Xd91ALR81yx+d9Z9dm7Dypz/5V89wtF4abp+7hYwcM7ufi3KcFF77ewqjOoUWauFlmyz0WrSD5wdH0rZRMKCQY7LRLtRerg92n+ank1fYcfwKsV2bMH1wBDYFxxAdtzerhc5LQ6uZ9hPzj3SpzaMDO7Ep/iLLdiU6WtFzottyLcvEhG5NsSoKt72yGYBa/j7UC9Ry7KJ0rVWm0gY/QI7ZytHLBu5wQTmk5e+GpM4Vx2SxlTh0R1nZbAo/Hr3IgDb1b9qtVto6W20K567lUD9Yh4+XmkyDGbVKhb/WC0VR+PXPNLo2q+04YWy1KVhtCj5e9g+v9YfOc2toMO9sPUWwrzcvDmtdYN/HLmZQN1DLvtNX+fNKFm9sPgHAn68ORq1WkWEwk5SaxfB3f8ZHo8aUOxrtmkndOHEpkwXfH2PpP28j9oN9vDyiDfd2bszZq9l8vCeZW+oHsT/pKl3Da/HcV/Yry/q3DuHHo5cI0nnRpLZ/gXMh93RsRI7Jyg/x9gEZn+rbks/2n+FSRv45jzua1WJv7oRK7UODOZxS8rmUyrYhthlt27S++ROLIN0+EoRuT+pc9Z1Lz6FhsI7rOfZzMzX8Cl6OqihKiR946dkmDsYfo/ftBe/WttkUVv18mn90CSNAe/PODLPVxsGz6dzetJbjuDbFPrZV/WAdOu/8D3KrTSHlWjZNavuTeEVPcloWvVvVw6ooeKlVKAoFrrK60aUMAwFaL45eyCAkUMe1bBPtQ4OxKfZjJlzI5NSVTFrWCySyQRBLt5+iY1hN5n2fwF0t6jBlwC1oVCqOHz/mkvyTbh8hRIVoVMM+jtJfQz/Pzb7p1PDzISSg8BVAarWKh3qEl7oc3hq1I/jzjqtRUeRAhhq1iia17cub1w2geV37yATq3OFUSipySJB9rKm8Y+V1NWpUACrahQY7uvwA/q+v/UbX71r2oCLIZC5CCOGBJPyFEMIDSfgLIYQHkvAXQggPJOEvhBAeSMJfCCE8kIS/EEJ4IAl/IYTwQNXiDt+/M4G7EEJ4KqPRSIcOHYpcVy3CXwghhHNJt48QQnggCX8hhPBAEv5CCOGBJPyFEMIDSfgLIYQHkvAXQggP5LaTudhsNmbNmsXx48fx8fFhzpw5NGnSpLKL5TT33HMPAQH2iSVCQ0OJiYlh7ty5aDQaunfvzhNPPOE2r8GhQ4d47bXXWL16NcnJyUybNg2VSkXLli156aWXUKvVvPvuu+zYsQMvLy9mzJhB+/bti31udXBjnY8ePcqkSZNo2rQpAGPHjmXw4MFuU2ez2cyMGTM4d+4cJpOJRx99lBYtWrj1+1xUnRs0aFCx77PipjZt2qQ899xziqIoyh9//KE88sgjlVwi5zEYDMqIESMKLBs+fLiSnJys2Gw25aGHHlLi4+Pd4jVYsWKFMnToUGXMmDGKoijKpEmTlD179iiKoigvvPCC8uOPPypxcXFKbGysYrPZlHPnzikjR44s9rnVwV/rvGbNGuWDDz4o8Bx3qvOXX36pzJkzR1EURbl27ZrSq1cvt3+fi6pzRb/PVfvj8W/47bff6NHDPh1ahw4diIuLq+QSOc+xY8fIycnhwQcfZMKECezfvx+TyURYWBgqlYru3bvzyy+/uMVrEBYWxuLFix2P4+Pj6dKlCwA9e/Z01LN79+6oVCoaNmyI1Wrl6tWrRT63OvhrnePi4tixYwfjx49nxowZ6PV6t6rzwIEDeeqppwD73LYajcbt3+ei6lzR77Pbhr9er3d0iwBoNBosFksllsh5dDodEydO5IMPPmD27NlMnz4dX19fx3p/f38yMzPd4jUYMGAAXl75vZPKDZN8F1fPvOVFPbc6+Gud27dvz9SpU/nkk09o3LgxS5Yscas6+/v7ExAQgF6v58knn2Ty5Mlu/z4XVeeKfp/dNvwDAgLIyspyPLbZbAX+oaqzZs2aMXz4cFQqFc2aNSMwMJD09HTH+qysLIKCgtzyNbixX7O4emZlZREYGFjkc6uju+++m7Zt2zp+P3r0qNvV+cKFC0yYMIERI0YwbNgwj3if/1rnin6f3Tb8b7vtNnbt2gXYB4Zr1apVJZfIeb788kvmz58PwKVLl8jJycHPz48zZ86gKAq7d++mc+fObvkatG7dmr179wKwa9cuRz13796NzWbj/Pnz2Gw2atWqVeRzq6OJEydy+PBhAH799VfatGnjVnVOTU3lwQcfZMqUKYwePRpw//e5qDpX9PvstgO75V3pcuLECRRF4dVXX6V58+aVXSynMJlMTJ8+nfPnz6NSqXj22WdRq9W8+uqrWK1WunfvztNPP+02r0FKSgrPPPMMa9as4fTp07zwwguYzWbCw8OZM2cOGo2GxYsXs2vXLmw2G9OnT6dz587FPrc6uLHO8fHxvPLKK3h7e1OnTh1eeeUVAgIC3KbOc+bM4fvvvyc8PNyx7Pnnn2fOnDlu+z4XVefJkyezaNGiCnuf3Tb8hRBCFM9tu32EEEIUT8JfCCE8kIS/EEJ4IAl/IYTwQBL+Qgjhgar3HT9CONHevXuZPHkyLVq0cCyrWbMm77zzzt/a77Rp0xg8eDA9e/b8u0UUwmkk/IW4QdeuXXnzzTcruxhCuJyEvxA3ERsbS7NmzTh9+jSKovDmm29St25d5s+fz2+//QbA0KFDue+++0hKSmLmzJmYzWZ0Op3jg+Tzzz9n5cqV6PV6Zs2aRfv27SuzSkJI+Atxoz179hAbG+t43KtXL8A+XMjLL7/MJ598wvLly7nrrrtISUlhzZo1WCwWxo0bR9euXXnrrbd4+OGH6dmzJ1u3buXo0aMAtGnThscee4y1a9eydu1aCX9R6ST8hbhBUd0+O3fupGvXroD9Q2Dbtm3Ur1+fzp07o1Kp8Pb25tZbbyUxMZHTp0/TsWNHAPr27QvAhg0baNOmDQB16tTBYDBUYI2EKJpc7SNEKeTNhfD777/TokULmjdv7ujyMZvN/PHHHzRp0oTmzZtz5MgRANavX8/q1asBHMPvClFVSMtfiBv8tdsHwGAwsG7dOj788EN8fX1ZuHAhNWvWZN++fcTExGA2mxk4cCBt2rRh6tSpvPjii7z33nvodDoWLVpEfHx8JdVGiOLJwG5C3ERsbCyzZs2qliOiClEc6fYRQggPJC1/IYTwQNLyF0IIDyThL4QQHkjCXwghPJCEvxBCeCAJfyGE8ED/DwmYoKGUDlA5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n",
      "MAE: 0.49570250629323775\n",
      "MSE: 0.5485826777916293\n",
      "RMSE: 0.740663673870691\n",
      "MAPE: 0.22083423613305322\n",
      "R2: 0.5386809946402153\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.321348</td>\n",
       "      <td>-2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.715999</td>\n",
       "      <td>-1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.359442</td>\n",
       "      <td>-2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.440207</td>\n",
       "      <td>-2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.359442</td>\n",
       "      <td>-2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-3.682683</td>\n",
       "      <td>-2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-2.440207</td>\n",
       "      <td>-2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-2.456916</td>\n",
       "      <td>-2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-1.840274</td>\n",
       "      <td>-1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-1.189222</td>\n",
       "      <td>-1.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  actual\n",
       "0    -2.321348   -2.30\n",
       "1    -1.715999   -1.65\n",
       "2    -2.359442   -2.67\n",
       "3    -2.440207   -2.49\n",
       "4    -2.359442   -2.37\n",
       "..         ...     ...\n",
       "108  -3.682683   -2.80\n",
       "109  -2.440207   -2.38\n",
       "110  -2.456916   -2.40\n",
       "111  -1.840274   -1.21\n",
       "112  -1.189222   -1.55\n",
       "\n",
       "[113 rows x 2 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = ANN_model(X_train, X_test, y_train, y_test)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores['predictions_ANN'] = final_result['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores['mean'] = (results_scores['predictions_ANN'] + results_scores['predictions_CatBoostRegressor'] + results_scores['predictions_GradientBoostingRegressor'] + results_scores['predictions_LGBMRegressor']) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>Compound</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>predictions_LGBMRegressor</th>\n",
       "      <th>predictions_GradientBoostingRegressor</th>\n",
       "      <th>predictions_CatBoostRegressor</th>\n",
       "      <th>predictions_ANN</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.30</td>\n",
       "      <td>butoxyethanol</td>\n",
       "      <td>CCCCOCCO</td>\n",
       "      <td>-2.255343</td>\n",
       "      <td>-2.315491</td>\n",
       "      <td>-2.260737</td>\n",
       "      <td>-2.278637</td>\n",
       "      <td>-2.277552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.65</td>\n",
       "      <td>Dimethyl phthalate</td>\n",
       "      <td>COC(=O)C1=CC=CC=C1C(=O)OC</td>\n",
       "      <td>-1.645769</td>\n",
       "      <td>-1.605519</td>\n",
       "      <td>-1.519364</td>\n",
       "      <td>-1.693404</td>\n",
       "      <td>-1.616014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.67</td>\n",
       "      <td>Corticosterone</td>\n",
       "      <td>CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O</td>\n",
       "      <td>-2.497803</td>\n",
       "      <td>-2.499318</td>\n",
       "      <td>-2.504789</td>\n",
       "      <td>-2.378143</td>\n",
       "      <td>-2.470013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.49</td>\n",
       "      <td>Estradiol</td>\n",
       "      <td>CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O</td>\n",
       "      <td>-2.516031</td>\n",
       "      <td>-2.517663</td>\n",
       "      <td>-2.517964</td>\n",
       "      <td>-2.444862</td>\n",
       "      <td>-2.499130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.37</td>\n",
       "      <td>Corticosterone</td>\n",
       "      <td>CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O</td>\n",
       "      <td>-2.497803</td>\n",
       "      <td>-2.499318</td>\n",
       "      <td>-2.504789</td>\n",
       "      <td>-2.378143</td>\n",
       "      <td>-2.470013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-2.80</td>\n",
       "      <td>Methanol</td>\n",
       "      <td>CO</td>\n",
       "      <td>-3.143522</td>\n",
       "      <td>-3.147833</td>\n",
       "      <td>-3.229730</td>\n",
       "      <td>-3.608455</td>\n",
       "      <td>-3.282385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-2.38</td>\n",
       "      <td>Estradiol</td>\n",
       "      <td>CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O</td>\n",
       "      <td>-2.516031</td>\n",
       "      <td>-2.517663</td>\n",
       "      <td>-2.517964</td>\n",
       "      <td>-2.444862</td>\n",
       "      <td>-2.499130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-2.40</td>\n",
       "      <td>Lidocaine</td>\n",
       "      <td>CCN(CC)CC(=O)NC1=C(C=CC=C1C)C</td>\n",
       "      <td>-2.506980</td>\n",
       "      <td>-2.398611</td>\n",
       "      <td>-2.519638</td>\n",
       "      <td>-2.556184</td>\n",
       "      <td>-2.495353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-1.21</td>\n",
       "      <td>Octanol</td>\n",
       "      <td>CCCCCCCCO</td>\n",
       "      <td>-1.173194</td>\n",
       "      <td>-1.245904</td>\n",
       "      <td>-1.191847</td>\n",
       "      <td>-1.784392</td>\n",
       "      <td>-1.348834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-1.55</td>\n",
       "      <td>Naphthalene</td>\n",
       "      <td>C1=CC=C2C=CC=CC2=C1</td>\n",
       "      <td>-1.597767</td>\n",
       "      <td>-1.623966</td>\n",
       "      <td>-1.770463</td>\n",
       "      <td>-1.139698</td>\n",
       "      <td>-1.532973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual            Compound                                        SMILES  \\\n",
       "0     -2.30       butoxyethanol                                      CCCCOCCO   \n",
       "1     -1.65  Dimethyl phthalate                     COC(=O)C1=CC=CC=C1C(=O)OC   \n",
       "2     -2.67      Corticosterone  CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O   \n",
       "3     -2.49           Estradiol           CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O   \n",
       "4     -2.37      Corticosterone  CC12CCC(=O)C=C1CCC3C2C(CC4(C3CCC4C(=O)CO)C)O   \n",
       "..      ...                 ...                                           ...   \n",
       "108   -2.80            Methanol                                            CO   \n",
       "109   -2.38           Estradiol           CC12CCC3C(C1CCC2O)CCC4=C3C=CC(=C4)O   \n",
       "110   -2.40           Lidocaine                 CCN(CC)CC(=O)NC1=C(C=CC=C1C)C   \n",
       "111   -1.21             Octanol                                     CCCCCCCCO   \n",
       "112   -1.55         Naphthalene                           C1=CC=C2C=CC=CC2=C1   \n",
       "\n",
       "     predictions_LGBMRegressor  predictions_GradientBoostingRegressor  \\\n",
       "0                    -2.255343                              -2.315491   \n",
       "1                    -1.645769                              -1.605519   \n",
       "2                    -2.497803                              -2.499318   \n",
       "3                    -2.516031                              -2.517663   \n",
       "4                    -2.497803                              -2.499318   \n",
       "..                         ...                                    ...   \n",
       "108                  -3.143522                              -3.147833   \n",
       "109                  -2.516031                              -2.517663   \n",
       "110                  -2.506980                              -2.398611   \n",
       "111                  -1.173194                              -1.245904   \n",
       "112                  -1.597767                              -1.623966   \n",
       "\n",
       "     predictions_CatBoostRegressor  predictions_ANN      mean  \n",
       "0                        -2.260737        -2.278637 -2.277552  \n",
       "1                        -1.519364        -1.693404 -1.616014  \n",
       "2                        -2.504789        -2.378143 -2.470013  \n",
       "3                        -2.517964        -2.444862 -2.499130  \n",
       "4                        -2.504789        -2.378143 -2.470013  \n",
       "..                             ...              ...       ...  \n",
       "108                      -3.229730        -3.608455 -3.282385  \n",
       "109                      -2.517964        -2.444862 -2.499130  \n",
       "110                      -2.519638        -2.556184 -2.495353  \n",
       "111                      -1.191847        -1.784392 -1.348834  \n",
       "112                      -1.770463        -1.139698 -1.532973  \n",
       "\n",
       "[113 rows x 8 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores['diff'] = abs(results_scores['actual'] - results_scores['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores.to_excel(\"../../results/predictions.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature-Selection\"></a>\n",
    "### 3.2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs1 = SFS(GradientBoostingRegressor(), \n",
    "           k_features=30,\n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=0,\n",
    "           scoring=\"neg_mean_absolute_error\",\n",
    "           cv=0)\n",
    "\n",
    "sfs1 = sfs1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ALogP',\n",
       " 'ATSc4',\n",
       " 'BCUTw-1h',\n",
       " 'BCUTp-1l',\n",
       " 'nBase',\n",
       " 'C1SP1',\n",
       " 'C2SP1',\n",
       " 'C1SP2',\n",
       " 'VCH-3',\n",
       " 'VCH-4',\n",
       " 'JPLogP',\n",
       " 'khs.sLi',\n",
       " 'khs.ssBe',\n",
       " 'khs.ssssBe',\n",
       " 'khs.ssBH',\n",
       " 'khs.sssB',\n",
       " 'khs.ssssB',\n",
       " 'khs.tCH',\n",
       " 'khs.ddC',\n",
       " 'khs.tsC',\n",
       " 'khs.aaaC',\n",
       " 'khs.sNH3',\n",
       " 'khs.sNH2',\n",
       " 'khs.ssNH2',\n",
       " 'khs.dNH',\n",
       " 'khs.dSe',\n",
       " 'khs.sssPbH',\n",
       " 'MDEO-12',\n",
       " 'nRingBlocks',\n",
       " 'WPATH')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try selecting more features\n",
    "sfs1.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_idx</th>\n",
       "      <th>cv_scores</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>feature_names</th>\n",
       "      <th>ci_bound</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>std_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1,)</td>\n",
       "      <td>[-0.3869180313716998]</td>\n",
       "      <td>-0.386918</td>\n",
       "      <td>(ALogP,)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 219)</td>\n",
       "      <td>[-0.3273518503308258]</td>\n",
       "      <td>-0.327352</td>\n",
       "      <td>(ALogP, WPATH)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1, 28, 219)</td>\n",
       "      <td>[-0.3083673982758248]</td>\n",
       "      <td>-0.308367</td>\n",
       "      <td>(ALogP, BCUTp-1l, WPATH)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 28, 89, 219)</td>\n",
       "      <td>[-0.30124225789930265]</td>\n",
       "      <td>-0.301242</td>\n",
       "      <td>(ALogP, BCUTp-1l, JPLogP, WPATH)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, 28, 89, 186, 219)</td>\n",
       "      <td>[-0.29888624161698196]</td>\n",
       "      <td>-0.298886</td>\n",
       "      <td>(ALogP, BCUTp-1l, JPLogP, MDEO-12, WPATH)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1, 25, 28, 89, 186, 219)</td>\n",
       "      <td>[-0.2977788347943648]</td>\n",
       "      <td>-0.297779</td>\n",
       "      <td>(ALogP, BCUTw-1h, BCUTp-1l, JPLogP, MDEO-12, W...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1, 12, 25, 28, 89, 186, 219)</td>\n",
       "      <td>[-0.2959466196502808]</td>\n",
       "      <td>-0.295947</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, JPLogP, MDE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1, 12, 25, 28, 89, 186, 201, 219)</td>\n",
       "      <td>[-0.29472214338119773]</td>\n",
       "      <td>-0.294722</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, JPLogP, MDE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(1, 12, 25, 28, 35, 89, 186, 201, 219)</td>\n",
       "      <td>[-0.2935410327410466]</td>\n",
       "      <td>-0.293541</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, C1SP2, JPLo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(1, 12, 25, 28, 35, 89, 110, 186, 201, 219)</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, C1SP2, JPLo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(1, 12, 25, 28, 31, 35, 89, 110, 186, 201, 219)</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(1, 12, 25, 28, 31, 35, 89, 93, 110, 186, 201,...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 35, 89, 93, 110, 186, ...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 89, 93, 110, 1...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 89, 93, 95, 11...</td>\n",
       "      <td>[-0.2933334870347154]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...</td>\n",
       "      <td>[-0.2933334870347154]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.2933334870347154]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...</td>\n",
       "      <td>[-0.29333348703471546]</td>\n",
       "      <td>-0.293333</td>\n",
       "      <td>(ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature_idx               cv_scores  \\\n",
       "1                                                (1,)   [-0.3869180313716998]   \n",
       "2                                            (1, 219)   [-0.3273518503308258]   \n",
       "3                                        (1, 28, 219)   [-0.3083673982758248]   \n",
       "4                                    (1, 28, 89, 219)  [-0.30124225789930265]   \n",
       "5                               (1, 28, 89, 186, 219)  [-0.29888624161698196]   \n",
       "6                           (1, 25, 28, 89, 186, 219)   [-0.2977788347943648]   \n",
       "7                       (1, 12, 25, 28, 89, 186, 219)   [-0.2959466196502808]   \n",
       "8                  (1, 12, 25, 28, 89, 186, 201, 219)  [-0.29472214338119773]   \n",
       "9              (1, 12, 25, 28, 35, 89, 186, 201, 219)   [-0.2935410327410466]   \n",
       "10        (1, 12, 25, 28, 35, 89, 110, 186, 201, 219)  [-0.29333348703471546]   \n",
       "11    (1, 12, 25, 28, 31, 35, 89, 110, 186, 201, 219)  [-0.29333348703471546]   \n",
       "12  (1, 12, 25, 28, 31, 35, 89, 93, 110, 186, 201,...  [-0.29333348703471546]   \n",
       "13  (1, 12, 25, 28, 31, 33, 35, 89, 93, 110, 186, ...  [-0.29333348703471546]   \n",
       "14  (1, 12, 25, 28, 31, 33, 34, 35, 89, 93, 110, 1...  [-0.29333348703471546]   \n",
       "15  (1, 12, 25, 28, 31, 33, 34, 35, 89, 93, 95, 11...   [-0.2933334870347154]   \n",
       "16  (1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...  [-0.29333348703471546]   \n",
       "17  (1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...  [-0.29333348703471546]   \n",
       "18  (1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...  [-0.29333348703471546]   \n",
       "19  (1, 12, 25, 28, 31, 33, 34, 35, 48, 89, 93, 95...   [-0.2933334870347154]   \n",
       "20  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "21  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "22  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "23  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "24  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "25  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...   [-0.2933334870347154]   \n",
       "26  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "27  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "28  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "29  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "30  (1, 12, 25, 28, 31, 33, 34, 35, 47, 48, 89, 93...  [-0.29333348703471546]   \n",
       "\n",
       "   avg_score                                      feature_names ci_bound  \\\n",
       "1  -0.386918                                           (ALogP,)      NaN   \n",
       "2  -0.327352                                     (ALogP, WPATH)      NaN   \n",
       "3  -0.308367                           (ALogP, BCUTp-1l, WPATH)      NaN   \n",
       "4  -0.301242                   (ALogP, BCUTp-1l, JPLogP, WPATH)      NaN   \n",
       "5  -0.298886          (ALogP, BCUTp-1l, JPLogP, MDEO-12, WPATH)      NaN   \n",
       "6  -0.297779  (ALogP, BCUTw-1h, BCUTp-1l, JPLogP, MDEO-12, W...      NaN   \n",
       "7  -0.295947  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, JPLogP, MDE...      NaN   \n",
       "8  -0.294722  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, JPLogP, MDE...      NaN   \n",
       "9  -0.293541  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, C1SP2, JPLo...      NaN   \n",
       "10 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, C1SP2, JPLo...      NaN   \n",
       "11 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "12 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "13 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "14 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "15 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "16 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "17 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "18 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "19 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "20 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "21 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "22 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "23 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "24 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "25 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "26 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "27 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "28 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "29 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "30 -0.293333  (ALogP, ATSc4, BCUTw-1h, BCUTp-1l, nBase, C1SP...      NaN   \n",
       "\n",
       "   std_dev std_err  \n",
       "1      0.0     NaN  \n",
       "2      0.0     NaN  \n",
       "3      0.0     NaN  \n",
       "4      0.0     NaN  \n",
       "5      0.0     NaN  \n",
       "6      0.0     NaN  \n",
       "7      0.0     NaN  \n",
       "8      0.0     NaN  \n",
       "9      0.0     NaN  \n",
       "10     0.0     NaN  \n",
       "11     0.0     NaN  \n",
       "12     0.0     NaN  \n",
       "13     0.0     NaN  \n",
       "14     0.0     NaN  \n",
       "15     0.0     NaN  \n",
       "16     0.0     NaN  \n",
       "17     0.0     NaN  \n",
       "18     0.0     NaN  \n",
       "19     0.0     NaN  \n",
       "20     0.0     NaN  \n",
       "21     0.0     NaN  \n",
       "22     0.0     NaN  \n",
       "23     0.0     NaN  \n",
       "24     0.0     NaN  \n",
       "25     0.0     NaN  \n",
       "26     0.0     NaN  \n",
       "27     0.0     NaN  \n",
       "28     0.0     NaN  \n",
       "29     0.0     NaN  \n",
       "30     0.0     NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(sfs1.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (337, 30) \t Shape of y_train: (337,)\n",
      "Shape of X_test: (113, 30) \t Shape of y_test: (113,)\n",
      "LinearRegression\n",
      "Lasso\n",
      "DecisionTree\n",
      "RandomForest\n",
      "XGBRegressor\n",
      "GradientBoostingRegressor\n",
      "Elastic Net\n",
      "BayesianRidge\n",
      "CatBoostRegressor\n",
      "LGBMRegressor\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Mean CV</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R_Squared</th>\n",
       "      <th>Adjusted_R_Squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>-0.805315</td>\n",
       "      <td>1.090880e+11</td>\n",
       "      <td>3.296793e+10</td>\n",
       "      <td>6.723605e+23</td>\n",
       "      <td>8.199759e+11</td>\n",
       "      <td>-4.967275e+23</td>\n",
       "      <td>-6.784571e+23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>-0.887162</td>\n",
       "      <td>9.734274e-01</td>\n",
       "      <td>7.433510e-01</td>\n",
       "      <td>1.354955e+00</td>\n",
       "      <td>1.164025e+00</td>\n",
       "      <td>-1.015262e-03</td>\n",
       "      <td>-3.672404e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>-0.664369</td>\n",
       "      <td>4.907985e-01</td>\n",
       "      <td>3.958904e-01</td>\n",
       "      <td>4.757398e-01</td>\n",
       "      <td>6.897389e-01</td>\n",
       "      <td>6.485323e-01</td>\n",
       "      <td>5.199465e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>-0.539165</td>\n",
       "      <td>4.490700e-01</td>\n",
       "      <td>3.446840e-01</td>\n",
       "      <td>4.021617e-01</td>\n",
       "      <td>6.341622e-01</td>\n",
       "      <td>7.028904e-01</td>\n",
       "      <td>5.941917e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>-0.511765</td>\n",
       "      <td>4.854216e-01</td>\n",
       "      <td>3.886612e-01</td>\n",
       "      <td>4.544673e-01</td>\n",
       "      <td>6.741419e-01</td>\n",
       "      <td>6.642480e-01</td>\n",
       "      <td>5.414119e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>-0.519993</td>\n",
       "      <td>4.483893e-01</td>\n",
       "      <td>3.638765e-01</td>\n",
       "      <td>4.078666e-01</td>\n",
       "      <td>6.386443e-01</td>\n",
       "      <td>6.986758e-01</td>\n",
       "      <td>5.884352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>-0.808434</td>\n",
       "      <td>9.199339e-01</td>\n",
       "      <td>7.141803e-01</td>\n",
       "      <td>1.204527e+00</td>\n",
       "      <td>1.097509e+00</td>\n",
       "      <td>1.101179e-01</td>\n",
       "      <td>-2.154487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>-0.774421</td>\n",
       "      <td>6.973730e-01</td>\n",
       "      <td>5.934655e-01</td>\n",
       "      <td>8.168659e-01</td>\n",
       "      <td>9.038064e-01</td>\n",
       "      <td>3.965146e-01</td>\n",
       "      <td>1.757273e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>-0.530989</td>\n",
       "      <td>4.645373e-01</td>\n",
       "      <td>3.888652e-01</td>\n",
       "      <td>4.388632e-01</td>\n",
       "      <td>6.624675e-01</td>\n",
       "      <td>6.757760e-01</td>\n",
       "      <td>5.571575e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>-0.533690</td>\n",
       "      <td>4.485061e-01</td>\n",
       "      <td>3.533778e-01</td>\n",
       "      <td>3.925345e-01</td>\n",
       "      <td>6.265257e-01</td>\n",
       "      <td>7.100028e-01</td>\n",
       "      <td>6.039063e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model   Mean CV           MAE          MAPE  \\\n",
       "0           LinearRegression -0.805315  1.090880e+11  3.296793e+10   \n",
       "1                      Lasso -0.887162  9.734274e-01  7.433510e-01   \n",
       "2               DecisionTree -0.664369  4.907985e-01  3.958904e-01   \n",
       "3               RandomForest -0.539165  4.490700e-01  3.446840e-01   \n",
       "4               XGBRegressor -0.511765  4.854216e-01  3.886612e-01   \n",
       "5  GradientBoostingRegressor -0.519993  4.483893e-01  3.638765e-01   \n",
       "6                Elastic Net -0.808434  9.199339e-01  7.141803e-01   \n",
       "7              BayesianRidge -0.774421  6.973730e-01  5.934655e-01   \n",
       "8          CatBoostRegressor -0.530989  4.645373e-01  3.888652e-01   \n",
       "9              LGBMRegressor -0.533690  4.485061e-01  3.533778e-01   \n",
       "\n",
       "            MSE          RMSE     R_Squared  Adjusted_R_Squared  \n",
       "0  6.723605e+23  8.199759e+11 -4.967275e+23       -6.784571e+23  \n",
       "1  1.354955e+00  1.164025e+00 -1.015262e-03       -3.672404e-01  \n",
       "2  4.757398e-01  6.897389e-01  6.485323e-01        5.199465e-01  \n",
       "3  4.021617e-01  6.341622e-01  7.028904e-01        5.941917e-01  \n",
       "4  4.544673e-01  6.741419e-01  6.642480e-01        5.414119e-01  \n",
       "5  4.078666e-01  6.386443e-01  6.986758e-01        5.884352e-01  \n",
       "6  1.204527e+00  1.097509e+00  1.101179e-01       -2.154487e-01  \n",
       "7  8.168659e-01  9.038064e-01  3.965146e-01        1.757273e-01  \n",
       "8  4.388632e-01  6.624675e-01  6.757760e-01        5.571575e-01  \n",
       "9  3.925345e-01  6.265257e-01  7.100028e-01        6.039063e-01  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_X = X.iloc[:, list(sfs1.k_feature_idx_)]\n",
    "y = model_data['logkpl']\n",
    "\n",
    "train_and_evalute(selected_features_X, y, metric=\"MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1738: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - self.ssr/self.uncentered_tss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nAtom</td>\n",
       "      <td>4984.386364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATSc3</td>\n",
       "      <td>11.900787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATSc4</td>\n",
       "      <td>8.413273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATSm5</td>\n",
       "      <td>382.720982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATSp5</td>\n",
       "      <td>429.363298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BCUTp-1l</td>\n",
       "      <td>124.104834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nBase</td>\n",
       "      <td>4.268558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nB</td>\n",
       "      <td>7506.930061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C1SP1</td>\n",
       "      <td>1.074174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C2SP1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C2SP3</td>\n",
       "      <td>88.679891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C3SP3</td>\n",
       "      <td>114.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C4SP3</td>\n",
       "      <td>149.057702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SCH-7</td>\n",
       "      <td>166.700292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VC-6</td>\n",
       "      <td>66.806936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SP-0</td>\n",
       "      <td>24101.516393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VP-4</td>\n",
       "      <td>3098.689748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>VP-5</td>\n",
       "      <td>193.868570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>VP-6</td>\n",
       "      <td>2768.328093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FMF</td>\n",
       "      <td>62.631901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fsp3</td>\n",
       "      <td>114.516973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JPLogP</td>\n",
       "      <td>114.412829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>khs.sLi</td>\n",
       "      <td>47.877503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>khs.ssBe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>khs.sSH</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MDEC-12</td>\n",
       "      <td>39.028374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MDEN-22</td>\n",
       "      <td>2.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TopoPSA</td>\n",
       "      <td>214.025265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VABC</td>\n",
       "      <td>16447.901973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>XLogP</td>\n",
       "      <td>62.974423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variables           VIF\n",
       "0      nAtom   4984.386364\n",
       "1      ATSc3     11.900787\n",
       "2      ATSc4      8.413273\n",
       "3      ATSm5    382.720982\n",
       "4      ATSp5    429.363298\n",
       "5   BCUTp-1l    124.104834\n",
       "6      nBase      4.268558\n",
       "7         nB   7506.930061\n",
       "8      C1SP1      1.074174\n",
       "9      C2SP1           NaN\n",
       "10     C2SP3     88.679891\n",
       "11     C3SP3    114.151807\n",
       "12     C4SP3    149.057702\n",
       "13     SCH-7    166.700292\n",
       "14      VC-6     66.806936\n",
       "15      SP-0  24101.516393\n",
       "16      VP-4   3098.689748\n",
       "17      VP-5    193.868570\n",
       "18      VP-6   2768.328093\n",
       "19       FMF     62.631901\n",
       "20      Fsp3    114.516973\n",
       "21    JPLogP    114.412829\n",
       "22   khs.sLi     47.877503\n",
       "23  khs.ssBe           NaN\n",
       "24   khs.sSH           NaN\n",
       "25   MDEC-12     39.028374\n",
       "26   MDEN-22      2.831600\n",
       "27   TopoPSA    214.025265\n",
       "28      VABC  16447.901973\n",
       "29     XLogP     62.974423"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_vif(selected_features_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature-Selection-PCA\"></a>\n",
    "### 3.3 Feature Selection using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components by PCA 36\n",
      "Explained Variance Ratio 0.9910016806575245\n"
     ]
    }
   ],
   "source": [
    "X = model_data.drop([\"logkpl\"], axis=1)\n",
    "y = model_data['logkpl']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# initilizing and fitting the pca\n",
    "pca = PCA(n_components=0.99)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Number of Components by PCA\", X_pca.shape[1])\n",
    "print(\"Explained Variance Ratio\", pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of PCA(n_components=0.99)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (223, 36), indices imply (36, 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Skin Permeation\\Skin-Permeation\\notebooks\\trials\\3. Trial4.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_ \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(pca\u001b[39m.\u001b[39;49mcomponents_\u001b[39m.\u001b[39;49mT, index \u001b[39m=\u001b[39;49m {\u001b[39m'\u001b[39;49m\u001b[39mPC\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(X_pca\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])})\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    695\u001b[0m             data,\n\u001b[0;32m    696\u001b[0m             index,\n\u001b[0;32m    697\u001b[0m             columns,\n\u001b[0;32m    698\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    699\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    700\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    701\u001b[0m         )\n\u001b[0;32m    703\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    420\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 422\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (223, 36), indices imply (36, 36)"
     ]
    }
   ],
   "source": [
    "test_ = pd.DataFrame(pca.components_.T, index = {'PC{}'.format(i+1) for i in range(X_pca.shape[1])}) #columns=X.columns,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texpi</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>ATSc1</th>\n",
       "      <th>...</th>\n",
       "      <th>MW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PC22</th>\n",
       "      <td>-0.031134</td>\n",
       "      <td>0.030326</td>\n",
       "      <td>0.029669</td>\n",
       "      <td>0.114115</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>-0.010320</td>\n",
       "      <td>-0.029310</td>\n",
       "      <td>-0.028832</td>\n",
       "      <td>0.114519</td>\n",
       "      <td>0.052669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101812</td>\n",
       "      <td>0.115455</td>\n",
       "      <td>0.064667</td>\n",
       "      <td>0.032302</td>\n",
       "      <td>0.060440</td>\n",
       "      <td>-0.018330</td>\n",
       "      <td>0.103617</td>\n",
       "      <td>0.117898</td>\n",
       "      <td>0.023811</td>\n",
       "      <td>0.117122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC26</th>\n",
       "      <td>0.010533</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.010250</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.004319</td>\n",
       "      <td>-0.001354</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>-0.004386</td>\n",
       "      <td>-0.006872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069261</td>\n",
       "      <td>-0.012309</td>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.133848</td>\n",
       "      <td>-0.005664</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>-0.007815</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>-0.006046</td>\n",
       "      <td>-0.007859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC18</th>\n",
       "      <td>0.051618</td>\n",
       "      <td>-0.196747</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>-0.003918</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>-0.137567</td>\n",
       "      <td>-0.136193</td>\n",
       "      <td>0.017040</td>\n",
       "      <td>0.199363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>-0.075486</td>\n",
       "      <td>0.122665</td>\n",
       "      <td>0.199015</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>0.056777</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>-0.188817</td>\n",
       "      <td>-0.003149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC5</th>\n",
       "      <td>0.100162</td>\n",
       "      <td>-0.031766</td>\n",
       "      <td>0.043603</td>\n",
       "      <td>0.046351</td>\n",
       "      <td>0.021389</td>\n",
       "      <td>0.057670</td>\n",
       "      <td>0.202798</td>\n",
       "      <td>0.203425</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.114939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072590</td>\n",
       "      <td>0.061656</td>\n",
       "      <td>0.090736</td>\n",
       "      <td>0.128473</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>0.185328</td>\n",
       "      <td>0.068606</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>-0.029105</td>\n",
       "      <td>0.044390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC19</th>\n",
       "      <td>0.013156</td>\n",
       "      <td>0.169669</td>\n",
       "      <td>0.171723</td>\n",
       "      <td>0.068874</td>\n",
       "      <td>0.071087</td>\n",
       "      <td>0.008856</td>\n",
       "      <td>0.064905</td>\n",
       "      <td>0.063447</td>\n",
       "      <td>0.075688</td>\n",
       "      <td>-0.055343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063525</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>0.056302</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.046111</td>\n",
       "      <td>-0.050427</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>-0.031660</td>\n",
       "      <td>0.162314</td>\n",
       "      <td>-0.012495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC3</th>\n",
       "      <td>-0.038630</td>\n",
       "      <td>0.022768</td>\n",
       "      <td>-0.123096</td>\n",
       "      <td>0.029593</td>\n",
       "      <td>0.026201</td>\n",
       "      <td>0.075240</td>\n",
       "      <td>-0.064603</td>\n",
       "      <td>-0.062018</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0.011610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035915</td>\n",
       "      <td>0.010416</td>\n",
       "      <td>-0.123941</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>-0.069951</td>\n",
       "      <td>0.210349</td>\n",
       "      <td>0.058728</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.014053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC20</th>\n",
       "      <td>0.074036</td>\n",
       "      <td>0.065755</td>\n",
       "      <td>0.129346</td>\n",
       "      <td>0.029632</td>\n",
       "      <td>0.037801</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>0.073475</td>\n",
       "      <td>0.070509</td>\n",
       "      <td>0.040948</td>\n",
       "      <td>-0.033702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069635</td>\n",
       "      <td>0.018906</td>\n",
       "      <td>-0.120407</td>\n",
       "      <td>-0.075146</td>\n",
       "      <td>0.036866</td>\n",
       "      <td>-0.091467</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>0.084629</td>\n",
       "      <td>0.035862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC11</th>\n",
       "      <td>-0.027987</td>\n",
       "      <td>-0.031357</td>\n",
       "      <td>-0.119799</td>\n",
       "      <td>-0.006994</td>\n",
       "      <td>-0.010037</td>\n",
       "      <td>-0.038942</td>\n",
       "      <td>-0.035307</td>\n",
       "      <td>-0.038564</td>\n",
       "      <td>-0.012992</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015015</td>\n",
       "      <td>-0.010449</td>\n",
       "      <td>-0.052940</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.021083</td>\n",
       "      <td>0.030460</td>\n",
       "      <td>-0.003296</td>\n",
       "      <td>-0.000883</td>\n",
       "      <td>-0.045563</td>\n",
       "      <td>-0.002798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC12</th>\n",
       "      <td>0.075948</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.053060</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.138356</td>\n",
       "      <td>0.012995</td>\n",
       "      <td>0.019829</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>-0.013816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019384</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.028088</td>\n",
       "      <td>-0.035190</td>\n",
       "      <td>-0.022426</td>\n",
       "      <td>0.063407</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>-0.028877</td>\n",
       "      <td>0.007813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC13</th>\n",
       "      <td>-0.106433</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>-0.016095</td>\n",
       "      <td>0.010013</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>0.189053</td>\n",
       "      <td>-0.025590</td>\n",
       "      <td>-0.023749</td>\n",
       "      <td>-0.008754</td>\n",
       "      <td>-0.007759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012833</td>\n",
       "      <td>0.015194</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>-0.005839</td>\n",
       "      <td>-0.007580</td>\n",
       "      <td>-0.028162</td>\n",
       "      <td>0.083090</td>\n",
       "      <td>-0.001552</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.005351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC16</th>\n",
       "      <td>-0.024160</td>\n",
       "      <td>-0.037733</td>\n",
       "      <td>-0.082037</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>-0.001437</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006637</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>-0.014606</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>-0.009761</td>\n",
       "      <td>0.020692</td>\n",
       "      <td>0.014883</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC27</th>\n",
       "      <td>0.040169</td>\n",
       "      <td>0.094654</td>\n",
       "      <td>0.176209</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.023089</td>\n",
       "      <td>-0.082017</td>\n",
       "      <td>-0.085205</td>\n",
       "      <td>-0.004987</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>-0.006169</td>\n",
       "      <td>0.024947</td>\n",
       "      <td>-0.063560</td>\n",
       "      <td>-0.002133</td>\n",
       "      <td>0.010923</td>\n",
       "      <td>0.136784</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC30</th>\n",
       "      <td>-0.029679</td>\n",
       "      <td>-0.014519</td>\n",
       "      <td>-0.068333</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>-0.004482</td>\n",
       "      <td>0.115438</td>\n",
       "      <td>0.060124</td>\n",
       "      <td>0.066535</td>\n",
       "      <td>-0.008399</td>\n",
       "      <td>-0.051195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.042696</td>\n",
       "      <td>-0.006499</td>\n",
       "      <td>-0.036438</td>\n",
       "      <td>0.099545</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>-0.045915</td>\n",
       "      <td>-0.004260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC36</th>\n",
       "      <td>0.035436</td>\n",
       "      <td>-0.022509</td>\n",
       "      <td>0.073719</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>-0.134064</td>\n",
       "      <td>-0.073024</td>\n",
       "      <td>-0.073349</td>\n",
       "      <td>0.020417</td>\n",
       "      <td>-0.043346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004885</td>\n",
       "      <td>-0.005689</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>-0.129302</td>\n",
       "      <td>0.177240</td>\n",
       "      <td>-0.021119</td>\n",
       "      <td>-0.018696</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>-0.002919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC29</th>\n",
       "      <td>-0.098841</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>-0.006581</td>\n",
       "      <td>-0.013119</td>\n",
       "      <td>-0.016133</td>\n",
       "      <td>0.088695</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.037289</td>\n",
       "      <td>-0.025512</td>\n",
       "      <td>-0.027366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.011014</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>-0.003951</td>\n",
       "      <td>-0.048172</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>0.057590</td>\n",
       "      <td>-0.013224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC1</th>\n",
       "      <td>-0.185272</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>-0.124613</td>\n",
       "      <td>-0.009987</td>\n",
       "      <td>-0.020965</td>\n",
       "      <td>-0.001843</td>\n",
       "      <td>0.045386</td>\n",
       "      <td>0.049865</td>\n",
       "      <td>-0.025050</td>\n",
       "      <td>0.043105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015658</td>\n",
       "      <td>-0.012999</td>\n",
       "      <td>-0.028868</td>\n",
       "      <td>-0.029042</td>\n",
       "      <td>-0.017439</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>0.073216</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>-0.062048</td>\n",
       "      <td>-0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC33</th>\n",
       "      <td>-0.051700</td>\n",
       "      <td>-0.019953</td>\n",
       "      <td>-0.014658</td>\n",
       "      <td>0.023568</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>-0.059643</td>\n",
       "      <td>-0.030274</td>\n",
       "      <td>-0.036246</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.019598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038043</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>0.080504</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.112427</td>\n",
       "      <td>-0.010666</td>\n",
       "      <td>0.016750</td>\n",
       "      <td>-0.038729</td>\n",
       "      <td>0.007993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC24</th>\n",
       "      <td>0.367531</td>\n",
       "      <td>-0.078163</td>\n",
       "      <td>-0.136711</td>\n",
       "      <td>-0.001511</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.028835</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>-0.008521</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020396</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>-0.052944</td>\n",
       "      <td>-0.015676</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>-0.024909</td>\n",
       "      <td>-0.002041</td>\n",
       "      <td>-0.068576</td>\n",
       "      <td>0.004920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC2</th>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.074318</td>\n",
       "      <td>-0.050300</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>0.011789</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>0.047217</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>-0.020320</td>\n",
       "      <td>-0.007507</td>\n",
       "      <td>-0.043512</td>\n",
       "      <td>0.016801</td>\n",
       "      <td>0.012265</td>\n",
       "      <td>-0.093543</td>\n",
       "      <td>0.002162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC32</th>\n",
       "      <td>-0.239073</td>\n",
       "      <td>-0.037706</td>\n",
       "      <td>-0.101855</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>-0.097813</td>\n",
       "      <td>-0.051766</td>\n",
       "      <td>-0.042184</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.028280</td>\n",
       "      <td>-0.002757</td>\n",
       "      <td>-0.007800</td>\n",
       "      <td>-0.028650</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>-0.004309</td>\n",
       "      <td>-0.036865</td>\n",
       "      <td>0.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC6</th>\n",
       "      <td>-0.072137</td>\n",
       "      <td>0.034161</td>\n",
       "      <td>0.060950</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>-0.002258</td>\n",
       "      <td>-0.361454</td>\n",
       "      <td>-0.016970</td>\n",
       "      <td>-0.009341</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>-0.047996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011951</td>\n",
       "      <td>0.006808</td>\n",
       "      <td>0.032534</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.136396</td>\n",
       "      <td>-0.025411</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC9</th>\n",
       "      <td>0.128207</td>\n",
       "      <td>-0.022174</td>\n",
       "      <td>-0.082979</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>-0.001953</td>\n",
       "      <td>0.054362</td>\n",
       "      <td>-0.020570</td>\n",
       "      <td>-0.022754</td>\n",
       "      <td>-0.008278</td>\n",
       "      <td>0.035694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.007794</td>\n",
       "      <td>0.026916</td>\n",
       "      <td>0.025499</td>\n",
       "      <td>0.055581</td>\n",
       "      <td>-0.062498</td>\n",
       "      <td>-0.078428</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>-0.071234</td>\n",
       "      <td>0.008117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC7</th>\n",
       "      <td>-0.100963</td>\n",
       "      <td>-0.033207</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>-0.243088</td>\n",
       "      <td>-0.033744</td>\n",
       "      <td>-0.030470</td>\n",
       "      <td>0.019879</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.011614</td>\n",
       "      <td>-0.026872</td>\n",
       "      <td>-0.005591</td>\n",
       "      <td>-0.049099</td>\n",
       "      <td>0.108010</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>0.020721</td>\n",
       "      <td>-0.068408</td>\n",
       "      <td>0.007586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC14</th>\n",
       "      <td>0.321346</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.177112</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>-0.002732</td>\n",
       "      <td>-0.011383</td>\n",
       "      <td>-0.021534</td>\n",
       "      <td>-0.016535</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>-0.015799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003768</td>\n",
       "      <td>-0.015527</td>\n",
       "      <td>-0.049301</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.101273</td>\n",
       "      <td>0.047315</td>\n",
       "      <td>0.015798</td>\n",
       "      <td>-0.005740</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>-0.007190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC4</th>\n",
       "      <td>-0.013300</td>\n",
       "      <td>-0.028283</td>\n",
       "      <td>-0.075141</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>-0.137686</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>-0.012555</td>\n",
       "      <td>0.011184</td>\n",
       "      <td>-0.024851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.092154</td>\n",
       "      <td>-0.003127</td>\n",
       "      <td>-0.003586</td>\n",
       "      <td>-0.014331</td>\n",
       "      <td>-0.027642</td>\n",
       "      <td>-0.024996</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.003996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC8</th>\n",
       "      <td>-0.047033</td>\n",
       "      <td>0.054066</td>\n",
       "      <td>0.233897</td>\n",
       "      <td>-0.014562</td>\n",
       "      <td>-0.009978</td>\n",
       "      <td>0.553997</td>\n",
       "      <td>-0.071840</td>\n",
       "      <td>-0.082075</td>\n",
       "      <td>-0.008873</td>\n",
       "      <td>-0.042517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>-0.077975</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>-0.027064</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.072843</td>\n",
       "      <td>-0.015010</td>\n",
       "      <td>0.120930</td>\n",
       "      <td>-0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC31</th>\n",
       "      <td>0.064535</td>\n",
       "      <td>-0.009429</td>\n",
       "      <td>-0.159236</td>\n",
       "      <td>-0.020531</td>\n",
       "      <td>-0.014778</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.048282</td>\n",
       "      <td>0.051457</td>\n",
       "      <td>-0.013662</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>-0.002174</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>0.022172</td>\n",
       "      <td>0.026159</td>\n",
       "      <td>0.015689</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.050391</td>\n",
       "      <td>0.003615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC34</th>\n",
       "      <td>0.014729</td>\n",
       "      <td>-0.022557</td>\n",
       "      <td>-0.156807</td>\n",
       "      <td>-0.014083</td>\n",
       "      <td>-0.013435</td>\n",
       "      <td>0.041562</td>\n",
       "      <td>0.027163</td>\n",
       "      <td>0.026322</td>\n",
       "      <td>-0.015152</td>\n",
       "      <td>0.037280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>-0.006823</td>\n",
       "      <td>-0.019377</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>-0.035079</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>-0.049483</td>\n",
       "      <td>-0.004872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC17</th>\n",
       "      <td>0.192437</td>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.151587</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>-0.054660</td>\n",
       "      <td>-0.002844</td>\n",
       "      <td>-0.009741</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>-0.048301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017496</td>\n",
       "      <td>-0.002765</td>\n",
       "      <td>-0.032543</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.015521</td>\n",
       "      <td>-0.008679</td>\n",
       "      <td>0.053111</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>-0.041076</td>\n",
       "      <td>-0.005051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC15</th>\n",
       "      <td>-0.046574</td>\n",
       "      <td>-0.066867</td>\n",
       "      <td>0.031460</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-0.001906</td>\n",
       "      <td>0.304731</td>\n",
       "      <td>-0.045656</td>\n",
       "      <td>-0.021781</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>-0.035326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031126</td>\n",
       "      <td>-0.007064</td>\n",
       "      <td>0.080796</td>\n",
       "      <td>-0.038302</td>\n",
       "      <td>-0.052324</td>\n",
       "      <td>0.054879</td>\n",
       "      <td>-0.138246</td>\n",
       "      <td>-0.006459</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC35</th>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.025378</td>\n",
       "      <td>-0.069678</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>-0.072834</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>-0.026442</td>\n",
       "      <td>-0.014596</td>\n",
       "      <td>-0.001603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>-0.074357</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.033305</td>\n",
       "      <td>0.023689</td>\n",
       "      <td>0.055107</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>-0.058378</td>\n",
       "      <td>0.007925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC10</th>\n",
       "      <td>-0.026487</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.179626</td>\n",
       "      <td>-0.016608</td>\n",
       "      <td>-0.023543</td>\n",
       "      <td>-0.228798</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.033793</td>\n",
       "      <td>-0.032265</td>\n",
       "      <td>-0.011384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030009</td>\n",
       "      <td>-0.015418</td>\n",
       "      <td>-0.126470</td>\n",
       "      <td>-0.013341</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>-0.049852</td>\n",
       "      <td>-0.016910</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>-0.064023</td>\n",
       "      <td>-0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC21</th>\n",
       "      <td>0.515822</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>-0.024487</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>-0.026459</td>\n",
       "      <td>0.009043</td>\n",
       "      <td>-0.036775</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>-0.009576</td>\n",
       "      <td>-0.018066</td>\n",
       "      <td>-0.002493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC28</th>\n",
       "      <td>-0.161869</td>\n",
       "      <td>-0.009141</td>\n",
       "      <td>0.078020</td>\n",
       "      <td>-0.004289</td>\n",
       "      <td>-0.005797</td>\n",
       "      <td>0.127818</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.085211</td>\n",
       "      <td>-0.013051</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>-0.134436</td>\n",
       "      <td>0.010222</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>-0.021512</td>\n",
       "      <td>0.104383</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>-0.018417</td>\n",
       "      <td>-0.001353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC23</th>\n",
       "      <td>-0.120955</td>\n",
       "      <td>-0.062796</td>\n",
       "      <td>0.213229</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.186510</td>\n",
       "      <td>0.030863</td>\n",
       "      <td>0.035922</td>\n",
       "      <td>-0.005717</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>-0.147413</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>-0.041675</td>\n",
       "      <td>0.050647</td>\n",
       "      <td>0.091695</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>-0.000856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC25</th>\n",
       "      <td>0.206507</td>\n",
       "      <td>-0.000722</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>-0.001228</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.104022</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>-0.007229</td>\n",
       "      <td>-0.018290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-0.016388</td>\n",
       "      <td>-0.014157</td>\n",
       "      <td>-0.032413</td>\n",
       "      <td>0.147584</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-0.047566</td>\n",
       "      <td>-0.001376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Texpi     ALogP    ALogp2       AMR      apol     nAcid  naAromAtom  \\\n",
       "PC22 -0.031134  0.030326  0.029669  0.114115  0.115273 -0.010320   -0.029310   \n",
       "PC26  0.010533  0.008850  0.010250 -0.002350 -0.004319 -0.001354    0.021178   \n",
       "PC18  0.051618 -0.196747  0.021007 -0.016240 -0.003918  0.019890   -0.137567   \n",
       "PC5   0.100162 -0.031766  0.043603  0.046351  0.021389  0.057670    0.202798   \n",
       "PC19  0.013156  0.169669  0.171723  0.068874  0.071087  0.008856    0.064905   \n",
       "PC3  -0.038630  0.022768 -0.123096  0.029593  0.026201  0.075240   -0.064603   \n",
       "PC20  0.074036  0.065755  0.129346  0.029632  0.037801 -0.014044    0.073475   \n",
       "PC11 -0.027987 -0.031357 -0.119799 -0.006994 -0.010037 -0.038942   -0.035307   \n",
       "PC12  0.075948  0.004944  0.053060 -0.003113  0.005771  0.138356    0.012995   \n",
       "PC13 -0.106433  0.033448 -0.016095  0.010013 -0.000685  0.189053   -0.025590   \n",
       "PC16 -0.024160 -0.037733 -0.082037  0.000704 -0.001584  0.000119    0.013810   \n",
       "PC27  0.040169  0.094654  0.176209 -0.001935 -0.000090 -0.023089   -0.082017   \n",
       "PC30 -0.029679 -0.014519 -0.068333  0.000469 -0.004482  0.115438    0.060124   \n",
       "PC36  0.035436 -0.022509  0.073719  0.000005  0.013214 -0.134064   -0.073024   \n",
       "PC29 -0.098841  0.074548 -0.006581 -0.013119 -0.016133  0.088695    0.033715   \n",
       "PC1  -0.185272 -0.018505 -0.124613 -0.009987 -0.020965 -0.001843    0.045386   \n",
       "PC33 -0.051700 -0.019953 -0.014658  0.023568  0.006289 -0.059643   -0.030274   \n",
       "PC24  0.367531 -0.078163 -0.136711 -0.001511  0.001794  0.028835    0.001717   \n",
       "PC2   0.000668 -0.074318 -0.050300  0.007543  0.011789  0.026008    0.047217   \n",
       "PC32 -0.239073 -0.037706 -0.101855  0.003938  0.007683 -0.097813   -0.051766   \n",
       "PC6  -0.072137  0.034161  0.060950  0.007441 -0.002258 -0.361454   -0.016970   \n",
       "PC9   0.128207 -0.022174 -0.082979  0.009797 -0.001953  0.054362   -0.020570   \n",
       "PC7  -0.100963 -0.033207  0.025519  0.017866  0.015301 -0.243088   -0.033744   \n",
       "PC14  0.321346  0.009518  0.177112 -0.005440 -0.002732 -0.011383   -0.021534   \n",
       "PC4  -0.013300 -0.028283 -0.075141  0.014726  0.013393 -0.137686   -0.021528   \n",
       "PC8  -0.047033  0.054066  0.233897 -0.014562 -0.009978  0.553997   -0.071840   \n",
       "PC31  0.064535 -0.009429 -0.159236 -0.020531 -0.014778  0.178947    0.048282   \n",
       "PC34  0.014729 -0.022557 -0.156807 -0.014083 -0.013435  0.041562    0.027163   \n",
       "PC17  0.192437  0.009609  0.151587  0.000932  0.000218 -0.054660   -0.002844   \n",
       "PC15 -0.046574 -0.066867  0.031460  0.002200 -0.001906  0.304731   -0.045656   \n",
       "PC35 -0.001325 -0.025378 -0.069678  0.022180 -0.003749 -0.072834   -0.004113   \n",
       "PC10 -0.026487  0.008262  0.179626 -0.016608 -0.023543 -0.228798    0.025300   \n",
       "PC21  0.515822 -0.005165  0.007256  0.001169 -0.012987  0.013008   -0.000266   \n",
       "PC28 -0.161869 -0.009141  0.078020 -0.004289 -0.005797  0.127818    0.056186   \n",
       "PC23 -0.120955 -0.062796  0.213229  0.003885  0.000560 -0.186510    0.030863   \n",
       "PC25  0.206507 -0.000722  0.018294 -0.001228 -0.000281 -0.104022    0.013958   \n",
       "\n",
       "      nAromBond     nAtom     ATSc1  ...        MW    WTPT-1    WTPT-2  \\\n",
       "PC22  -0.028832  0.114519  0.052669  ...  0.101812  0.115455  0.064667   \n",
       "PC26   0.022900 -0.004386 -0.006872  ...  0.069261 -0.012309  0.096491   \n",
       "PC18  -0.136193  0.017040  0.199363  ...  0.022298  0.006382 -0.075486   \n",
       "PC5    0.203425  0.009061  0.114939  ...  0.072590  0.061656  0.090736   \n",
       "PC19   0.063447  0.075688 -0.055343  ...  0.063525  0.028867  0.056302   \n",
       "PC3   -0.062018  0.029834  0.011610  ... -0.035915  0.010416 -0.123941   \n",
       "PC20   0.070509  0.040948 -0.033702  ... -0.069635  0.018906 -0.120407   \n",
       "PC11  -0.038564 -0.012992 -0.002797  ... -0.015015 -0.010449 -0.052940   \n",
       "PC12   0.019829  0.015674 -0.013816  ... -0.019384  0.013657  0.028088   \n",
       "PC13  -0.023749 -0.008754 -0.007759  ...  0.012833  0.015194 -0.008154   \n",
       "PC16   0.013108 -0.001437 -0.001574  ... -0.006637 -0.000595 -0.014606   \n",
       "PC27  -0.085205 -0.004987  0.074251  ...  0.002445  0.005331  0.025443   \n",
       "PC30   0.066535 -0.008399 -0.051195  ...  0.004233 -0.008705 -0.042696   \n",
       "PC36  -0.073349  0.020417 -0.043346  ... -0.004885 -0.005689  0.013212   \n",
       "PC29   0.037289 -0.025512 -0.027366  ...  0.003448 -0.019462 -0.011014   \n",
       "PC1    0.049865 -0.025050  0.043105  ... -0.015658 -0.012999 -0.028868   \n",
       "PC33  -0.036246 -0.001882 -0.019598  ...  0.038043  0.014948  0.007622   \n",
       "PC24  -0.008521  0.005482  0.015830  ... -0.020396  0.006605 -0.052944   \n",
       "PC2    0.050880  0.011681 -0.001224  ...  0.000748  0.005750  0.021135   \n",
       "PC32  -0.042184  0.001993  0.030119  ...  0.007713  0.004392  0.028280   \n",
       "PC6   -0.009341  0.007113 -0.047996  ... -0.011951  0.006808  0.032534   \n",
       "PC9   -0.022754 -0.008278  0.035694  ...  0.011730  0.007794  0.026916   \n",
       "PC7   -0.030470  0.019879  0.000760  ...  0.003207  0.011614 -0.026872   \n",
       "PC14  -0.016535 -0.007470 -0.015799  ... -0.003768 -0.015527 -0.049301   \n",
       "PC4   -0.012555  0.011184 -0.024851  ...  0.010287  0.004172  0.092154   \n",
       "PC8   -0.082075 -0.008873 -0.042517  ... -0.004842 -0.012249 -0.077975   \n",
       "PC31   0.051457 -0.013662 -0.001548  ... -0.014197 -0.002174 -0.001275   \n",
       "PC34   0.026322 -0.015152  0.037280  ...  0.002893 -0.006823 -0.019377   \n",
       "PC17  -0.009741  0.005733 -0.048301  ...  0.017496 -0.002765 -0.032543   \n",
       "PC15  -0.021781 -0.001383 -0.035326  ... -0.031126 -0.007064  0.080796   \n",
       "PC35  -0.026442 -0.014596 -0.001603  ...  0.006482  0.009019 -0.074357   \n",
       "PC10   0.033793 -0.032265 -0.011384  ... -0.030009 -0.015418 -0.126470   \n",
       "PC21   0.024363 -0.024487  0.022926  ...  0.014976  0.002098 -0.026459   \n",
       "PC28   0.085211 -0.013051  0.006585  ...  0.015562  0.000780 -0.134436   \n",
       "PC23   0.035922 -0.005717  0.005686  ...  0.009423  0.003035 -0.147413   \n",
       "PC25   0.005190 -0.007229 -0.018290  ... -0.000485  0.000744 -0.004894   \n",
       "\n",
       "        WTPT-3    WTPT-4    WTPT-5     WPATH      WPOL     XLogP    Zagreb  \n",
       "PC22  0.032302  0.060440 -0.018330  0.103617  0.117898  0.023811  0.117122  \n",
       "PC26  0.133848 -0.005664  0.005569 -0.007815 -0.007042 -0.006046 -0.007859  \n",
       "PC18  0.122665  0.199015  0.005294  0.056777  0.005852 -0.188817 -0.003149  \n",
       "PC5   0.128473  0.064389  0.185328  0.068606  0.021310 -0.029105  0.044390  \n",
       "PC19 -0.024750 -0.046111 -0.050427  0.030127 -0.031660  0.162314 -0.012495  \n",
       "PC3   0.006293 -0.069951  0.210349  0.058728  0.006815  0.005178  0.014053  \n",
       "PC20 -0.075146  0.036866 -0.091467  0.038523  0.024288  0.084629  0.035862  \n",
       "PC11 -0.000013 -0.021083  0.030460 -0.003296 -0.000883 -0.045563 -0.002798  \n",
       "PC12 -0.035190 -0.022426  0.063407  0.027432  0.010967 -0.028877  0.007813  \n",
       "PC13 -0.005839 -0.007580 -0.028162  0.083090 -0.001552  0.021814  0.005351  \n",
       "PC16  0.000973 -0.009761  0.020692  0.014883 -0.003327 -0.054030 -0.000261  \n",
       "PC27 -0.006169  0.024947 -0.063560 -0.002133  0.010923  0.136784  0.001804  \n",
       "PC30 -0.006499 -0.036438  0.099545  0.022315  0.000716 -0.045915 -0.004260  \n",
       "PC36  0.002041 -0.129302  0.177240 -0.021119 -0.018696 -0.001367 -0.002919  \n",
       "PC29  0.010395 -0.003951 -0.048172  0.004431 -0.008512  0.057590 -0.013224  \n",
       "PC1  -0.029042 -0.017439 -0.000964  0.073216  0.004413 -0.062048 -0.016474  \n",
       "PC33  0.080504  0.000394  0.112427 -0.010666  0.016750 -0.038729  0.007993  \n",
       "PC24 -0.015676  0.037017  0.009528 -0.024909 -0.002041 -0.068576  0.004920  \n",
       "PC2  -0.020320 -0.007507 -0.043512  0.016801  0.012265 -0.093543  0.002162  \n",
       "PC32 -0.002757 -0.007800 -0.028650  0.006550 -0.004309 -0.036865  0.002201  \n",
       "PC6   0.033435  0.002047  0.136396 -0.025411  0.002968  0.083020  0.008100  \n",
       "PC9   0.025499  0.055581 -0.062498 -0.078428  0.004845 -0.071234  0.008117  \n",
       "PC7  -0.005591 -0.049099  0.108010 -0.001399  0.020721 -0.068408  0.007586  \n",
       "PC14 -0.014735 -0.101273  0.047315  0.015798 -0.005740  0.002527 -0.007190  \n",
       "PC4  -0.003127 -0.003586 -0.014331 -0.027642 -0.024996 -0.000793 -0.003996  \n",
       "PC8   0.006352 -0.027064  0.044000  0.072843 -0.015010  0.120930 -0.003968  \n",
       "PC31  0.022172  0.026159  0.015689  0.002319  0.014855  0.050391  0.003615  \n",
       "PC34 -0.000236  0.024341 -0.035079  0.005337  0.005971 -0.049483 -0.004872  \n",
       "PC17  0.000240  0.015521 -0.008679  0.053111  0.008933 -0.041076 -0.005051  \n",
       "PC15 -0.038302 -0.052324  0.054879 -0.138246 -0.006459  0.015680  0.000427  \n",
       "PC35 -0.000288 -0.033305  0.023689  0.055107  0.004922 -0.058378  0.007925  \n",
       "PC10 -0.013341  0.004777 -0.049852 -0.016910  0.014048 -0.064023 -0.002321  \n",
       "PC21  0.009043 -0.036775  0.009511  0.027508 -0.009576 -0.018066 -0.002493  \n",
       "PC28  0.010222 -0.019741 -0.021512  0.104383  0.018890 -0.018417 -0.001353  \n",
       "PC23  0.004058 -0.041675  0.050647  0.091695  0.001286  0.018615 -0.000856  \n",
       "PC25 -0.016388 -0.014157 -0.032413  0.147584 -0.019944 -0.047566 -0.001376  \n",
       "\n",
       "[36 rows x 223 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "initial_feature_names = X.columns\n",
    "\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=most_important_names)\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "df[1].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20,\n",
       " 205,\n",
       " 185,\n",
       " 200,\n",
       " 92,\n",
       " 218,\n",
       " 168,\n",
       " 203,\n",
       " 25,\n",
       " 124,\n",
       " 33,\n",
       " 116,\n",
       " 162,\n",
       " 143,\n",
       " 138,\n",
       " 145,\n",
       " 146,\n",
       " 0,\n",
       " 141,\n",
       " 129,\n",
       " 123,\n",
       " 129,\n",
       " 100,\n",
       " 113,\n",
       " 100,\n",
       " 5,\n",
       " 100,\n",
       " 162,\n",
       " 13,\n",
       " 103,\n",
       " 103,\n",
       " 145,\n",
       " 0,\n",
       " 110,\n",
       " 13,\n",
       " 190]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (337, 36) \t Shape of y_train: (337,)\n",
      "Shape of X_test: (113, 36) \t Shape of y_test: (113,)\n",
      "LinearRegression\n",
      "Lasso\n",
      "DecisionTree\n",
      "RandomForest\n",
      "XGBRegressor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 931, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 401, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 945, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 666, in __init__\n    self.feature_names = feature_names\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 1035, in feature_names\n    raise ValueError('feature_names must be unique')\nValueError: feature_names must be unique\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Skin Permeation\\Skin-Permeation\\notebooks\\trials\\3. Trial4.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pca_features \u001b[39m=\u001b[39m train_and_evalute(X_pca_df, y, metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMAPE\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pca_features\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Skin Permeation\\Skin-Permeation\\notebooks\\trials\\3. Trial4.ipynb Cell 33\u001b[0m in \u001b[0;36mtrain_and_evalute\u001b[1;34m(X, y, metric)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# fitting our data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     pipe\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     evaluate_model(models_scores_df, i, model_name, pipe, X, y, X_test, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# selecting top 3 score based on metric\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m filtered_models_scores_df \u001b[39m=\u001b[39m  models_scores_df\u001b[39m.\u001b[39msort_values(metric)\u001b[39m.\u001b[39miloc[:\u001b[39m3\u001b[39m, :]\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Skin Permeation\\Skin-Permeation\\notebooks\\trials\\3. Trial4.ipynb Cell 33\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(model_df, i, model_name, model, X, y, X_test, y_test)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mthis function is for regression takes the model with the data and calculate\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mthe scores, with cross validation techniques, in addition to MAE, MSE, RMSE, MAPE\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m:param X_train, X_test, y_train, y_test: data that was used\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# cross validation with 5 folds\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m all_cv_5 \u001b[39m=\u001b[39m cross_val_score(model, X, y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, scoring\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mneg_mean_absolute_percentage_error\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#print(\"all CV 5: {}\".format(all_cv_5))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(\"Mean Cross-Validation score: {}\".format(all_cv_5.mean()))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# predictions from our model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Skin%20Permeation/Skin-Permeation/notebooks/trials/3.%20Trial4.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 931, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 401, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 945, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 666, in __init__\n    self.feature_names = feature_names\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 1035, in feature_names\n    raise ValueError('feature_names must be unique')\nValueError: feature_names must be unique\n"
     ]
    }
   ],
   "source": [
    "pca_features = train_and_evalute(X_pca_df, y, metric=\"MAPE\")\n",
    "pca_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ann\"></a>\n",
    "# 4. ANN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "328543c14e67749527378fd9d598798d406739103cb7dff262233cddae2d7f33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
